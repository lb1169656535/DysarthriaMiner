ID,Title,Abstract,Date Added
8512311,Quantitative Assessment of Syllabic Timing Deficits in Ataxic Dysarthria,"Parametric analysis of Cerebellar Dysarthria (CD) may be valuable and more informative compared to its clinical assessment. A quantifiable estimation of the timing deficits in repeated syllabic utterance is described in the current study. Thirty-five individuals were diagnosed with cerebellar ataxia to varying degrees and twenty-six age-matched healthy controls were recruited. To automatically detect the local maxima of each syllable in the recorded speech files, a topographic prominence incorporated concept is designed. Subsequently, four acoustic features and eight corresponding parametric measurements are extracted to identify articulatory deficits in ataxic dysarthria. A comparative study on the behaviour of these measures for dysarthric and non-dysarthric subjects is presented in this paper. The results are further explored using a dimensionreduction tool (Principal Component Analysis) to emphasize variation and bring out the strongest discriminating patterns in our feature dataset.",Date Added to IEEEXplore:28 October 2018
10781716,Non-invasive stroke diagnosis using speech data from dysarthria patients,"Acute Ischemic Stroke (AIS) is a major cause of disability and can lead to death in severe cases. A common symptom of AIS, dysarthria, significantly impacts the quality of life of patients. In this study, we developed a deep learning model using dysarthria data for cost-effective and non-invasive brain stroke diagnosis. We utilized models such as ResNet50, InceptionV4, ResNeXt50, SEResNeXt18, and AttResNet50 to effectively extract and classify speech features indicative of stroke symptoms. These models demonstrated high performance, with Sensitivity, Specificity, Precision, Accuracy, and F1-score values reaching 96.77%, 96.08%, 92.82%, 95.52%, and 93.82%, respectively. Our approach offers a non-invasive, cost-effective alternative for early stroke detection, with potential for further accuracy improvements through additional research. This method promises rapid, economical early diagnosis, which could positively impact long-term treatment and healthcare options.",Date Added to IEEEXplore:17 December 2024
10800228,Fine-Tuning Pre-Trained Audio Models for Dysarthria Severity Classification: A Second Place Solution in the Multimodal Dysarthria Severity Classification Challenge,"Dysarthria, a neurological disorder affecting speech, poses significant challenges for automatic diagnosis and severity classification due to its complexity and the scarcity of relevant datasets. This study addresses these challenges by leveraging the dataset provided by the ISCSLP 2024 Multimodal Dysarthria Severity Classification Challenge. Our approach primarily involves two key steps: splitting the training set into training and validation subsets, and fine-tuning pre-trained au-dio models to adapt them to the task. To ensure robust evaluation and prevent label leakage, we employed the StratifiedGroupKFold function from sklearn for dataset splitting, ensuring that training and validation sets do not share participants. This method allowed us to maintain consistent statistical distributions across folds. For model finetuning, we selected two pre-trained audio models, w2v-bert-2.0 and whisper-Iarge-v3, and fine-tuned them on different folds of the dataset. Our results indicate that the w2v-bert-2.0 model fine-tuned on fold-2 achieved the highest performance, with a validation Fl-score of 0.699 and an online score of 7.7452. The experimental results, including confusion matrices and embedding distributions, highlight the model's ability to distinguish between different severity levels of dysarthria. Our approach achieved a commendable second place in the competition’ demonstrating the effectiveness of fine-tuning pre-trained audio models for this task. Future work will explore the integration of multimodal data and multi-task learning strategies to further enhance model performance.",Date Added to IEEEXplore:23 December 2024
10307923,Automated Detection and Severity Assessment of Dysarthria using Raw Speech,"Dysarthria is a medical condition that impairs an individual’s ability to speak clearly due to muscle weakness or paralysis. To diagnose and monitor dysarthria severity, this article proposes the use of a deep learning model that utilizes raw speech waveforms. This approach eliminates the need for feature engineering and enhances the model’s ability to handle noise and speech variability. The proposed system was compared to a standard convolutional neural network (CNN) model and was found to perform better in both dysarthria severity classification and dysarthria/healthy control classification tasks. The results indicate that the SincNet model achieved an accuracy of 95.7% and 99.6% in these tasks, respectively. The proposed system can aid clinicians in diagnosing and monitoring dysarthria severity and could have broader applications in speech-related disorders. The study emphasizes the importance of using raw waveform-based models for speech analysis and demonstrates the effectiveness of SincNet in automatic dysarthria/healthy control classification and dysarthria severity assessment.",Date Added to IEEEXplore:23 November 2023
6936631,Investigation on articulatory and acoustic characteristics of dysarthria,"Direct measurement of articulation contributes to figure out the mechanism of dysarthric speech production accurately, comparing to analyzing acoustic signal alone. This study makes a statistical comparison between dysarthric and normal speech, and investigates and analyzes dysarthric characteristics based on articulatory and acoustic data of continuous English utterance. The distribution of the articulatory point is calculated for each vowel, where the vowels show a relatively smaller region for dysarthria than for normal speech. This implies that the speakers with dysarthria may have some difficulties to fully use the articulatory space as the speakers without dysarthria. The rhythm and sound source are analyzed using autocorrelation function, power spectrum and linear prediction coding. The results reveal that the speakers without dysarthria can keep steady rhythm and energy in uttering consonant-vowel repetition sequences but it is hard for the speakers with dysarthria to maintain the stability. Meanwhile, the dysarthric sound source shows unstable period of the fundamental frequency, which reflects that the speakers with dysarthria could not control the glottis movement well.",Date Added to IEEEXplore:27 October 2014
10550236,Evolving Diagnostic Techniques for Speech Disorders: Investigating Dysarthria Classification Through DenseNet201 CNN Framework,"The primary subject of this research work pertains to dysarthria, a prevalent speech impairment observed in individuals diagnosed with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Prompt detection of dysarthria is essential for effective therapies and improved patient results, as it significantly impacts communication proficiency. The research employed a DenseNet201 Convolutional Neural Network (CNN) to categorize speech using the TORGO database, which has 2000 samples of individuals with and without dysarthria, representing various genders. The dataset comprises individuals of both genders, encompassing both dysarthric and non-dysarthric individuals. Each dataset consists of 500 samples, which were collected during distinct sessions. The DenseNet201 convolutional neural network (CNN) technique has a notable accuracy rate of 96% in distinguishing between cases of dysarthria and non-dysarthria. This outcome underscores the potential of deep learning technology in aiding the timely identification of dysarthria and facilitating fast interventions for affected individuals. This work provides valuable insights on the progress of diagnostic capabilities and offers a potential avenue for enhancing the well-being of those experiencing speech impairments.",Date Added to IEEEXplore:11 June 2024
10902941,Analysis of Features for Dysarthria Severity Classification from Speech,"Dysarthria is a disorder that affects the ability of an individual to speak clearly. Diagnosis of the severity of dysarthria can aid in providing appropriate therapy to the individual. Therefore, the current work focuses on identifying features that can be used to estimate the severity of dysarthria. In this regard, two feature sets are considered, namely DisVoice and OpenSMILE, and the genetic algorithm is used to identify the most suitable list of features from both feature sets using 3 speech corpora, namely SSN-TDSC, Torgo, and UA Speech corpora. In order to test the effectiveness of these features, classifiers are trained on each feature set as a whole and on the features identified by the genetic algorithm, and their performance compared. It is observed that articulatory and prosodic features are most important in the diagnosis of severity of dysarthria. A maximum accuracy of 85% is obtained with the shortlisted DisVoice features and 97% with the OpenSMILE features.",Date Added to IEEEXplore:05 March 2025
10800159,Multi-Modal Dysarthria Severity Assessment Using Dual-Branch Feature Decoupling Network and Mixed Expert Framework,"Dysarthria, a motor speech disorder resulting from neurological damage, significantly impairs an individual's ability to articulate words. Assessing the severity of dysarthria is crucial for effective therapeutic interventions and rehabilitation strategies. However, current methods are time-consuming and subjective, leading to potential inconsistencies. This study proposed a dual-branch feature decoupling network and mixed expert framework for the automatic diagnosis and assessment of dysarthria. The proposed hybrid network architecture integrates a ResNet-based branch for audio data with an attention module and a 3D ResNet branch for video data, and was evaluated on the Multimodal Dysarthria Severity Assessment Challenge (MDSA) dataset, achieving significant performance improvements and ranking first in the challenge with a best score of 8.7404. The study's findings highlight the efficacy of the proposed framework in capturing essential features from multimodal data, contributing to more accurate and reliable assessments of dysarthria.",Date Added to IEEEXplore:23 December 2024
10448175,Spectral Analysis of Vowels and Fricatives at Varied Levels of Dysarthria Severity for Amyotrophic Lateral Sclerosis,"Dysarthria due to Amyotrophic Lateral Sclerosis (ALS) affects the acoustic characteristics of different speech sounds. The effects intensify with increasing severity leading to the collapse of the acoustic space of the affected individuals. With an aim to characterize such changes in the acoustic space, this paper studies the variations in band-specific and full-band spectral properties of 4 sustained vowels (/a/, /i/, /o/, /u/) and 3 sustained fricatives (/s/, /sh/, /f/) at different dysarthria severity levels. Effect of dysarthria on spectral features of these phonemes are not well explored. Statistical comparison of these features among different severities for the phonemes considered and among different vowels/fricatives for every severity level using speech data from 119 ALS and 40 healthy subjects indicate the followings. Though all band-specific and full-band features of the three fricatives and most of those features for the four vowels become statistically similar at high severity levels, certain features remain distinguishable. Spectral differences in 0-2 kHz band between /a/ and the other vowels and in the 2-6 kHz band between /a/ and /o/, /u/ persist through all severity levels. Moreover, properties of /f/ remain mostly unchanged with increasing dysarthria severity levels.",Date Added to IEEEXplore:18 March 2024
10673651,Advancing Speech Disorder Diagnostics: A Comprehensive Study on Dysarthria Classification with CNN,"This study article focuses on dysarthria, a speech problem that is often seen in patients with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early identification of dysarthria is crucial for successful treatments and better patient outcomes, since it has a substantial influence on communication ability. Using the TORGO database, which consists of 2000 samples of persons with and without dysarthria, spanning different genders, the research use a Convolutional Neural Network (CNN) to classify speech. The dataset includes both dysarthric and non-dysarthric individuals of both genders, with 500 samples each, captured during separate sessions. The CNN-based technique demonstrates an impressive 96% accuracy in differentiating between dysarthric and non-dysarthric instances, highlighting the promise of deep learning technology in assisting with the early detection of dysarthria and enabling prompt therapies for afflicted people. This study makes significant contributions to the advancement of diagnostic capacities and presents a potential opportunity to improve the quality of life for those with speech difficulties.",Date Added to IEEEXplore:18 September 2024
10581063,Progressing Speech Disorder Identification: A Thorough Investigation into Dysarthria Categorization Utilizing ResNet50 CNN,"The speech issue of dysarthria, which is frequently observed in individuals with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS), is the subject of this research article. Timely detection of dysarthria is essential for effective interventions and improved patient results, as it significantly impacts communication proficiency. The study employs the TORGO database, which has 2000 samples of individuals with and without dysarthria, encompassing various genders. The researchers utilize a ResNet50 Convolutional Neural Network (CNN) to categorize speech. The dataset comprises individuals of both genders, encompassing both dysarthric and non-dysarthric individuals. Each group consists of 500 samples, which were recorded over distinct sessions. The CNN-based technique achieves a remarkable 96% accuracy in distinguishing between dysarthric and non-dysarthric instances. This underscores the potential of deep learning technology in aiding the early identification of dysarthria and facilitating timely treatments for affected individuals. This study provides valuable contributions to the enhancement of diagnostic capabilities and offers a potential avenue for enhancing the quality of life for individuals experiencing speech impairments.",Date Added to IEEEXplore:12 July 2024
10625627,Enhancing the Diagnosis of Speech Disorders: An In-Depth Investigation into Dysarthria Classification Using the ResNet18 Model,"This research article concentrates on dysarthria, a speech impediment commonly observed in individuals with conditions such as cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early detection of dysarthria is essential for effective interventions and improved patient outcomes, as it significantly impacts communication abilities. Employing the TORGO database, comprising 2000 samples from individuals with and without dysarthria, encompassing diverse genders, the study utilizes a Convolutional Neural Network (ResNet18 Model) for speech classification. The dataset encompasses both dysarthric and non-dysarthric subjects of both genders, with 500 samples each, recorded in separate sessions. The ResNet18 Model-based approach demonstrates an impressive 96% accuracy in distinguishing between dysarthric and non-dysarthric instances, underscoring the potential of deep learning technology in aiding early dysarthria detection and facilitating timely interventions. This investigation significantly contributes to advancing diagnostic capabilities and presents a promising avenue to enhance the quality of life for individuals grappling with speech difficulties.",Date Added to IEEEXplore:22 August 2024
10095366,Statistical Analysis of Speech Disorder Specific Features to Characterise Dysarthria Severity Level,"Poor coordination of the speech production subsystems due to any neurological injury or a neuro-degenerative disease leads to dysarthria, a neuro-motor speech disorder. Dysarthric speech impairments can be mapped to the deficits caused in phonation, articulation, prosody, and glottal functioning. With the aim of reducing the subjectivity in clinical evaluations, many automated systems are proposed in the literature to assess the dysarthria severity level using these features. This work aims to analyse the suitability of these features in determining the severity level. A detailed investigation is done to rank these features for their efficacy in modelling the pathological aspects of dysarthric speech, using the technique of paraconsistent feature engineering. The study used two dysarthric speech databases, UA-Speech and TORGO. It puts light into the fact that both the prosody and articulation features are best useful for dysarthria severity estimation, which was supported by the classification accuracies obtained on using different machine learning classifiers.",Date Added to IEEEXplore:05 May 2023
8251336,Identification of Cerebellar Dysarthria with SISO Characterisation,"Quantitative identification of dysarthria plays a major role in the classification of its severity. This paper quantitatively analyses several components of cerebellar dysarthria. The methodology described in this study will be extended to other types of dysarthria via systematic analysis. The speech production model is characterized as a second-order singleinput and single-output (SISO), linear, time-invariant (LTI) system in our study. A comparative study on the behavior of the damping ratio and resonant frequency for dysarthric and non-dysarthric subjects is presented. The results are further analyzed using the Principal component analysis (PCA) technique to emphasize the variation and uncover strong patterns in the selected features. The effects of some other related factors like decay time and Q-factor are also highlighted.",Date Added to IEEEXplore:11 January 2018
10739183,CNN-Driven Innovations in Dysarthria Classification and Speech Disorder Management,"This research article addresses dysarthria, a speech disorder frequently observed in individuals with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early detection of dysarthria is essential for effective treatment and improved patient outcomes, as it significantly impacts communication abilities. Utilizing the TORGO database, which comprises 2000 speech samples from both dysarthric and non-dysarthric individuals across various genders, this study employs a Convolutional Neural Network (CNN) for classification. The dataset includes 500 samples each from both groups, collected in different sessions. The CNN approach achieves an impressive 96% accuracy in distinguishing dysarthric from non-dysarthric speech, showcasing the potential of deep learning technology in facilitating early dysarthria diagnosis and enabling timely interventions. This study makes substantial contributions to enhancing diagnostic capabilities and offers a promising avenue for improving the quality of life for individuals with speech disorders.",Date Added to IEEEXplore:04 November 2024
10800051,Constant Q Transform for Audio-Visual Dysarthria Severity Assessment,"This paper describes an automatic dysarthria severity assessment system submitted for the MDSA2024 challenge. The system utilizes audio and visual features to classify the severity of dysarthria. We investigate the effectiveness of different acoustic feature combinations, such as Mel spectrogram, filterbank and constant Q transform. Different encoders for audio data processing are compared in the experiments. The results demonstrate that the Conformer encoder achieves superior performance compared to the Densenet encoder on pure audio features. We further explore model-level fusion by combining audio and visual embeddings, leading to improvements in precision’ recall, and accuracy metrics. The best-achieved score on the test set is 7.5579, with an individual F1-score of 0.7028. The findings suggest that the proposed system with Conformer encoder and model-level fusion holds promise for automatic dysarthria assessment.",Date Added to IEEEXplore:23 December 2024
10781584,Dysarthria Detection with Deep Representation Learning for Patients with Parkinson’s Disease,"Dysarthria is a very common motor speech symptom in Parkinson’s disease impairing normal communications of patients. Detection of dysarthria could assist clinical diagnosis and intervention of Parkinson’s disease, provide monitoring approach for treatment-related side effects, and lead to effective speech therapy to prevent further communication and social deficits. Applying machine learning techniques to speech analysis for patients offers more resource-efficient, accessible and objective tools for screening and assessment of dysarthria. In this study, we constructed a multi-task speech dataset recorded from 600 participants with high data diversity to facilitate the development of detection models. We established a remote data acquisition and end-to-end prediction pipeline with deep representation learning, compared the performance with different feature-based learning and classification methods, and achieved a superior accuracy of over 90%. Different affecting factors were analyzed for model performance. Our proposed framework demonstrates the potential of developing and deploying an automated self-monitoring approach of dysarthria for patients with Parkinson’s disease, which could benefit a large-scale population and their disease managements.",Date Added to IEEEXplore:17 December 2024
7953122,Automatic assessment of dysarthria severity level using audio descriptors,"Dysarthria is a motor speech impairment, often characterized by speech that is generally indiscernible by human listeners. Assessment of the severity level of dysarthria provides an understanding of the patient's progression in the underlying cause and is essential for planning therapy, as well as improving automatic dysarthric speech recognition. In this paper, we propose a non-linguistic manner of automatic assessment of severity levels using audio descriptors or a set of features traditionally used to define timbre of musical instruments and have been modified to suit this purpose. Multitapered spectral estimation based features were computed and used for classification, in addition to the audio descriptors for timbre. An Artificial Neural Network (ANN) was trained to classify speech into various severity levels within Universal Access dysarthric speech corpus and the TORGO database. An average classification accuracy of 96.44% and 98.7% was obtained for UA speech corpus and TORGO database respectively.",Date Added to IEEEXplore:19 June 2017
9287741,Automated Dysarthria Severity Classification Using Deep Learning Frameworks,"Dysarthria is a neuro-motor speech disorder that renders speech unintelligible, in proportional to its severity. Assessing the severity level of dysarthria, apart from being a diagnostic step to evaluate the patient's improvement, is also capable of aiding automatic dysarthric speech recognition systems. In this paper, a detailed study on dysarthia severity classification using various deep learning architectural choices, namely deep neural network (DNN), convolutional neural network (CNN) and long short-term memory network (LSTM) is carried out. Mel frequency cepstral coefficients (MFCCs) and its derivatives are used as features. Performance of these models are compared with a baseline support vector machine (SVM) classifier using the UA-Speech corpus and the TORGO database. The highest classification accuracy of 96.18% and 93.24% are reported for TORGO and UA-Speech respectively. Detailed analysis on performance of these models shows that a proper choice of a deep learning architecture can ensure better performance than the conventionally used SVM classifier.",Date Added to IEEEXplore:18 December 2020
10584052,Deep Learning Based Speech Recognition for Hyperkinetic Dysarthria Disorder,"Speech recognition is a technology that aims to transform human speech into text and has applications in a variety of fields, including information technology, healthcare, automobiles, and others. This research explores the advantages of employing deep learning methods to provide individualized assistance technology solutions for people with dysarthria. We proposed a Convolutional Temporal Bidirectional Network (CTBNet) model for translating Russian Hyperkinetic Dysarthria Disorder (RHDD) speech into text. CTBNet encodes input audio features using 1D convolution layers, and BidirectionalGRU (Bi-GRU) layers. The encoder effectively captures both short-term and long-term spatial temporal information. More importantly, CTBNet includes with an attention-CTC decoder, that is responsible for producing output texts. We utilized a dataset of 2797 recordings of RHDD speech, with a cumulative recorded duration of 2 hours and 33 minutes, for training purposes. The CTBNet model has achieved a 15% Character Error Rate (CER) and a 59% Word Error Rate (WER) on the dataset proposed. Our experimental findings show superior performance compared to state-of-the-art models, namely, 3xCNN, and 3xLSTM, when evaluated on the same dataset.",Date Added to IEEEXplore:09 July 2024
10805075,Developing a Hyperkinetic Dysarthria Speech Classification System using Residual Learning,"Hyperkinetic dysarthria abnormalities are exceedingly widespread across the worldwide populace. One of the most effective methodologies for disordered audio speech identification is classical deep learning technique. However, the efficiency of classification is impacted by the limitations imposed by the quality of the training dataset, such as expense and limited resources, data imbalance, and data annotation difficulties. Accordingly, we suggest implementing a residual learning framework (ResN et architecture) with various conditions and hyperparameters (batch size and learning rate) that improve the training process of deeper networks compared to earlier approaches. Pre-emphasis, windowing, and lifting techniques are applied to improve the quality of Mel-Frequency Cepstral Coefficients. In addition, we have introduced the hyperkinetic dysarthria speech dataset in the Russian language, consisting of 4 hours and 4 minutes of recorded speech. The empirical findings demonstrate that ResNet40 has an overall validation accuracy of 86.7%, surpassing other research models. It is anticipated that it will be recognized as an alternative diagnostic instrument for physicians in the future as research continues to progress.",Date Added to IEEEXplore:25 December 2024
10889800,Multilingual Speaker-Invariant Dysarthria Severity Assessment Using Adversarial Domain Adaptation and Self-Supervised Learning,"Traditional assessments for dysarthria are subjective and time-consuming, highlighting the need for automated, objective approaches that can be scaled for remote and resource-constrained environments. This paper introduces an adversarial domain adaptation framework tailored for dysarthria severity assessment, addressing the challenges of high intra-class variability and limited availability of dysarthric speech data. By framing speaker variability as a domain adaptation problem, we utilise an adversarially trained feature extractor to derive speaker-invariant yet discriminatively powerful representations utilising speech features learned through self-supervised learning. Experiments on previously unseen and diverse speakers reveal that the proposed approach yields a 7.86% average improvement across multilingual datasets compared to traditional severity-discriminative training, outperforming competitive baselines by 12.33% on average. Additionally, the method inherently supports privacy-preserving applications by minimising reliance on speaker-specific information. The results demonstrate strong alignment with clinical assessments, reinforcing our model’s clinical relevance and effectiveness.",Date Added to IEEEXplore:07 March 2025
10929844,AI-Enabled Speech Monitoring for Dysarthria Detection in Consumer Electronics,"Speech is essential in human communication, and recent advancements in artificial intelligence (AI) have enabled new applications for voice data, particularly in health monitoring. Building on this progress, a home-based health detection system that utilizes voice signals provides an accessible method for users to independently identify and respond to conditions such as dysarthria, without the need for hospital or clinic visits. In this study, we developed and evaluated convolutional neural network (CNN) models to classify audio data from both healthy individuals and individuals with dysarthria. The dataset comprised command-based speech samples, which were segmented into sentence-specific input. Preprocessing steps included segmentation, downsampling, and feature extraction tailored for CNN input. The models exhibited high potential in distinguishing between normal and dysarthric states, with the most effective model achieving an f1-score of 96.18±4.7% and an accuracy of 96.01±5.02%. Furthermore, sentence structure and composition significantly influenced model performance, with specific sentences demonstrating higher classification accuracy. These findings suggest that AI-driven speech analysis may support detecting and managing specific health conditions in non-clinical environments. However, further work is needed to address data imbalance and sentence variability limitations. Future research could expand on real-world applications, particularly in settings with limited traditional health monitoring.",Date Added to IEEEXplore:26 March 2025
10848959,Dysarthria Severity Classification Using Phase Based Features of LP Residual,"Classifying the severity of speech impairment due to dysarthria is crucial for optimizing care and enhancing communication abilities for affected individuals. This study explores the use of the Modified Group Delay Function (MGDF) of LP residual signal in classifying dysarthria severity-levels. Evaluations were conducted using standard UA-Speech and TORGO datasets. A stratified Convolutional Neural Network (CNN) with 5-fold cross-validation validated the results. Baseline features included Linear Frequency Cepstral Coefficients (LFCC), Mel Frequency Cepstral Coefficients (MFCC), and Whisper module. Key performance evaluation metrics were accuracy, precision, recall, and F1-score. Finally, the latency period was analyzed for practical deployment of the system, system’s ability to accurately recognize and process speech from any speaker, without needing to be specifically trained or adapted to individual voice characteristics.",Date Added to IEEEXplore:27 January 2025
10200180,Automatic Early Detection of Dysarthria using Deep Neural Network,"Dysarthria is dis-coordination among articulatory muscles responsible for articulation in the production of speech. A person with dysarthric disorder cannot control their tongue, hence, are prone to slurred speech or swallowing words which lead to improper pronunciation and less intelligibility of speech. This work focuses on the automatic early detection of Dysarthria. To elucidate the main idea, it must be understood that the whole detection procedure revolves around the speech intelligibility of a person. It becomes an arduous job to distinguish between a healthy person's speech and a person who is slowly evolving with the disorder. The prime motive of the work is to provide a proper classification of the disorder in its earliest stages. The common words and phrases recorded in the Universal Access database are used for this work. Mel Frequency Cepstral Coefficients (MFCC) are extracted from both controlled and high intelligible speech data, and fed to a Deep Neural Network (DNN) classifier. MFCC fed to the DNN classifier has provided the best classification accuracy for the above experimental setup.",Date Added to IEEEXplore:07 August 2023
8682324,Learning to Detect Dysarthria from Raw Speech,"Speech classifiers of paralinguistic traits traditionally learn from diverse hand-crafted low-level features, by selecting the relevant information for the task at hand. We explore an alternative to this selection, by learning jointly the classifier, and the feature extraction. Recent work on speech recognition has shown improved performance over speech features by learning from the waveform. We extend this approach to paralinguistic classification and propose a neural network that can learn a filterbank, a normalization factor and a compression power from the raw speech, jointly with the rest of the architecture. We apply this model to dysarthria detection from sentence-level audio recordings. Starting from a strong attention-based baseline on which mel-filterbanks outperform standard low-level descriptors, we show that learning the filters or the normalization and compression improves over fixed features by 10% absolute accuracy. We also observe a gain over OpenSmile features by learning jointly the feature extraction, the normalization, and the compression factor with the architecture. This constitutes a first attempt at learning jointly all these operations from raw audio for a speech classification task.",Date Added to IEEEXplore:17 April 2019
7078586,Modeling fundamental frequency dynamics in hypokinetic dysarthria,"Hypokinetic dysarthria (Hd), which often accompanies Parkinson's Disease (PD), is characterized by hypernasality and by compromised phonation, prosody, and articulation. This paper proposes automated methods for detection of Hd. Whereas most such studies focus on measures of phonation, this paper focuses on prosody, specifically on fundamental frequency (F0) dynamics. Prosody in Hd is clinically described as involving monopitch, which has been confirmed in numerous studies reporting reduced within-utterance pitch variability. We show that a new measure of F0 dynamics, based on a superpositional pitch model that decomposes the F0 contour into a declining phrase curve and (generally, single-peaked) accent curves, performs more accurate Hd vs. Control classification than simpler versions of the model or than conventional variability statistics.",Date Added to IEEEXplore:02 April 2015
8361563,Automatic detection of speech disorder in dysarthria using extended speech feature extraction and neural networks classification,"This paper presents an automatic detection of Dysarthria, a motor speech disorder, using extended speech features called Centroid Formants. Centroid Formants are the weighted averages of the formants extracted from a speech signal. This involves extraction of the first four formants of a speech signal and averaging their weighted values. The weights are determined by the peak energies of the bands of frequency resonance, formants. The resulting weighted averages are called the Centroid Formants. In our proposed methodology, these centroid formants are used to automatically detect Dysarthric speech using neural network classification technique. The experimental results recorded after testing this algorithm are presented. The experimental data consists of 200 speech samples from 10 Dysarthric speakers and 200 speech samples from 10 age-matched healthy speakers. The experimental results show a high performance using neural networks classification. A possible future research related to this work is the use of these extended features in speaker identification and recognition of disordered speech.",Date Added to IEEEXplore:21 May 2018
9076507,Analysis of Time Domain Features of Dysarthria Speech,"In general, Speech can be described as the ability to express thoughts and feelings by articulating sounds in order to communicate with the person who understand the speech and accordingly interpret their action. The types of communication disorders that affects a person’s ability to produce speech sounds due to the lack of control over motor functions and articulators are speech disorders. Many people of varying age groups suffer from speech disorders and require early treatment in order to correct them. This paper analyses the time domain features such as jitter and shimmer that are extracted from the speech samples of dysarthria and healthy people. This is done by looking for dissimilarities between the different speech features preset in the normal healthy person and the disordered one by employing steps such as pre-processing followed by feature extraction.",Date Added to IEEEXplore:23 April 2020
9291830,Emotional Communication Assist Interface App for People with Dysarthria,"Text to speech (TTS) helps people who have speech disorders communicate with other people. The current TTS can express emotion, but this requires extra time and effort. We developed a TTS application by which users can render messages with emotion with ease by changing the pitch angle of a smartphone. We designed two types of TTS: free input and phrase selection. Also, we calculated the TTS editing parameters for each degree of emotion on the basis of the Online Game Voice Chat Corpus (OGVC).",Date Added to IEEEXplore:21 December 2020
9762324,Automated Dysarthria Severity Classification: A Study on Acoustic Features and Deep Learning Techniques,"Assessing the severity level of dysarthria can provide an insight into the patient’s improvement, assist pathologists to plan therapy, and aid automatic dysarthric speech recognition systems. In this article, we present a comparative study on the classification of dysarthria severity levels using different deep learning techniques and acoustic features. First, we evaluate the basic architectural choices such as deep neural network (DNN), convolutional neural network, gated recurrent units and long short-term memory network using the basic speech features, namely, Mel-frequency cepstral coefficients (MFCCs) and constant-Q cepstral coefficients. Next, speech-disorder specific features computed from prosody, articulation, phonation and glottal functioning are evaluated on DNN models. Finally, we explore the utility of low-dimensional feature representation using subspace modeling to give i-vectors, which are then classified using DNN models. Evaluation is done using the standard UA-Speech and TORGO databases. By giving an accuracy of 93.97% under the speaker-dependent scenario and 49.22% under the speaker-independent scenario for the UA-Speech database, the DNN classifier using MFCC-based i-vectors outperforms other systems.",N/A
10661368,Fuzzy Expectation Maximization Phoneme Prediction in Diffusion Model-based Dysarthria Voice Conversion,"In this paper proposed an effective fuzzy expectation-maximization phoneme prediction method in diffusion model-based dysarthria voice conversion (FEMPPDM-DVC) which is accessible to (i) training without parallel data (ii) converting a longer duration of the audio data (iii) preserves speaker identity. By integrating Fuzzy Expectation-Maximization (FEM) clustering and Diffusion model-based dysarthria voice conversion approach, the proposed method is able gradually generate normal utterances. Feature extraction is performed using Mel-frequency cepstral coefficients (MFCCs), a robust method in acoustic feature extraction in Text-to-Speech systems. Ensures the effectiveness and accuracy, through repeated parameter adjustment using the FEM clustering algorithm. The conversion network is a diffusion model-based structure, which can convert normal utterances to dysarthria utterances in the forward diffusion process. After forward diffusion process, the dysarthria utterance will go through a reverse diffusion process to convert dysarthria voice to normal utterance. Once converted, a GAN-based vocoder is applied to convert mel-frequency spectrogram to waveform. Objective evaluation is conducted on the Saarbrücken Voice Database (SVD) dataset show that FEMDDPM-DVC is able to improve the intelligibility and naturalness of dysarthria utterances in 15 kinds of dysarthria voice. The proposed method is compared with five other dysarthria voice conversion methods, show that the proposed method has the most performance among other method.",Date Added to IEEEXplore:06 September 2024
10912315,Revolutionary Dysarthria Analysis Through Convolutional Neural Networks,"Dysarthria occurs when there are impairments in the muscles necessary for speech, including those in the lips, tongue, voice box, and breathing muscles. This condition can result in unclear speech, even in individuals who are skilled communicators. Dysarthria can range from mild speech difficulties, where speech is hard to understand, to severe cases, where speech is completely unclear. A method for distinguishing between different types of speech involves using recordings and advanced computational techniques such as convolutional neural networks (CNNs). The strengths of CNNs are leveraged to construct a highly effective model for detecting dysarthria in speech samples. The dataset utilized includes recordings from individuals with and without dysarthria. The results indicate that the CNN model performs exceptionally well, achieving precision, recall, and F1-scores of 0.99 for both categories. With an accuracy of 0.99, the model demonstrates a high level of proficiency in differentiating between speech affected by dysarthria and normal speech. These findings highlight the effectiveness and utility of this CNN-based approach in diagnosing and assessing dysarthria. It offers valuable potential for assisting healthcare professionals and enhancing personalized speech therapy interventions.",Date Added to IEEEXplore:13 March 2025
10675073,A Study of Speech Recognition Techniques for Dysarthria Speeches Based on Digit Recognition,"With the introduction of speech recognition technology into people's lives, the application of which is no longer limited to ordinary people, but begins to target special populations, such as patients with dysarthria. It has become a major research challenge to build a high-performance speech recognition system for patients with dysarthria. To address this problem, in this paper the speech data of four patients with dysarthria and four healthy speakers on digital commands are firstly collected, secondly several speech recognition experiments are separately conducted by using Aishell-2 and Wenetspeech, two pre-trained models based on the WeNet open-source architecture, as well as the four major commercial recognition API systems, at the end a method to optimize the dysarthria data by using the so-vits-svc tool is proposed. The experiments show that the current state-of-the-art speech recognition model has a high recognition rate for speech data from the general population, but there is still room for improvement in the recognition performance of speech data from individuals with dysarthria. Later the optimization method for the dysarthria data proposed in this paper effectively improves the quality of speech data. For the two above open-source pre-trained models, the recognition rates have increased by 60% and 67.64 % compared to the original data, respectively.",Date Added to IEEEXplore:24 September 2024
7897299,Fractal features for automatic detection of dysarthria,"Amytrophic lateral sclerosis (ALS) is an incurable neurodegenerative disease. Difficulty articulating speech, dysarthria, is a common early symptom of ALS. Detecting dysarthria currently requires manual analysis of several different speech tasks by pathology experts. This is time consuming and can lead to misdiagnosis. Many existing automatic classification approaches require manually preprocessing recordings, separating individual spoken utterances from a repetitive task. In this paper, we propose a fully automated approach which does not rely on manual preprocessing. The proposed method uses novel features based on fractal analysis. Acoustic and associated articulatory recordings of a standard speech diagnostic task, the diadochokinetic test (DDK), are used for classification. This study's experiments show that this approach attains 90.2% accuracy with 94.2% sensitivity and 85.1% specificity.",Date Added to IEEEXplore:13 April 2017
6190184,Assessing Dysarthria severity using global statistics and boosting,"A new method for automatic assessment of Dysarthria severity is described. It uses the forward selection method (FSM) on global statistics of low-complexity features to find effective feature sets. FSM is embedded in a boosting algorithm that combines multiple weak classifiers to achieve a single strong classifier. Unlike standard boosting, this uses nonlinear class boundaries and unique feature sets per iteration. Results on a 39 speaker dysarthria database are described.",Date Added to IEEEXplore:26 April 2012
10250600,An Intelligent System for Dysarthria Classification of Male and Female Processed Dataset using Sequential Model Parameters,"There are several current studies including various methods for treating dysarthria in various developing countries. Research-based dysarthria treatments take a multidisciplinary approach that encompasses speech therapy, assistive technology, neurosurgery, pharmaceutical therapies, non-invasive brain stimulation, machine learning and AI methods. These methods seek to raise the general quality of life, communication and speaking abilities of dysarthria sufferers. With a 90% accuracy rate, the proposed sequential model was found to have good classification performance for identifying Dysarthria. Researchers have also been looking at the use of Machine Learning (ML) and Artificial Intelligence (AI) approaches to create tools and systems that can help people with dysarthria to speak and communicate for social welfare and provide access to best remedies.",Date Added to IEEEXplore:22 September 2023
7344537,Dysarthria diagnosis via respiration and phonation,This report discusses the implementation of a computerized application - the Computerised Frenchay Dysarthria Assessment Procedure (CFDA) - which uses digital signal processing (DSP) techniques to objectively evaluate digitised speech recordings in order to detect any symptoms of dysarthria (a type of motor speech disorder). This investigation focuses specifically on two CFDA diagnostic sub-applications which assess a patient's ability to maintain a prolonged exhalation (the “Respiration at Rest” task) as well as execute a steady state phonation (i.e. the “Sustained Phonation” task). It is demonstrated that combining the functionality of these two sub-applications substantially increases their diagnostic effectiveness in the context of identifying a particular variety of dysarthria known as spastic dysarthria. It is further demonstrated that certain patterns of energy fluctuations are closely correlated with manifestations of spastic dysarthria. The CFDA is intended for use in real-world conditions to assist speech and language therapists when conducting clinical evaluations.,Date Added to IEEEXplore:03 December 2015
10892620,Deep Learning Based Dysarthria Detection: A Comprehensive Approach,"Making a model that can analyze numerous input features and predict whether a person will develop dysarthria is the first step in utilizing machine learning to forecast diseases like dysarthria. A motor speech disorder called dysarthria can be caused by a variety of underlying conditions, such as degenerative disorders, traumatic brain injuries, or neurological disorders. The authors used the 2000 audio signals from males and females with and without dysarthria from the TORGO data set. To extract the important features from the audio signals, the authors used the MFCC approach. To determine if dysarthria is present or absent, a machine learning model or algorithm will be fed with these MFCC coefficients.",Date Added to IEEEXplore:25 February 2025
9629802,"Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition","In this paper, we propose a deep learning-based algorithm to improve the performance of automatic speech recognition (ASR) systems for aphasia, apraxia, and dysarthria speech by utilizing electroencephalography (EEG) features recorded synchronously with aphasia, apraxia, and dysarthria speech. We demonstrate a significant decoding performance improvement by more than 50% during test time for isolated speech recognition task and we also provide preliminary results indicating performance improvement for the more challenging continuous speech recognition task by utilizing EEG features. The results presented in this paper show the first step towards demonstrating the possibility of utilizing non-invasive neural signals to design a real-time robust speech prosthetic for stroke survivors recovering from aphasia, apraxia, and dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will be released to the public to help further advance this interesting and crucial research.",Date Added to IEEEXplore:09 December 2021
10841469,Multiangle Correlation Feature Extraction and Disease Prediction Model Construction for Patients With Post-Stroke Dysarthria,"The clinical diagnosis and treatment of motor dysarthria in post-stroke patients is often subjective and neglects the impact of psychological and emotional disorders on disease progression. This study aims to analyze the correlation among emotional expression, psychological state, facial expression, and dysarthria disease severity and is dedicated to the construction of a dysarthria prediction model. We first designed THE-POSSD, a novel Chinese multimodal emotional pathology expression database, which collected acoustic, glottal, and facial data under emotional stimuli from patients at different disease stages and healthy controls. Emotional speech was labeled for intelligibility scores, emotion types, and discrete dimensional space. Then, their correlation with disease development was investigated and analyzed. A total of 154 significant correlation features were extracted for analysis. To mitigate the limitations of subjective clinical scale diagnosis and account for psychological and emotional factors, this study introduced the grey correlation theory and constructed a dysarthria prediction model based on the grey relational analysis-deep belief network (GRA-DBN). Principal Component Analysis and Variance Inflation Factor were employed to optimize GRA-DBN model. Both proposed models achieved a high prediction accuracy, with an adjusted R2 value of 0.85 for GRA-DBN and 0.92 for optimised model. This study fills the gap in the international multimodal emotional pathological expression dataset and provides a comprehensive framework for analyzing the association between mental state, emotional expression, and the degree of dysarthria. Furthermore, the incorporation of key multimodal features into the predictive model highlights its potential to enhance the precision of clinical diagnostic processes significantly.",N/A
1660840,Hmm-Based and Svm-Based Recognition of the Speech of Talkers With Spastic Dysarthria,"This paper studies the speech of three talkers with spastic dysarthria caused by cerebral palsy. All three subjects share the symptom of low intelligibility, but causes differ. First, all subjects tend to reduce or delete word-initial consonants; one subject deletes all consonants. Second, one subject exhibits a painstaking stutter. Two algorithms were used to develop automatic isolated digit recognition systems for these subjects. HMM-based recognition was successful for two subjects, but failed for the subject who deletes all consonants. Conversely, digit recognition experiments assuming a fixed word length (using SVMs) were successful for two subjects, but failed for the subject with the stutter.",Date Added to IEEEXplore:24 July 2006
5495563,Design of a dysarthria classifier using global statistics of speech features,"Dysarthria is a neurological disorder in which the speech production system is impaired. There are five main types of dysarthrias depending on the location of the lesion in the nervous system. There is evidence suggesting a relationship between the location of the lesion and the resulting speech characteristics. This paper describes a non-intrusive classifier to identify the dysarthria type in a person using global statistics, e.g., mean, variance, etc., of speech features. A tree-based classifier was developed using multiple low-level maximum likelihood classifiers as inputs. An error of 10.5% was achieved in the classification of three types of dysarthrias.",Date Added to IEEEXplore:28 June 2010
10157285,Dysarthria Speech Disorder Classification Using Traditional and Deep Learning Models,"Dysarthria is a motor speech disorder that results in speech difficulties due to the weakness of associated muscles. This unclear speech makes it difficult for dysarthric patients to present himself understood. This neurological limitation is usually occurs due to damages to the brain or central nervous system. Speech therapy can be effectively employed to enhance the range and consistency of voice production and improve intelligibility and communicative effectiveness. Assessing the degree of severity of dysarthria provides vital information on the patient's progress which inturn assists pathologists in arriving at a treatment plan that includes developing automated voice recognition system suitable for dysarthria patients. This work performs an exhaustive study on dysarthria severity level classification using deep neural network (DNN) and convolution neural network (CNN) architectures. Mel Frequency Cepstral Coefficients (MFCCs) and their derivatives constitute feature vectors for classification. Using the UA-Speech database, the performance metrics of DNN/CNN based learning models have been compared to baseline classifiers like support vector machine (SVM) and Random Forest (RF). The highest classification accuracy of 97.6\% is reported for DNN under UA speech database. A detailed examination of the performance from the models discussed above reveal that appropriate choice of deep learning architecture ensures better results than traditional classifiers like SVM and Random Forest.",Date Added to IEEEXplore:26 June 2023
10527645,Harnessing Deep Learning Techniques for Dysarthria Detection,"Dysarthria, a motor speech disorder resulting from neurological impairments. This study explores various approaches for the automated detection of dysarthria, integrating both traditional and emerging technologies. Speech assessments by acoustic analysis, machine learning models, and deep learning techniques are considered. Machine learning models are trained on datasets containing normal and dysarthric speech for males and females, while deep learning methods, including convolutional and recurrent neural networks, are employed to automatically learn features from speech data. Our goal is to develop a reliable and accessible dysarthria detection system thatcomplements the expertise of healthcare professionals. Validation using diverse datasets is emphasized, acknowledging the importance of early detection and intervention in improving outcomes for individuals with dysarthria.",Date Added to IEEEXplore:16 May 2024
10696797,Next-Gen Speech Disorder Diagnostics: CNN Methods for Dysarthria Classification,"Neurological disorders such as amyotrophic lateral sclerosis (ALS) and cerebral palsy (CP) are common causes of dysarthria, the subject of this research article. Dysarthria has a significant impact on communication capacity, making early diagnosis essential for effective therapies and improved patient outcomes. The research used a Convolutional Neural Network (CNN) to categorise speech using the TORGO database, which includes 2000 samples of individuals with and without dysarthria, representing different genders. There are 500 samples from each gender in the collection, representing dysarthric and non-dysarthric people who were recorded during different sessions. Highlighting the promise of deep learning technology in aiding early identification of dysarthria and enabling rapid therapy for affected individuals, the CNN-based approach exhibits an amazing 96% accuracy in discriminating between dysarthric and non-dysarthric cases. This study offers a promising chance to enhance the quality of life for those with speech impairments while also making substantial contributions to the development of diagnostic capabilities.",Date Added to IEEEXplore:04 October 2024
10730941,Advancing Speech Disorder Diagnostics: CNN Innovations in Dysarthria Recognition,"Neurological disorders such as amyotrophic lateral sclerosis (ALS) and cerebral palsy (CP) are common causes of dysarthria, the subject of this research article. The capacity to communicate is greatly affected by dysarthria, thus early diagnosis is critical for effective therapies and improved patient outcomes. The study employed a Convolutional Neural Network (CNN) for speech classification using the TORGO database, which includes 2000 samples of individuals with and without dysarthria, representing different genders. With 500 samples each, collected over distinct sessions, the dataset contains both dysarthric and non-dysarthric people of both sexes. Distinguishing between dysarthric and non-dysarthric occurrences, the CNN-based approach achieves an astonishing 96% accuracy. This shows how deep learning technology may help with early dysarthria identification and enable rapid therapy for affected individuals. The findings of this study have important implications for the development of diagnostic tools and may lead to better living conditions for those who struggle with speaking.",Date Added to IEEEXplore:29 October 2024
10340908,A preliminary study on self-care telemonitoring of dysarthria in spinal muscular atrophy,"Spinal muscular atrophy (SMA) is a rare neuromuscular disease which may cause impairments in oro-facial musculature. Most of the individuals with SMA present bulbar signs such as flaccid dysarthria which mines their abilities to speak and, as consequence, their psychic balance. To support clinicians, recent work has demonstrated the feasibility of video-based techniques for assessing the oro-facial functions in patients with neurological disorders such as amyotrophic lateral sclerosis. However, no work has so far focused on automatic and quantitative monitoring of dysarthria in SMA. To overcome limitations this work’s aim is to propose a cloud-based store-and-forward telemonitoring system for automatic and quantitative evaluation of oro-facial muscles in individuals with SMA. The system integrates a convolutional neural network (CNN) aimed at identifying the position of facial landmarks from video recordings acquired via a web application by an SMA patient.Clinical relevance— The proposed work is in the preliminary stage, but it represents the first step towards a better understanding of the bulbar-functions’ evolution in patients with SMA.",Date Added to IEEEXplore:11 December 2023
9414283,"Automatic And Perceptual Discrimination Between Dysarthria, Apraxia of Speech, and Neurotypical Speech","Automatic techniques in the context of motor speech disorders (MSDs) are typically two-class techniques aiming to discriminate between dysarthria and neurotypical speech or between dysarthria and apraxia of speech (AoS). Further, although such techniques are proposed to support the perceptual assessment of clinicians, the automatic and perceptual classification accuracy has never been compared. In this paper, we investigate a three-class automatic technique and a set of handcrafted features for the discrimination of dysarthria, AoS and neurotypical speech. Instead of following the commonly used One-versus-One or One-versus-Rest approaches for multi-class classification, a hierarchical approach is proposed. Further, a perceptual study is conducted where speech and language pathologists are asked to listen to recordings of dysarthria, AoS, and neurotypical speech and decide which class the recordings belong to. The proposed automatic technique is evaluated on the same recordings and the automatic and perceptual classification performance are compared. The presented results show that the hierarchical classification approach yields a higher classification accuracy than baseline One-versus-One and One-versus-Rest approaches. Further, the presented results show that the automatic approach yields a higher classification accuracy than the perceptual assessment of speech and language pathologists, demonstrating the potential advantages of integrating automatic tools in clinical practice.",Date Added to IEEEXplore:13 May 2021
10480411,The Detection of Dysarthria Severity Levels Using AI Models: A Review,"Dysarthria, a speech disorder stemming from neurological conditions, affects communication and life quality. Precise classification and severity assessment are pivotal for therapy but are often subjective in traditional speech-language pathologist evaluations. Machine learning models offer objective assessment potential, enhancing diagnostic precision. This systematic review aims to comprehensively analyze current methodologies for classifying dysarthria based on severity levels, highlighting effective features for automatic classification and optimal AI techniques. We systematically reviewed the literature on the automatic classification of dysarthria severity levels. Sources of information will include electronic databases and grey literature. Selection criteria will be established based on relevance to the research questions. The findings of this systematic review will contribute to the current understanding of dysarthria classification, inform future research, and support the development of improved diagnostic tools. The implications of these findings could be significant in advancing patient care and improving therapeutic outcomes for individuals affected by dysarthria.",N/A
10932720,Automatic Assessment of Chinese Dysarthria Using Audio-visual Vowel Graph Attention Network,"Automatic assessment of dysarthria remains a highly challenging task due to the high heterogeneity in acoustic signals and the limited data. Currently, research on the automatic assessment of dysarthria primarily focuses on two approaches: one that utilizes expert features combined with machine learning, and the other that employs data-driven deep learning methods to extract representations. Studies have shown that expert features can effectively account for the heterogeneity of dysarthria but may lack comprehensiveness. In contrast, deep learning methods excel at uncovering latent features. Therefore, integrating the advantages of expert knowledge and deep learning to construct a neural network architecture based on expert knowledge may be beneficial for interpretability and assessment performance. In this context, the present paper proposes a vowel graph attention network based on audio-visual information, which effectively integrates the strengths of expert knowledge and deep learning. Firstly, the VGAN (Vowel Graph Attention Network) structure based on vowel space theory was designed, which has two branches to mine the information in features and the spatial correlation between vowels respectively. Secondly, a feature set based on expert knowledge and deep representation is designed. Finally, visual information was incorporated into the model to further enhance its robustness and generalizability. Tested on the Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) Database, this method exhibited superior performance in regression experiments targeting Frenchay scores compared to existing approaches.",N/A
10832239,Summary of Low-Resource Dysarthria Wake-Up Word Spotting Challenge,"In recent years, the rapid advancement and widespread adoption of speech technology have made smart home systems a common feature in many households. However, individuals with dysarthria face difficulties using these technologies due to inconsistent speech patterns. This paper summarizes the Low-Resource Dysarthria Wake-Up Word Spotting (LRDWWS) Challenge at SLT 2024, which aimed to develop effective voice wake-up systems for individuals with dysarthria. The challenge attracted 25 teams from 4 countries, with 7 teams submitting results and 5 providing detailed system descriptions. This paper presents an overview of the dataset, evaluation metrics, and key innovations from participating teams. Our findings highlight the potential of these systems to enhance the accessibility and usability of smart home technologies for individuals with dysarthria. The challenge results underscore the importance of developing specialized solutions to meet the unique needs of this user group.",Date Added to IEEEXplore:16 January 2025
10724295,Transformer-based Transfer Learning for Enhanced Speech Dysarthria Severity Assessment,"Dysarthria, a neuromuscular communications disorder presented with impaired pronunciation, is a daunting task to identify and quantify the level of dysfunction. Through this paper, a critical study of automated dysarthria severity classification using transformer-based deep learning methods will be discussed. Transfer learning is facilitated by employing three variations of Vision Transformer (ViT) models: ViT-L-16, ViT-L-32, and ViT-B-16, on UA-Speech Corpus and TORGO datasets, achieving remarkable results with 99.01% accuracy for the UA-Speech dataset and 99.39% accuracy for the TORGO database. The study evaluates the models’ performance focusing on accuracy, precision, recall, and F1-scores. Experimental results emphasise the potential for automated diagnostic units in neurology clinical practice and establish a baseline for leading work in dysarthria severity classification with the selection of an efficient ViT model.",Date Added to IEEEXplore:04 November 2024
8117233,Enhancing speech rate estimation techniques to improve dysarthria diagnosis,"This report discusses the implementation of a computerized algorithm specifically designed to measure the syllables-per-minute rate of abnormal speech typically produced by persons suffering from an articulatory disorder known as dysarthria. This speech rate measurement application - which can also serve as a diagnostic tool in itself - has been integrated into the computerised Frenchay Dysarthria Assessment (CFDA) suite of diagnostic tests. It is demonstrated that, when processing dysarthric speech, syllables-per-minute measurements are more accurate when based on vowel transition detection techniques as opposed to using spectral moment computations.",Date Added to IEEEXplore:23 November 2017
9508522,Personalizing TTS Voices for Progressive Dysarthria,"Amyotrophic lateral sclerosis (ALS) patients experience progressive speech deterioration due to muscle paralysis, leading to eventual loss of verbal communication capability. Text-to-speech synthesis (TTS) is an important technology for speech generating devices, enabling users to communicate using generic electronic voices, but often without the vocal identity of the users. Our work is aimed at personalizing TTS voices for people with ALS induced dysarthria by integrating machine learning and speech processing techniques of voice conversion (VC) and TTS. This is challenging as only small quantities of dysarthric speech are available from individual patients. Our system includes both timbre and prosody conversion for VC, neural TTS to generate TTS speech, and neural feature converter to interface VC and TTS. We collected speech data from 4 ALS target speakers with mild to severe dysarthria. Subjective listening tests showed that on average, our approach improved speech intelligibility by about 72% over the target speakers’ speech, the converted voice was 2 to 3 times more similar to ALS targets than to TTS sources, and the converted speech quality was in the MOS scale of fair to good.",Date Added to IEEEXplore:10 August 2021
9336404,A Phonological Control Method on A Speech Compensation System for Dysarthria Using A Standardized Space,"We have developed a speech compensation system for dysarthria. The system aims at improving the phonological properties of vowels without losing speaker individuality. We propose a method for phonological control of vowels using a standardized space to control vowels in the normalized articulation space, normalized for speaker individuality. The method maps an original dysarthric speaker's normalized articulation space to a standardized space, then from the standardized space to the target speaker's normalized articulation space assuming normality to improve the phonological properties of vowels. We confirm phonological control of vowels by performing a processing simulation, comparison different target speakers and a processing simulation using a dummy original speaker as a dysarthria.",Date Added to IEEEXplore:02 February 2021
10808415,Japanese Vowel-mora Visualization for Dysarthria Rehabilitation with Variational Autoencoder,"This work proposes a Variational AutoEncoder (VAE)-based rehabilitation framework that visualizes the vowel-mora for Japanese dysarthria. Traditionally, Speech-Language Pathology (SLP) has shown the guideline of rehabilitation for dysarthria, but they should rely only on clinical experience and case-by-case adaption, highlighting the urgent necessity to push the boundary for showing a subjective guideline, which does not depend on the perspective of SLPs. The proposed framework takes advantage of two-dimensional latent representations of vowel-mora, which is assumed to be pre-processed by mel-spectrogram, via VAE. The experiments highlight the effectiveness of our proposed framework.",Date Added to IEEEXplore:27 December 2024
9648166,Artificial Intelligence for Dysarthria Assessment in Children With Ataxia: A Hierarchical Approach,"Early onset ataxia represents a group of heterogeneous neurological conditions typically characterized by motor disability. Speech problems are one of the main core features of ataxic syndromes, where automatic and computational characterization of speech impairment might represent a source of biomarkers for early screening and stratification of patients. The main contribution of this paper consists in proposing a novel hierarchical machine learning model (HMLM) to improve detection and assessment of dysarthria from a structured speech disturbance test. Performances are tested on a new audio dataset containing 10 seconds recordings of standardized clinical PATA test for 55 subjects: 18 healthy subjects and 37 with ataxia. Results show that the proposed HMLM achieves performances with an accuracy of about 90% at the first level (healthy vs patients) selecting an optimal subset of conventional features. In cascade, at the second level, speech disturbance severity (Low vs High) is assessed using deep learning feature extraction technique based on a VGG pre-trained network with maximum accuracy of about 80%. Both levels are processed through the majority voting ensemble technique testing Support Vector Machine (SVM), k-Nearest Neighbors (kNN), Decision Tree (DT) and Naïve Bayes (NB). In our results, the use of HMLM considerably outperforms the results achieved with a single machine learning or deep learning modeling. These outcomes demonstrate that the investigation of the PATA speech test through HMLM can be considered very promising. We also observed that the use of conventional feature extraction techniques and machine learning modeling seems to be a good solution for the diagnosis of patients with ataxia, while the deep learning approach is more appropriate for stratification of severity of dysarthria.",N/A
10419588,Machine Learning Approaches for Automated Detection and Classification of Dysarthria Severity,"Dysarthria, a speech disorder caused by neuro-motor problems resulting in impaired articulation, requires an assessment of its severity for diagnostic and monitoring purposes. Additionally, accurate severity classification facilitates the development of automated dysarthric speech detection and classification systems. This paper presents a comprehensive investigation into detecting dysarthric voices within a collection of normal voice samples, followed by the dysarthria severity classification utilizing neural network frameworks, specifically long short-term memory network (LSTM) and recurrent neural network (RNN). The study employs various features including Mel frequency cepstral coefficients (MFCC), formants, prosodic parameters, and voice quality. The performance of these models is evaluated against a baseline support vector machine (SVM) classifier using the Nemours corpus database. Remarkably, the highest classification accuracy achieved for this corpus is 99.69%. Detailed analysis demonstrates that selecting an appropriate neural network architecture yields superior performance compared to the conventional SVM classifier.",Date Added to IEEEXplore:13 February 2024
10800436,"The ISCSLP 2024 Multimodal Dysarthria Severity Assessment (MDSA) Challenge: Dataset, Tracts, Baseline and Results","To advance multimodal speech assessment and related research in developing objective diagnostic methods, we are launching the Multimodal Dysarthria Severity Assessment (MDSA) Challenge. This paper summarizes the outcomes from the ISCSLP 2024 MDSA Challenge. We first address the necessity of the challenge and then introduce the associated audio-video dataset selected from the MSDM database, including 62 subacute stroke patients and 25 normal controls. We then describe the challenge arrangement and the baseline system. Specifically, we set up a four-classification task for the severity of dysarthria (normal, mild, moderate and severe) with the aim of developing an objective and accurate automatic assessment method to assist in clinical diagnosis and treatment. Finally we summarize the challenge results and provide the major observations from the submitted systems. We hope the open data and challenge will serve as a benchmark and common test-bed for pathological speech assessment.",Date Added to IEEEXplore:23 December 2024
10097195,An Analysis of Degenerating Speech Due to Progressive Dysarthria on ASR Performance,"Although personalized automatic speech recognition (ASR) models have recently been improved to recognize even severely impaired speech, model performance may degrade over time for persons with degenerating speech. The aims of this study were to (1) analyze the change of performance of ASR over time in individuals with degrading speech, and (2) explore mitigation strategies to optimize recognition throughout disease progression. Speech was recorded by four individuals with degrading speech due to amyotrophic lateral sclerosis (ALS). Word error rates (WER) across recording sessions were computed for three ASR models: Unadapted Speaker Independent (U-SI), Adapted Speaker Independent (A-SI), and Adapted Speaker Dependent (A-SD or personalized). The performance of all models degraded significantly over time as speech became more impaired, but the A-SD model improved markedly when updated with recordings from the severe stages of speech progression. Recording additional utterances early in the disease before significant speech degradation did not improve the performance of A-SD models. This emphasizes the importance of continuous recording (and model retraining) when providing personalized models for individuals with progressive speech impairments.",Date Added to IEEEXplore:05 May 2023
10698732,Dysarthria Voice Disorder Detection Using Mel Frequency Logarithmic Spectrogram and Deep Convolution Neural Network,"Dysarthric speech recognition (DSR), often known as DSR, is an important tool that enables persons with vocal impairments to participate in voice-based automation systems and human-computer interaction. As a result of the poor intelligibility of handicapped speakers, the limited availability of datasets, and the low intra-class and inter-class variability in the speech samples, DSR is an essential component. This research provides a DSR based on the Mel Frequency Logarithmic Spectrogram (MFLS) and Deep Convolutional Neural Network (DCNN). The suggested MFLS+DCNN offers an improved representation of the voice signal in terms of its spectral and temporal characteristics. The results of the MFLS-DCNN scheme are validated using the UASpeech dataset, which was developed based on accuracy, recall, precision, and F1-score. With an accuracy of 96.83%, a recall performance of 0.97, a precision performance of 0.96, and an F1-score of 0.97, the scheme that is proposed has shown a considerable improvement above the conventional state-of-the-art.",Date Added to IEEEXplore:08 October 2024
10094857,Wav2vec-Based Detection and Severity Level Classification of Dysarthria From Speech,"Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).",Date Added to IEEEXplore:05 May 2023
9054492,Improved Speaker Independent Dysarthria Intelligibility Classification Using Deepspeech Posteriors,"Individuals with dysarthria are unable to control rapid movement of the velum leading to reduction in intelligibility, audibility, naturalness and efficiency of vocal communication. Automatic intelligibility assessment of dysarthric patients allows clinicians diagnose the impact of therapy and medication and also to plan future course of action. Earlier works have concentrated on building speaker dependent machine learning systems for intelligibility assessment, due to limited availability of data. However, a speaker independent assessment system is of greater use by clinicians. Motivated by this observation, we propose a speaker independent intelligibility assessment system which relies on a novel set of features obtained by processing the output of DeepSpeech, an end to end Speech-to-Text engine. All experiments have been performed on the Universal Access Speech database. An accuracy of 53.9% was obtained using Support Vector Machine based four-class classification system for the speaker independent scenario while the accuracy obtained for the speaker dependent scenario is 97.4%.",Date Added to IEEEXplore:09 April 2020
10832235,PB-LRDWWS System For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge,"For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting (LRDWWS) Challenge, we introduce the PB-LRDWWS system. This system combines a dysarthric speech content feature extractor for prototype construction with a prototype-based classification method. The feature extractor is a fine-tuned HuBERT model obtained through a three-stage fine-tuning process using cross-entropy loss. This fine-tuned HuBERT extracts features from the target dysarthric speaker’s enrollment speech to build prototypes. Classification is achieved by calculating the cosine similarity between the HuBERT features of the target dysarthric speaker’s evaluation speech and prototypes. Despite its simplicity, our method demonstrates effectiveness through experimental results. Our system achieves second place in the final Test-B of the LRDWWS Challenge.",Date Added to IEEEXplore:16 January 2025
7334306,On the use of array learners towards Automatic Speech Recognition for dysarthria,"Providing Automatic Speech Recognition (ASR) systems for dysarthria is a challenging task since the normal and the disabled speech have different attributes; hence, using ASR systems designed and trained for normal speakers is not an effective approach. It is important to craft ASR technologies specifically for the speech disabled. Nonetheless, because of the complexity and variability of dysarthric speech, previous studies failed to achieve adequate performance. In this paper we investigated the applications of array learners towards dysarthric speech recognition. The array was implemented by several neural networks that configured to work in parallel. The proposed approach was verified by using the speech materials of seven dysarthric subjects with speech intelligibility from 2% to 86%. For comparison, the results were compared with a dysarthric ASR based on the legacy single-learner approach as the reference model. It is shown that the array learner-based dysarthric ASR improved the mean word recognition rate of 10.41% over the reference model, and decreased the error rate of 4.84%.",Date Added to IEEEXplore:23 November 2015
6563972,Automated assessment and treatment of speech rate and intonation in dysarthria,"Prosody assessment and treatment in dysarthria is clinically relevant, since prosodic impairment can have a negative impact on speech intelligibility and thus on participation in daily life conversation. We propose a speech-technology based software tool that provides automated numerical and visual feedback on two important aspects of prosody: speech rate and intonation. The tool includes speech rate and intonation algorithms, both specifically developed for the analysis of Dutch dysarthric speech. The tool enables speech-language pathologists to obtain objective measures of these prosodic aspects in a standardized and fast way, and enables dysarthric speakers to practise their prosodic skills intensively without the presence of a speech-language pathologist being required.",Date Added to IEEEXplore:22 July 2013
8076086,Identification of hypokinetic dysarthria using acoustic analysis of poem recitation,"Up to 90% of patients with Parkinson's disease (PD) suffer from hypokinetic dysarthria (HD). In this work, we analysed the power of conventional speech features quantifying imprecise articulation, dysprosody, speech dysfluency and speech quality deterioration extracted from a specialized poem recitation task to discriminate dysarthric and healthy speech. For this purpose, 152 speakers (53 healthy speakers, 99 PD patients) were examined. Only mildly strong correlation between speech features and clinical status of the speakers was observed. In case of univariate classification analysis, sensitivity of 62.63 % (imprecise articulation), 61.62% (dysprosody), 71.72% (speech dysfluency) and 59.60% (speech quality deterioration) was achieved. Multivariate classification analysis improved the classification performance. Sensitivity of 83.42% using only two features describing imprecise articulation and speech quality deterioration in HD was achieved. We showed the promising potential of the selected speech features and especially the use of poem recitation task to quantify and identify HD in PD.",Date Added to IEEEXplore:23 October 2017
9091571,The Application Analysis of Neural Network Techniques on Lexical Tone Rehabilitation of Mandarin-Speaking Patients With Post-Stroke Dysarthria,"The Objectives of this study are (1) to evaluate tone production in Mandarin-speaking patients with post-stroke dysarthria (PSD) using an artificial neural network (ANN), (2) to investigate the efficacy of recognition performance of the ANN model contrast to the human listeners and the convolutional neural network (CNN) model, and (3) to explore rehabilitation application of the artificial intelligence recognition for lexical tone production disorder with PSD. The subjects include two groups of native Mandarin speaking adults: 31 patients with PSD and 42 normal-speaking adults (NA) in a similar age range as controls. Each subject was recorded producing a list of 7 Mandarin monosyllables with 4 tones (i.e., a total of 28 tokens). The fundamental frequency (F0) of each monosyllable was extracted using auto-correlation algorithm. The ANN was trained with F0 data of the tone tokens from the NA, to generate the final model. The recognition rates of the human ears, ANN model, and CNN model were 87.78% ± 8.96% (mean ± SD), 89.11% ±11.80%, 65.91% ± 8.79% respectively for tone production of NA group; 70.28% ± 17.61%, 63.35% ± 17.40%, 34.71% ± 6.92% respectively for tone production of PSD group. For PSD group, there was significant correlation between the performance of the ANN model and human listeners (r = 0.826, P <; 0.001). However, the performance of CNN model was not correlated with that of the human ears (r = -0.108, P = 0.562). Thus, the experiments show that ANN is more objective and efficient, which could replace human listeners in the assessment of lexical tone production disorder in Mandarin-speaking patients with PSD. Furthermore, using ANN may reduce the heterogeneity of rehabilitation evaluation among different speech therapists and may give the feedback for achievement of rehabilitation treatment more accurately.",N/A
8076085,Assessing freezing of gait in parkinson's disease using analysis of hypokinetic dysarthria,"Hypokinetic dysarthria (HD) and freezing of gait (FOG) are frequent symptoms of Parkinson's disease (PD). The aim of this work is to reveal pathological mechanisms common for HD and FOG, and use acoustic analysis of dysarthric speech to assess the gait difficulties in PD. We used a correlation analysis to investigate a relationship between speech features and FOG evaluated by freezing of gait questionnaire (FOG-Q). We found speech features quantifying reduced mobility of the articulatory organs significantly correlated with all parts of the questionnaire. Next, we built multivariate regression models to estimate the FOG-Q total score. With this approach, mean estimation error rate of 14.71% was achieved. We confirmed the previous findings of a close relationship between HD and FOG in PD. Furthermore, we showed it is possible to accurately (with the error of approximately 0.5 points) estimate FOG-Q using a reasonable number of conventional speech features.",Date Added to IEEEXplore:23 October 2017
10662779,SARNet: Speaker-Attentive ResNet for Quantification of Dysarthria Severity,"Analyzing differences in audio data of individuals with articulation disorders from the perspective of human speech and employing objective methods for automated dysarthria evaluation can significantly aid doctors in early patient screening and diagnosis. This proactive approach enables timely intervention and treatment during the initial stages of the condition. This paper presents a speech recognition network(named Speaker-Attentive ResNet (SARNet)) that aggregates and propagates features from different hierarchical levels using an attention-based statistical pooling module, built upon the ResNet architecture. This network aims to extract subtle features specific to individual speakers. The proposed method is evaluated using the TORGO dysarthric speech database, employing eight different acoustic features as well as features aggregated from these eight. Comparative experiments with two other models, Time-Delay Neural Network (TDNN) and Panns-CNN10(Large-Scale Pretrained Audio Neural Networks), demonstrate that due to the superior ability of MFCC(Mel-scaleFrequency Cepstral Coefficients) to emulate human auditory features, the proposed approach achieves a classification accuracy of98%−99%in measuring speech intelligibility in healthy people, patients, and the severity of patient’s articulation disorders.",Date Added to IEEEXplore:17 September 2024
9754798,Data Augmentation for Dysarthric Speech Recognition Based on Text-to-Speech Synthesis,"In the field of automatic speech recognition (ASR) for people with dysarthria, it is problematic that not enough training speech data can be collected from people with dysarthria. To solve this problem, we propose a method of data augmentation using text-to-speech (TTS) synthesis. In the proposed data augmentation method, a deep neural network (DNN)-based TTS model is trained by utilizing speech data recorded from a speaker with dysarthria, and the trained TTS model is then used to generate the speaker’s speech data for training the ASR model for the speaker. The results of a speech recognition experiment on a person having spinal muscular atrophy (SMA) showed that the speech recognition error rate was improved by using the proposed data augmentation.",Date Added to IEEEXplore:14 April 2022
10482956,Analysis and Classification of Dysarthric Speech,"Classifying dysarthria using neural networks is challenging due to several factors inherent to the nature of dysarthria, the complexity of speech signals, and the requirements of effective machine learning models. Impaired speech classification is challenging for two main reasons: firstly, the data is scarce, and secondly, it is heterogeneous. In this paper, we have trained two different architectures on a dysarthric speech database. A comparison of results shows that according to precision, recall, f1-score, and accuracy, the Deep Neural Network (DNN) model outperforms the classical Convolutional Neural Network (CNN) model, even with a small database. For people with dysarthria, a DNN can improve the performance metrics during the classification of impaired speech by 99% compared to the classical architecture. This improvement is more than that provided by CNN. We have used the TORGO database for this work.",Date Added to IEEEXplore:02 April 2024
10760680,Speech Recognition for a Person With Cerebral Palsy Using Whisper Fine-Tuned on Japanese and English Dysarthric Speech,"People with cerebral palsy often have dysarthria, and this makes it hard for them to speak as they wish. In this paper, we present an automatic speech recognition (ASR) model for a person with cerebral palsy based on Whisper. Whisper is highly accurate when performing speech recognition for Japanese speech, but recognition accuracy for Japanese dysarthric speech tends to be low. One possible solution to this problem is to fine-tune Whisper using Japanese dysarthric speech. However, there is a problem in that it is difficult to record a large amount of speech for people with dysarthria. Therefore, in our proposed method, English dysarthric speech is utilized for training Whisper in addition to Japanese dysarthric speech. The results of our proposed method showed an improvement in the error rate of about 1% compared to using only Japanese dysarthric speech as training data.",Date Added to IEEEXplore:28 November 2024
8856560,A joint-feature learning-based voice conversion system for dysarthric user based on deep learning technology,"Dysarthria speakers suffer from poor communication, and voice conversion (VC) technology is a potential approach for improving their speech quality. This study presents a joint feature learning approach to improve a sub-band deep neural network-based VC system, termed J_SBDNN. In this study, a listening test of speech intelligibility is used to confirm the benefits of the proposed J_SBDNN VC system, with several well-known VC approaches being used for comparison. The results showed that the J_SBDNN VC system provided a higher speech intelligibility performance than other VC approaches in most test conditions. It implies that the J_SBDNN VC system could potentially be used as one of the electronic assistive technologies to improve the speech quality for a dysarthric speaker.",Date Added to IEEEXplore:07 October 2019
8884185,Spectro-Temporal Representation of Speech for Intelligibility Assessment of Dysarthria,"Recently, spectro-temporal representation of speech has been used in many fields of speech processing. Owing to this, we explore the use of spectro-temporal representation for speech intelligibility assessment especially for dysarthric speech. In this work, we investigate the use of spectro-temporal representations to evaluate intelligibility levels using artificial neural network (ANN) and convolutional neural network (CNN). Standard American English dysarthric databases namely Universal Access and TORGO are used for evaluation. Performance of CNN classifier is superior to ANN as it is an advanced classifier. Further, use of Time-Frequency CNN configuration proved to capture spectro-temporal variations together resulting in an improved performance compared to either Time-CNN or Frequency-CNN configurations which capture either temporal or spectral variations respectively.",N/A
8892556,Knowledge Transferability Between the Speech Data of Persons With Dysarthria Speaking Different Languages for Dysarthric Speech Recognition,"In this paper, we present an end-to-end speech recognition system for Japanese persons with articulation disorders resulting from athetoid cerebral palsy. Because their utterance is often unstable or unclear, speech recognition systems struggle to recognize their speech. Recent deep learning-based approaches have exhibited promising performance. However, these approaches require a large amount of training data, and it is difficult to collect sufficient data from such dysarthric people. This paper proposes a transfer learning method that transfers two types of knowledge corresponding to the different datasets: the language-dependent (phonetic and linguistic) characteristic of unimpaired speech and the language-independent characteristic of dysarthric speech. The former is obtained from Japanese non-dysarthric speech data, and the latter is obtained from non-Japanese dysarthric speech data. In the proposed method, we pre-train a model using Japanese non-dysarthric speech and non-Japanese dysarthric speech, and thereafter, we fine-tune the model using the target Japanese dysarthric speech. To handle the speech data of the two different languages in one model, we employ language-specific decoder modules. Experimental results indicate that our proposed approach can significantly improve speech recognition performance compared with other approaches that do not use additional speech data.",N/A
7454559,A multi-smartwatch system for assessing speech characteristics of people with dysarthria in group settings,"Speech-language pathologists (SLPs) frequently use vocal exercises in the treatment of patients with speech disorders. Patients receive treatment in a clinical setting and need to practice outside of the clinical setting to generalize speech goals to functional communication. In this paper, we describe the development of technology that captures mixed speech signals in a group setting and allows the SLP to analyze the speech signals relative to treatment goals. The mixed speech signals are blindly separated into individual signals that are preprocessed before computation of loudness, pitch, shimmer, jitter, semitone standard deviation and sharpness. The proposed method has been previously validated on data obtained from clinical trials of people with Parkinson disease and healthy controls.",Date Added to IEEEXplore:19 April 2016
9287502,Weak Speech Supervision: A case study of Dysarthria Severity Classification,"Machine Learning methodologies are making a remarkable contribution, and yielding state-of-the-art results in different speech domains. With this exceptionally significant achievement, a large amount of labeled data is the largest bottleneck in the deployment of these speech systems. To generate massive data, hand-labeling training data is an intensively laborious task. This is problematic for clinical applications where obtaining such data labeled by speech pathologists is expensive and time-consuming. To overcome these problems, we introduce a new paradigm called Weak Speech Supervision (WSS), a first-of-its-kind system that helps users to train state-of-the-art classification models without hand-labeling training data. Users can write labeling functions (i.e., weak rules) to generate weak data from the unlabeled training set. In this paper, we provide the efficiency of this methodology via showing the case study of the severity-based binary classification of dysarthric speech. In WSS, we train a classifier on trusted data (labeled with 100% accuracy) via utilizing the weak data (labeled using weak supervision) to make our classifier model more efficient. Analysis of the proposed methodology is performed on Universal Access (UA) corpus. We got on an average 35.68% and 43.83% relative improvement in terms of accuracy and F1-score w.r.t. baselines, respectively.",Date Added to IEEEXplore:18 December 2020
8683803,End-to-end Dysarthric Speech Recognition Using Multiple Databases,"We present in this paper an end-to-end automatic speech recognition (ASR) system for a person with an articulation disorder resulting from athetoid cerebral palsy. In the case of a person with this type of articulation disorder, the speech style is quite different from that of a physically unimpaired person, and the amount of their speech data available to train the model is limited because their burden is large due to strain on the speech muscles. Therefore, the performance of ASR systems for people with an articulation disorder degrades significantly. In this paper, we propose an end-to-end ASR framework trained by not only the speech data of a Japanese person with an articulation disorder but also the speech data of a physically unimpaired Japanese person and a non-Japanese person with an articulation disorder to relieve the lack of training data of a target speaker. An end-to-end ASR model encapsulates an acoustic and language model jointly. In our proposed model, an acoustic model portion is shared between persons with dysarthria, and a language model portion is assigned to each language regardless of dysarthria. Experimental results show the merit of our proposed approach of using multiple databases for speech recognition.",Date Added to IEEEXplore:17 April 2019
10095981,On Using the UA-Speech and Torgo Databases to Validate Automatic Dysarthric Speech Classification Approaches,"Although the UA-Speech and TORGO databases of control and dysarthric speech are invaluable resources made available to the research community with the objective of developing robust automatic speech recognition systems, they have also been used to validate a considerable number of automatic dysarthric speech classification approaches. Such approaches typically rely on the underlying assumption that recordings from control and dysarthric speakers are collected in the same noiseless environment using the same recording setup. In this paper, we show that this assumption is violated for the UA-Speech and TORGO databases. Using voice activity detection to extract speech and non-speech segments, we show that the majority of state-of-the-art dysarthria classification approaches achieve the same or a considerably better performance when using the non-speech segments of these databases than when using the speech segments. These results demonstrate that such approaches trained and validated on the UA-Speech and TORGO databases are potentially learning characteristics of the recording environment or setup rather than dysarthric speech characteristics. We hope that these results raise awareness in the research community about the importance of the quality of recordings when developing and evaluating automatic dysarthria classification approaches.",Date Added to IEEEXplore:05 May 2023
10128416,Prediction of Parkinson’s Disease Using Machine Learning Based on Vocal Frequency,"Parkinson’s disease (PD) is a neurological disorder that affects most people after Alzheimer’s.The ageing neurodegenerative condition leads to PD that reduces dopamine levels in the brain are a defining feature. Tremor, stiffness, bradykinesia, and postural instability are the core characteristics of PD. PD patient’s(PWP) quality of life is impacted by both motor and non-motor symptoms, which may also have an indirect impact on family and caregivers. The patient’s quality of life can be improved and maintained with an early PD diagnosis. There is currently no cure, however there are therapies to control the illness, such as dopaminergic medications. Speech problems associated with PD are categorized under the term hypokinetic dysarthria for the diagnosis of PD.The traditional approach is based on their clinical history and also with their physical examination. With the help of automatic analysis tools, practitioners may diagnose patients, monitor their progress, and conduct regular, economical assessments that are objective. This study uses a machine learning approach to conduct a pilot experiment to identify the existence of dysarthria in speech and gauge its severity. With the help of SVM Algorithm and HyperTunningdisplays 93% accuracy on KAGGLE database test samples.",Date Added to IEEEXplore:24 May 2023
6121505,Acoustic Space in Motor Disorders of Speech: Two Case Studies,"Studies on acoustic space have strengthened the view that vowels are acoustically and perceptually defined in terms of their relative positioning in vowel space. Every speaker identifies an optimal vowel space within which perceptual, phonological contrast is maintained. This is an interdisciplinary study involving speech pathology, physics of speech and neurology of speech. Two case studies of dysarthria presented in this paper are -- one Parkinson's disease and one case of acute ischemic stroke with age-gender-language matched controls. A detailed acoustic analysis shows how acoustic space gets considerably reduced, in both PD and stroke, and in these two very different kinds of dysarthrias the acoustic space is also modified very differently. The study also examines the third formant to show that the higher formants are consistently lowered in both PD and stroke. Hypokinetic speech production in these cases is reflected in lower intensity. The results have significant applications in clinical acoustics and in the theoretical fields of neurology of speech, linguistics and phonology.",Date Added to IEEEXplore:02 January 2012
10313325,Improving the Efficiency of Dysarthria Voice Conversion System Based on Data Augmentation,"Dysarthria, a speech disorder often caused by neurological damage, compromises the control of vocal muscles in patients, making their speech unclear and communication troublesome. Recently, voice-driven methods have been proposed to improve the speech intelligibility of patients with dysarthria. However, most methods require a significant representation of both the patient’s and target speaker’s corpus, which is problematic. This study aims to propose a data augmentation-based voice conversion (VC) system to reduce the recording burden on the speaker. We propose dysarthria voice conversion 3.1 (DVC 3.1) based on a data augmentation approach, including text-to-speech and StarGAN-VC architecture, to synthesize a large target and patient-like corpus to lower the burden of recording. An objective evaluation metric of the Google automatic speech recognition (Google ASR) system and a listening test were used to demonstrate the speech intelligibility benefits of DVC 3.1 under free-talk conditions. The DVC system without data augmentation (DVC 3.0) was used for comparison. Subjective and objective evaluation based on the experimental results indicated that the proposed DVC 3.1 system enhanced the Google ASR of two dysarthria patients by approximately [62.4%, 43.3%] and [55.9%, 57.3%] compared to unprocessed dysarthria speech and the DVC 3.0 system, respectively. Further, the proposed DVC 3.1 increased the speech intelligibility of two dysarthria patients by approximately [54.2%, 22.3%] and [63.4%, 70.1%] compared to unprocessed dysarthria speech and the DVC 3.0 system, respectively. The proposed DVC 3.1 system offers significant potential to improve the speech intelligibility performance of patients with dysarthria and enhance verbal communication quality.",N/A
10126695,Robust Assessment of Dysarthrophonic Voice with RASTA-PLP Features: A Nonlinear Spectral Measures,This paper presents an artificial intelligence based speech signal processing technique to identify dysarthrophonic voice with relative spectral-perceptual linear prediction (RASTA-PLP) features. Dysarthria is a neural motor speech disorder caused by muscular weakness. Voice analysis of dysarthrophonic patients is challenging as this disease has multidimensional effects on the human voice generation system. Conventional spectral analysis is unable to accurately characterize the pathology associated with nonlinear dynamicity of human voice. This work investigates the suitability of RASTA-PLP features excerpted from speech signals to identify dysarthrophonic patients. The speech samples of healthy and dysarthrophonic patients are collected from the Saarbrücken Voice Database (SVD). Several machine learning and Artificial neural network (ANN) based algorithms are developed to evaluate the classification performance of the proposed system. The designed system can achieve excellent performance in terms of accuracy (100%) considering female and male subjects separately.,Date Added to IEEEXplore:24 May 2023
9047706,Comparison of English and Chinese Speech Recognition Using High-Density Electromyography,"Speaking different languages requires different ways of pronunciation, and the muscular activities associated with phonation show different articulation styles. Therefore, clarifying the contributions of the articulatory muscles in different regions, such as the face and neck, is helpful for automatic speech recognition. However, it remains unclear how the articulatory muscles at different positions affect the classification accuracies of speech recognition across different languages. In this study, the technique of high-density surface electromyography (HD sEMG) was proposed to investigate the role of different articulatory muscles in classifying English and Chinese speaking tasks, respectively. The HD sEMG signals were recorded by 120 electrodes evenly placed on the facial and neck muscles across six subjects while they were speaking five English and Chinese daily words. Four time-domain features were extracted from sEMG recordings and used to construct a linear-discriminant-analysis classifier for speech recognition. The results showed that the classification accuracies of using neck sEMG were higher than that of using facial sEMG in both English and Chinese recognition tasks. The accuracies for Chinese speaking tasks were significantly higher than that for English when using facial sEMG only. Moreover, there was no significant difference in accuracies between the two types of languages when using neck sEMG. This study might provide useful information about the contributions of different articulatory muscles, and pave the way for automatic speech recognition across different languages for patients with dysarthria.",Date Added to IEEEXplore:26 March 2020
10039514,Hybrid CNN-LSTM network to detect Dysarthria using Mel-Frequency Cepstral Coefficients,"Dysarthria is a speech problem acquired at birth due to cerebral palsy (CP) or developed after severe brain damage. Dysarthria affects more than 70% of Parkinson's patients and 10% to 65% of people with traumatic brain injury. It is critical to detect dysarthria and other voice speech difficulties early to diagnose the underlying cause. Intelligent systems capable of identifying dysarthria with incredible precision have been developed using audio processing techniques and various deep learning models. This paper presents a hybrid CNN-LSTM model for classifying patients with dysarthria using audio recordings. The CNN-LSTM combination helps capture spatial and temporal information where CNN acts as a feature extractor while LSTM functions as a classifier. The proposed model was trained on the publicly available 9184 audio recordings from the TORGO dataset, and various audio augmentation techniques were employed to generate synthetic data. A total of 128 features were extracted using Mel Frequency Cepstral Coefficients (MFCC) and fed into the architecture as inputs. The K-fold cross-validation technique was used to avoid overfitting and increase the generalization capability of the model. The proposed architecture achieved a state-of-the-art 99.59% accuracy on the dataset. The presented work will minimize the workload of speech pathologists and help them detect dysarthria precisely and effectively.",Date Added to IEEEXplore:13 February 2023
10731900,Advancing Voice Biometrics for Dysarthria Speakers Using Multitaper LFCC and Voice Conversion Data Augmentation,"Patients with dysarthria and physical impairments face challenges with traditional user interfaces. An Automatic Speaker Verification (ASV) system can enhance accessibility by replacing complex authentication methods and enabling voice biometrics in various applications for patients with dysarthria. This study focuses on enhancing accessibility of patients with dysarthria through an ASV system. In this study, a noval low variance Multitaper Linear Frequency Cepstral Coefficients (MTLFCC) feature is proposed. An ASV system for patients with dysarthria is implemented using the voice conversion data augmentation within a DNN framework. An extensive analysis is conducted to compare various multitaper techniques and taper weight choices using the Thomson multitaper method, specifically verifying patients with dysarthria as speakers. The impact of voice conversion through a cycle-consistent generative adversarial network (Cycle GAN) is also examined by modifying the acoustic attributes of control speech to make it perceptually similar to dysarthria speech and its implications for dysarthria ASV. Furthermore, the system performance is analyzed for different severity level of dysarthria to gain insight into how the selected multitaper parameters influence the outcomes. This study pioneers the use of MTLFCC features for ASV in the context of dysarthria, offering a novel approach to improve accessibility for this group.",N/A
10832263,Optimizing Dysarthria Wake-Up Word Spotting: an End-to-End Approach For SLT 2024 LRDWWS Challenge,"Speech has emerged as a widely embraced user interface across diverse applications. However, for individuals with dysarthria, the inherent variability in their speech poses significant challenges. This paper presents an end-to-end Pretrain-based Dual-filter Dysarthria Wake-up word Spotting (PD-DWS) system for the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge. Specifically, our system improves performance from two key perspectives: audio modeling and dual-filter strategy. For audio modeling, we propose an innovative 2 branch- d 2 v 2 model based on the pre-trained data2vec2(d2v2), which can simultaneously model automatic speech recognition (ASR) and wake-up word spotting (WWS) tasks through a unified multi-task finetuning paradigm. Additionally, a dual-filter strategy is introduced to reduce the false accept rate (FAR) while maintaining the same false reject rate (FRR). Experimental results demonstrate that our PD-DWS system achieves an FAR of 0.00321 and an FRR of 0.005, with a total score of 0.00821 on the test-B eval set, securing first place in the challenge.",Date Added to IEEEXplore:16 January 2025
10725823,Enhancing Dysarthria Diagnosis With Deep Learning Techniques,"Dysarthria is characterized by delayed, slurred speech that can be challenging to comprehend. This condition is frequently brought on by nerve injury that affects the muscles used to produce speech. Depending on which muscles are affected and the underlying cause, symptoms can differ greatly. Talk therapy can help with early detection and intervention, which can enhance treatment outcomes. Convolutional neural networks (CNNs) are the method suggested here for detecting dysarthria from audio data. The method highlights the distinctions in speech patterns between people with and without dysarthria by using feature extraction, notably Mel-frequency cepstral coefficients (MFCC), and audio visualization approaches. These properties are used in the development and training of several neural network models, such as CNN, Long Short Term Memory (LSTM), Gated Recurrent Units (GRU), Bidirectional LSTM, SimpleRNN, and Deep Neural Networks (DNN). These models’ performance is assessed with the use of confusion matrices and classification reports. This all-inclusive dysarthria detection pipeline includes advanced deep learning approaches for evaluation, model training, and data preprocessing. The objective is to develop a trustworthy dysarthria detection system that will help healthcare professionals identify and treat the ailment early on.",Date Added to IEEEXplore:04 November 2024
10487652,Using Novel Hybrid Convolutional Neural Network for Dysarthria Diagnosis,"Dysarthria is a motor speech disorder characterized by articulation and phonation difficulties resulting from speech muscle weakness, paralysis, or incoordination. A precise and timely diagnosis of dysarthria is essential for effective treatment and management of the condition, as it may deteriorate over time or be a precursor to a much more serious disease. On the other hand, this is becoming a severe problem in recent times owing to the rising ageing population and the prevalence of neurological disorders among such people. Hence early detection of dysarthria is deemed essential for the timely management of the disease. In recent years, Artificial Intelligence (AI) applications have shown promising results in various audio processing tasks and incorporated into pathological voice analysis for disease diagnosis. The majority of previous studies on dysarthria detection employed Machine Learning (ML) and Deep Learning (DL) models as the disease classification models. In light of this, this study presents a novel hybrid approach for classifying dysarthria based on audio data using Convolutional Neural Network (CNN) and Support Vector Machine (SVM). According to the experimental results, the proposed classification schema achieves an accuracy of 98.25 % compared to previous research work. Overall, our proposed method aims to automate the classification process, enabling faster and more reliable diagnoses.",Date Added to IEEEXplore:05 April 2024
9032852,Breathiness Indices for Classification of Dysarthria Based on Type and Speech Intelligibility,"Dysarthria classification based on intelligibility level is useful for speech pathologists for deciding the therapy. However, intelligibility assessment also depends on perceptual attributes like hypernasality, breathiness, slow rate, short pauses etc. These perceptual attributes vary depending on the cause for dysarthria giving rise to different types of dysarthria. In this work, we explore the use of breathiness features for intelligibility assessment of dysarthria and for distinguishing type of dysarthria. Voiced segments from two controlled speakers, two dysarthric speakers with low and mid intelligibility level each from UA database are used in the work. Features were analysed for use in dysarthria intelligibility assessment vs. distinguishing type of dysarthria.",Date Added to IEEEXplore:12 March 2020
9980124,"Cross-lingual Dysarthria Severity Classification for English, Korean, and Tamil","Data scarcity hinders research on dysarthria severity classification due to the limited size of datasets. While the crosslingual approach has been applied to alleviate the problem, the roles of language-specific features have been underestimated. This paper proposes a cross-lingual classification method for English, Korean, and Tamil, which employs both language-independent features and language-unique features. First, we extract thirty-nine features from diverse speech dimensions such as voice quality, pronunciation, and prosody. Second, feature selections are applied to identify the optimal feature set for each language. A set of shared features and a set of distinctive features are distinguished by comparing the feature selection results of the three languages. Lastly, automatic severity classification is performed, utilizing the two feature sets. Notably, the proposed method removes different features by languages to prevent the negative effect of unique features for other languages. Accordingly, eXtreme Gradient Boosting (XGBoost) algorithm is employed for classification, due to its strength in imputing missing data. In order to validate the effectiveness of our proposed method, two baseline experiments are conducted: experiments using the intersection set of mono-lingual feature sets (Intersection) and experiments using the union set of monolingual feature sets (Union). According to the experimental results, our method achieves better performance with a 67.14% F1 score, compared to 64.52% for the Intersection experiment and 66.74% for the Union experiment. Further, the proposed method attains better performances than mono-lingual classifications for all three languages, achieving 17.67%, 2.28%, 7.79% relative percentage increases for English, Korean, and Tamil, respectively. The result specifies that commonly shared features and language-specific features must be considered separately for cross-language dysarthria severity classification.",Date Added to IEEEXplore:21 December 2022
10799983,The Open-Access Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) Database for Intelligent Assessment,"Early objective identification and assessment of dysarthria due to neurological deficits are essential for neurorehabilitation. Developing a system to achieve this requires a large-scale database of pathological information with detailed labeling. In the present study, a high-quality Chinese multimodal audio-visual database, consisting of 64 subacute stroke patients and 25 healthy participants, named the “Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) database”, was established. The materials of MSDM include a series of speech tasks such as syllables, characters, words, sentences, and spontaneous speech. All audio-visual data in this database were manually annotated and simultaneously verified by experienced researchers. Additionally, comprehensive clinical assessments of speech-motor function (e.g., Frenchay Dysarthria Assessment) and cognitive function (e.g., Montreal Cognitive Assessment) for each individual were included in the database. In conclusion, the MSDM database is believed to provide sufficient data resources for developing automatic assessment and speech recognition methods and contribute to understanding the pathological mechanisms of dysarthria.",Date Added to IEEEXplore:23 December 2024
10464345,Improving Dysarthric Speech Segmentation With Emulated and Synthetic Augmentation,"Acoustic features extracted from speech can help with the diagnosis of neurological diseases and monitoring of symptoms over time. Temporal segmentation of audio signals into individual words is an important pre-processing step needed prior to extracting acoustic features. Machine learning techniques could be used to automate speech segmentation via automatic speech recognition (ASR) and sequence to sequence alignment. While state-of-the-art ASR models achieve good performance on healthy speech, their performance significantly drops when evaluated on dysarthric speech. Fine-tuning ASR models on impaired speech can improve performance in dysarthric individuals, but it requires representative clinical data, which is difficult to collect and may raise privacy concerns. This study explores the feasibility of using two augmentation methods to increase ASR performance on dysarthric speech: 1) healthy individuals varying their speaking rate and loudness (as is often used in assessments of pathological speech); 2) synthetic speech with variations in speaking rate and accent (to ensure more diverse vocal representations and fairness). Experimental evaluations showed that fine-tuning a pre-trained ASR model with data from these two sources outperformed a model fine-tuned only on real clinical data and matched the performance of a model fine-tuned on the combination of real clinical data and synthetic speech. When evaluated on held-out acoustic data from 24 individuals with various neurological diseases, the best performing model achieved an average word error rate of 5.7% and a mean correct count accuracy of 94.4%. In segmenting the data into individual words, a mean intersection-over-union of 89.2% was obtained against manual parsing (ground truth). It can be concluded that emulated and synthetic augmentations can significantly reduce the need for real clinical data of dysarthric speech when fine-tuning ASR models and, in turn, for speech segmentation.",N/A
9840837,Significance of Filterbank Structure for Capturing Dysarthric Information through Cepstral Coefficients,"The short-term Fourier transform magnitude spectra (STFT-MS) computed from the dysarthric speech deviates nonlinearly from the normal speech in different frequency bands depending on underlying sound units. This discriminating information can be captured by segmenting the STFT-MS into different frequency bands following the power spectra of board categories of sound units. Motivated by this observation in this study, we have computed the cepstral coefficients by analyzing the STFT-MS in 0–500 Hz, 500–2000 Hz, 2000–4000 Hz, and 4000 – 8000Hz, respectively for 16 kHz sampled speech data. Each of the selected frequency bands is analyzed by using a 30 channel Mel filterbank. The log filterbank energies computed for each sub-band are then polled together and discrete cosine transform (DCT) is applied to compute the cepstral coefficients, here termed as sub-band enhanced Mel frequency cepstral coefficients (SE-MFCC). The i-vector based dysarthric intelligibility assessment system reported in this study shows that the SEMFCC outperforms the conventional Mel frequency cepstral coefficients (MFCC), and the cepstral coefficients computed using inverse-Mel filterbank (IMFCC), and linear filterbank (LFCC). The score level combination of SE-MFCC with the MFCC further improves the overall performance.",Date Added to IEEEXplore:01 August 2022
9689552,Data Augmentation Based on Frequency Warping for Recognition of Cleft Palate Speech,"In this paper, we present an automatic speech recognition (ASR) system for the speech of a person with a cleft lip and palate (CLP). The accuracy of speech recognition for a person with CLP is lower than that of a physically-unimpaired (PU) person because the CLP speech has characteristics that differ from those of a PU person; moreover, the amount of available training data is quite limited. In the field of ASR for PU people, data augmentation and self-supervised learning have been studied to tackle this problem of data scarcity. In this paper, we evaluate the effectiveness of those approaches on CLP speech recognition, and propose a data augmentation technique based on frequency warping. The formant of CLP speech tends to fluctuate compared to that of PU people. In order to compensate for the large variety of formant components, our data augmentation method stretches or contracts the spectrogram through the frequency axis. The experimental results on an ASR task with two CLP subjects showed that both data augmentation and self-supervised learning were effective for CLP speech recognition, and our proposed method further improved the performance of those two approaches based on conventional SpecAugment techniques.",Date Added to IEEEXplore:03 February 2022
9292041,Improving Pronunciation Clarity of Dysarthric Speech Using CycleGAN with Multiple Speakers,"In this paper, we propose a method that improves pronunciation clarity of dysarthric speech using CycleGAN based non-parallel voice conversion. This method converts dysarthric speech into healthy speech using CycleGAN. We considered the use of single and multiple speakers as healthy speech. The subjective evaluations showed the effectiveness of using multiple speakers as healthy speech.",Date Added to IEEEXplore:21 December 2020
9754848,Adaptation of a Pronunciation Dictionary for Dysarthric Speech Recognition,"In the general framework of an automatic speech recognition system, a pronunciation dictionary, that is a mapping table from a phoneme sequence to a word, is used both in the processes of training and recognition. However, this pronunciation dictionary is not always adequate in the case of dysarthric speech recognition because dysarthric people often have difficulty pronouncing words in the same way they are pronounced in the dictionary. In this paper, we investigate the adaptation of a pronunciation dictionary to an individual dysarthric person and evaluate the effectiveness of adapting the dictionary using a dysarthric speech recognition task. In the proposed method, in order to find rules we can use to modify a dictionary, we analyze the pattern of mis-recognition in the phoneme recognition results. By following the extracted rules, we add pronunciations to the dictionary for the target dysarthric person. We evaluate the effectiveness of the adapted pronunciation dictionary on a continuous speech recognition task and demonstrate that the adapted dictionary can decrease the word error rate.",Date Added to IEEEXplore:14 April 2022
1616954,A Fuzzy Cognitive Map Hierarchical Model for Differential Diagnosis of Dysarthrias and Apraxia of Speech,"This paper presents a novel soft computing system for differential diagnosis of the dysarthrias and apraxia of speech based on well accepted dysarthrias' classification system used by speech and language pathologists. The dysarthrias and apraxia are complex disorders of speech because they represent a variety of neurological disturbances that can potentially affect every component of speech production. Since an accurate diagnosis is a very challenging task for the clinician, the under development system based on hierarchical fuzzy cognitive maps (FCMs) will be used as a ""second opinion"" or training system. The hierarchical FCM differential diagnosis system is capable of differentiating between the six types of dysarthria as well as apraxia. The system was tested using published case studies and real patients and examples are presented here",Date Added to IEEEXplore:10 April 2006
10776229,A Novel Gamified Approach for Collecting Speech Data from Young Children with Dysarthria: Feasibility and Positive Engagement Evaluation,"Dysarthria is a common and treatable speech problem in children, and computer-assisted speech therapy is a promising way for children's speech therapy. However, data collection poses a significant challenge for computer-assisted therapy, especially when it comes to gathering speech data from young children, particularly those with dysarthria. Finding a better way to collect young children's speech data is, therefore, an urgent need. This paper prompted a gamified speech collection method and carried out an experiment to compare the participation time, error rate, and collection efficiency with the gamified method and with a traditional method where adults are imitated. Moreover, we also explore whether the gamified collection methods increase the children's positive engagement. A feasibility study including 10 children with dysarthria and 10 children without speech problems was conducted. Their participation duration, number of spoken utterances, number of mispronunciation utterances, and a questionnaire about children's engagement attitude were recorded. The findings indicate that the gamified collecting method reduces pronunciation mistake rates in children with dysarthria while also increasing their engagement to participate.",Date Added to IEEEXplore:11 December 2024
10181220,Edge Computing Solutions Supporting Voice Recognition Services for Speakers with Dysarthria,"In the framework of Automatic Speech Recognition (ASR), the synergism between edge computing and artificial intelligence has led to the development of intelligent objects that process and respond to human speech. This acts as a key enabler for multiple application scenarios, such as smart home automation, where the user’s voice is an interface for interacting with appliances and computer systems. However, for millions of speakers with dysarthria worldwide, such a voice interaction is impossible because nowadays ASR technologies are not robust to their atypical speech commands. So these people, who also live with severe motor disabilities, are unable to benefit from many voice assistant services that might support their everyday life. To cope with the above challenges, this paper proposes a deep learning approach to isolated word recognition in the presence of dysarthria conditions, along with the deployment of customized ASR models on machine learning powered edge computing nodes. In this way, we work toward a low-cost, portable solution with the potential to operate next to the user with a disability, e.g., in a wheelchair or beside a bed, in an always active mode. Finally, experiments show the goodness (in terms of word error rate) of our speech recognition solution in comparison with other studies on isolated word recognition for impaired speech.",Date Added to IEEEXplore:19 July 2023
10485694,Dysarthria Diagnosis and Dysarthric Speaker Identification Using Raw Speech Model,"Dysarthria is a medical condition that causes difficulty in producing coherent speech due to muscle paralysis or weakness. This article presents a unique approach to identifying dysarthric speakers using a deep learning model that works directly with unprocessed speech waveforms. By eliminating the need for feature extractions, the model's resistance to noise and voice variability is increased. The proposed approach utilizes a SincNet layer model with multiple initializations including Mel, Erb, and Bark scales for dysarthria detection (DD) and dysarthric speaker identification (DSI). Bark scaling, aligning better with human auditory perception and capturing distinctive acoustic features, notably outperforms other initialization methods. When Bark scaling was employed, the study's results demonstrated outstanding accuracy rates of 97.0% for DD and 88.0% for DSI. The results demonstrated exceptional performance, that surpassing existing literature benchmarks.",Date Added to IEEEXplore:05 April 2024
8706615,Acoustic and Kinematic Examination of Dysarthria in Cantonese Patients of Parkinson’s Disease,"Hypokinetic dysarthria is one of the core symptoms of Parkinson's disease, characterized by reduced loudness, slurred speech and distorted consonant productions. Dopaminergic medication for Parkinson's disease (PD) has been proven to be effective in treating limb and gross motor movement problems. However, the literature sees contradictory findings regarding dopaminergic effects on speech problems associated with PD. Previous perceptual and acoustic studies of PD hypokinetic dysarthria mostly involved heterogeneous population and variations of speech tasks which complicated data interpretation. Also, the lack of kinematic data limited our understanding of the details of articulation errors associated with PD, as well as the medication effects. Electromagnetic articulography (EMA) enabled the examination of PD articulatory patterns with high degree of accuracy and safety. The aim of the present study was to address these inconsistencies by providing an integrative description of basic kinematic and acoustic parameters of speech production about the dopaminergic effect on early PD speech by using EMA. The results revealed a significant improvement of articulatory function on dopaminergic effects for PD speech, evidenced by increased vowel space, the velocity and accelerated velocity initiation and coordination of articulation during bilabial or alveolar productions.",Date Added to IEEEXplore:06 May 2019
7782309,Representation Learning Based Speech Assistive System for Persons With Dysarthria,"An assistive system for persons with vocal impairment due to dysarthria converts dysarthric speech to normal speech or text. Because of the articulatory deficits, dysarthric speech recognition needs a robust learning technique. Representation learning is significant for complex tasks such as dysarthric speech recognition. We focus on robust representation for dysarthric speech recognition that involves recognizing sequential patterns of varying length utterances. We propose a hybrid framework that uses a generative learning based data representation with a discriminative learning based classifier. In this hybrid framework, we propose to use Example Specific Hidden Markov Models (ESHMMs) to obtain log-likelihood scores for a dysarthric speech utterance to form fixed dimensional score vector representation. This representation is used as an input to discriminative classifier such as support vector machine. The performance of the proposed approach is evaluated using UA-Speech database.The recognition accuracy is much better than the conventional hidden Markov model based approach and Deep Neural Network-Hidden Markov Model (DNN-HMM). The efficiency of the discriminative nature of score vector representation is proved for “very low” intelligibility words.",N/A
1280248,A modern approach to dysarthria classification,"This work deals with the assessment of neurological diseases known as dysarthrias, using a novel approach based on objective and perceptual features extracted from pathological speech signals. A methodology for the classification of dysarthria is developed in which digital signal processing algorithms are used to appraise the severity of those features less reliably judged by the clinicians, while the others are taken directly from perceptual judgments or medical records. The assessment process evaluates the performance of two different classifiers and compares them with the traditional assessment system. The first approach is based on the lineal discriminant analysis and the second is a non-lineal technique based on self-organizing maps. The non-lineal classifier provided the highest percent of correct classification and the most accurate information on the relevance of the features in the classifier decision. It also provided a bi-dimensional representation of de data that allows a better understanding of the correspondence between the speech deviations and the location of the damage in the peripheral or central nervous system.",Date Added to IEEEXplore:05 April 2004
8578023,Observations from a Simple Vocal-Tract-Model's Behaviour for PD-Dysarthric Speech: Applicability,"The uniform-element tube model of a speaker's vocal tract is a by-product from LPC speech analysis. It is used here to observe a speaker's articulation. The aim is an insight into possible articulatory weaknesses of PD (Parkinson-Disease) patients who suffer from dysarthria, i.e., speaking problems. Approved auditive methods exist for evaluation of the patients' speech handicaps, applying also instrumental signal features. But a direct view on vocal-tract movements can be an additional diagnostic aid. Due to its real-time potential, the simple estimation of vocal-tract areas from LPC analysis is regarded here, despite its known impreciseness. In a first measurement set, it is checked with fluent speech of healthy persons, whether any reaction appears on articulatory changes between clear and intentionally mumbled speech. Then, with sustained vowels of healthy and slightly as well as strongly handicapped speakers, the potential of the approach to display such differences is examined.",Date Added to IEEEXplore:16 December 2018
8962210,Automatic Assessment of Sentence-Level Dysarthria Intelligibility Using BLSTM,"Dysarthria is a motor speech impairment, often characterized by slow and slurred speech that is generally incomprehensible by human listeners. An understanding of the intelligibility level of the patient's dysarthric speech can provide an insight into the progression/status of the underlying cause and is essential for planning therapy. Automatic assessment of dysarthric speech intelligibility can be of immense value and serve to assist speech language pathologists in diagnosis and therapy. However, this is a non-trivial problem due to the high intra and inter speaker variability in dysarthric speech. In this article we propose a machine learning-based method to automatically classify dysarthric speech into intelligible (I) and non-intelligible (NI) using Bidirectional Long-Short Term Memory (BLSTM) Networks. We explored balancing of training data to represent both the classes almost equally and its implications on the binary classification. Additionally, we present a mechanism to use the available pre-trained acoustic models for transfer-learning. It was observed that the transfer learning method was able to handle channel noise. This technique provided significant improvement of roughly 6% as compared to traditional machine learning method.",N/A
9162481,Robust Estimation of Hypernasality in Dysarthria With Acoustic Model Likelihood Features,"Hypernasality is a common characteristic symptom across many motor-speech disorders. For voiced sounds, hypernasality introduces an additional resonance in the lower frequencies and, for unvoiced sounds, there is reduced articulatory precision due to air escaping through the nasal cavity. However, the acoustic manifestation of these symptoms is highly variable, making hypernasality estimation very challenging, both for human specialists and automated systems. Previous work in this area relies on either engineered features based on statistical signal processing or machine learning models trained on clinical ratings. Engineered features often fail to capture the complex acoustic patterns associated with hypernasality, whereas metrics based on machine learning are prone to overfitting to the small disease-specific speech datasets on which they are trained. Here we propose a new set of acoustic features that capture these complementary dimensions. The features are based on two acoustic models trained on a large corpus of healthy speech. The first acoustic model aims to measure nasal resonance from voiced sounds, whereas the second acoustic model aims to measure articulatory imprecision from unvoiced sounds. To demonstrate that the features derived from these acoustic models are specific to hypernasal speech, we evaluate them across different dysarthria corpora. Our results show that the features generalize even when training on hypernasal speech from one disease and evaluating on hypernasal speech from another disease (e.g., training on Parkinson's disease, evaluation on Huntington's disease), and when training on neurologically disordered speech but evaluating on cleft palate speech.",N/A
10889515,A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information,"Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses.",Date Added to IEEEXplore:07 March 2025
1595343,Processing of pathological changes in speech caused by dysarthria,"Computer analysis of voice isolated sounds may lead to identification of parameters correlated with neurological diseases. This paper presents results of preliminary research of voice pathological changes caused by dysarthria. The selection of linguistic material was characterized according to the place and manner of articulation in the phonetic system of Polish. Results of clinical examination allowed to determine simple markers of neurodegenerative diseases, which serves as a basis for construction of objective examination model.",Date Added to IEEEXplore:21 February 2006
250584,A statistical causal model for the assessment of dysarthric speech and the utility of computer-based speech recognition,"The evaluation of the degree of speech impairment and the utility of computer recognition of impaired speech are separately and independently performed. Particular attention is paid to the question concerning whether or not there is a relationship between naive listeners' subjective judgments of impaired speech and the performance of a laboratory version of a speech recognition system. It is a difficult task to relate a speech impairment rating with speech recognition accuracy. Towards this end, a statistical causal model is proposed. This model is very appealing in its structure to support inference, and thus can be applied to perform various assessments such as the success of automatic recognition of dysarthric speech. The application of this model is illustrated with a case study of a dysarthric speaker compared against a normal speaker serving as a control.<>",N/A
10848870,Significance of Entropy Based Features For Dysarthric Severity Level Classification,"Dysarthria is a motor speech disorder arising from impairment of muscles that makes difficult to form or pronounce words while speaking. In this paper, we introduce an approach of multiband entropy based features extracted from dysarthric speech signals for dysarthric severity level classification. Generally, entropy is measured as the number of bits of information contained in each message signal. The information content of these signal measures how much randomness or uncertainity contains in a signal. Extending this, we use a frame-wise processing technique to divide the speech signal into short frames which allows for detailed analysis of different characteristics of speech signal. Furthermore we divide frames into equal sub bands and compute the entropy and zero mean entropy in each sub band using Gabor filterbank. It is expected that the mean entropy of very low dysarthric severity is lower compared to high dysarthric severity level indicating that randomness is increasing as the severity level of dysarthria is increasing. Experimental analysis were conducted on extensively used dataset namely UA Speech. Results were carried out by Convolutional Neural Network (CNN), along with 5-cross validation. The results are compared against standard MFCC, LFCC, and glottal source based LFRCC. It was observed that the addition of entropy information boosted the performance of MFCC by 4.38%, LFCC by 2.54%, and LFRCC by 1.88% compared to their traditional techniques indicating the crucial information captured by the entropy for dysarthria severity classification.",Date Added to IEEEXplore:27 January 2025
10782673,Automated Acoustic Analysis in Parkinson’s Disease Using a Smartphone*,"Dysarthria is a common speech disorder in Parkinson’s Disease (PD). The Dysarthria Analyzer software has emerged as a viable tool for automatic speech analysis in PD and quantification of dysarthria severity. However, most studies use the Dysarthria Analyzer with recordings obtained under tightly controlled conditions and high-quality microphones, and the utility of the Dysarthria Analyzer when used with recordings acquired under non-ideal conditions, such as in busy clinical settings, remains unexplored. This study investigates the Dysarthria Analyzer’s performance in a setting more akin to a clinical environment using a smartphone. We obtained data from three groups, including healthy controls (HC), PD patients with their deep brain stimulation on (ON-DBS), and PD patients with their DBS off (OFF-DBS). We found a significant decrease in pitch variability and an increase in speech rate for the OFF-DBS group compared to the HC. Furthermore, most of the estimated values for the speech markers fall within the reported values in the literature. Our findings demonstrate that the Dysarthria Analyzer effectively extracts relevant speech markers even when used with recordings obtained under non-ideal conditions, emphasizing its potential for widespread clinical adoption.Clinical Relevance— Our findings demonstrate the potential of using smartphone recordings obtained in clinical environments for automatic objective speech analysis. These findings are relevant for developing a clinical tool that can be widely accessible and easily implemented during routine clinical visits of PD to improve the assessment of dysarthria in PD.",Date Added to IEEEXplore:17 December 2024
9419963,Speech Vision: An End-to-End Deep Learning-Based Dysarthric Automatic Speech Recognition System,"Dysarthria is a disorder that affects an individual’s speech intelligibility due to the paralysis of muscles and organs involved in the articulation process. As the condition is often associated with physically debilitating disabilities, not only do such individuals face communication problems, but also interactions with digital devices can become a burden. For these individuals, automatic speech recognition (ASR) technologies can make a significant difference in their lives as computing and portable digital devices can become an interaction medium, enabling them to communicate with others and computers. However, ASR technologies have performed poorly in recognizing dysarthric speech, especially for severe dysarthria, due to multiple challenges facing dysarthric ASR systems. We identified these challenges are due to the alternation and inaccuracy of dysarthric phonemes, the scarcity of dysarthric speech data, and the phoneme labeling imprecision. This paper reports on our second dysarthric-specific ASR system, called Speech Vision (SV) that tackles these challenges by adopting a novel approach towards dysarthric ASR in which speech features are extracted visually, then SV learns to see the shape of the words pronounced by dysarthric individuals. This visual acoustic modeling feature of SV eliminates phoneme-related challenges. To address the data scarcity problem, SV adopts visual data augmentation techniques, generates synthetic dysarthric acoustic visuals, and leverages transfer learning. Benchmarking with other state-of-the-art dysarthric ASR considered in this study, SV outperformed them by improving recognition accuracies for 67% of UA-Speech speakers, where the biggest improvements were achieved for severe dysarthria.",N/A
8488841,An Optimal Speech Recognition Module for Patient's Voice Monitoring System in Smart Healthcare Applications,"During recent years, health care domain has rapidly developed in which patients and medical resources are directly connected with the smart way that enables Smart Health Care. The growth in design and development of a speech automated system will provide a life assistant service in smart health care environment. In automating the speech system, speech recognition is one of the basic steps to understand the human recognition and their behaviors. These speech recognition systems will be very much accessible for speakers who suffer from dysarthria, a neurological disability that damages the control of motor speech articulators. In this paper, the main objective is to develop an efficient speech recognition module based on the Voice Input Voice Output Communication Aid (VIVOCA) architecture that can device a support aid to the people with DYSARTHRIA. Totally there are seven features extracted from each noise eliminated real time bilingual isolated word speech signal data uttered by a speaker both in Tamil and English languages. Vector Quantization based Genetic Algorithm codebook is created for the recognition modeling. Optimization of Hidden Markov Model (HMM) is done based on Particle Swarm Optimization (PSO) method to improve the recognition accuracy compared to the conventional HMM and also experiment results of the proposed module shows 95% of accuracy. The proposed module will be very much useful for developing a speech recognition system that facilitates the patients and persons with special needs for communication. The proposed module is also evaluated for its complexity which will be therefore efficient for low consumption of energy.",Date Added to IEEEXplore:11 October 2018
10210422,Extracting Phonetic Posterior-Based Features for Detecting Multiple Sclerosis From Speech,"Multiple sclerosis (MS) is a chronic inflammatory disease of the central nervous system which, in addition to affecting motor and cognitive functions, may also lead to specific changes in the speech of patients. Speech production, comprehension, repetition and naming tasks, as well as structural and content changes in narratives, might indicate a limitation of executive functions. In this study we present a speech-based machine learning technique to distinguish speakers with relapsing-remitting subtype MS and healthy controls (HC). We exploit the fact that MS might cause a motor speech disorder similar to dysarthria, which, with our hypothesis, might affect the phonetic posterior estimates supplied by a Deep Neural Network acoustic model. From our experimental results, the proposed posterior posteriorgram-based feature extraction approach is useful for detecting MS: depending on the actual speech task, we obtained Equal Error Rate values as low as 13.3%, and AUC scores up to 0.891, indicating a competitive and more consistent classification performance compared to both the x-vector and the openSMILE ‘ComParE functionals’ attributes. Besides this discrimination performance, the interpretable nature of the phonetic posterior features might also make our method suitable for automatic MS screening or monitoring the progression of the disease. Furthermore, by examining which specific phonetic groups are the most useful for this feature extraction process, the potential utility of the proposed phonetic features could also be utilized in the speech therapy of MS patients.",N/A
6762967,A Multi-Views Multi-Learners Approach Towards Dysarthric Speech Recognition Using Multi-Nets Artificial Neural Networks,"Automatic speech recognition (ASR) can be very helpful for speakers who suffer from dysarthria, a neurological disability that damages the control of motor speech articulators. Although a few attempts have been made to apply ASR technologies to sufferers of dysarthria, previous studies show that such ASR systems have not attained an adequate level of performance. In this study, a dysarthric multi-networks speech recognizer (DM-NSR) model is provided using a realization of multi-views multi-learners approach called multi-nets artificial neural networks, which tolerates variability of dysarthric speech. In particular, the DM-NSR model employs several ANNs (as learners) to approximate the likelihood of ASR vocabulary words and to deal with the complexity of dysarthric speech. The proposed DM-NSR approach was presented as both speaker-dependent and speaker-independent paradigms. In order to highlight the performance of the proposed model over legacy models, multi-views single-learner models of the DM-NSRs were also provided and their efficiencies were compared in detail. Moreover, a comparison among the prominent dysarthric ASR methods and the proposed one is provided. The results show that the DM-NSR recorded improved recognition rate by up to 24.67% and the error rate was reduced by up to 8.63% over the reference model.",N/A
7876719,Regularized Speaker Adaptation of KL-HMM for Dysarthric Speech Recognition,"This paper addresses the problem of recognizing the speech uttered by patients with dysarthria, which is a motor speech disorder impeding the physical production of speech. Patients with dysarthria have articulatory limitation, and therefore, they often have trouble in pronouncing certain sounds, resulting in undesirable phonetic variation. Modern automatic speech recognition systems designed for regular speakers are ineffective for dysarthric sufferers due to the phonetic variation. To capture the phonetic variation, Kullback-Leibler divergence-based hidden Markov model (KL-HMM) is adopted, where the emission probability of state is parameterized by a categorical distribution using phoneme posterior probabilities obtained from a deep neural network-based acoustic model. To further reflect speaker-specific phonetic variation patterns, a speaker adaptation method based on a combination of L2 regularization and confusion-reducing regularization, which can enhance discriminability between categorical distributions of the KL-HMM states while preserving speaker-specific information is proposed. Evaluation of the proposed speaker adaptation method on a database of several hundred words for 30 speakers consisting of 12 mildly dysarthric, 8 moderately dysarthric, and 10 non-dysarthric control speakers showed that the proposed approach significantly outperformed the conventional deep neural network-based speaker adapted system on dysarthric as well as non-dysarthric speech.",N/A
10225595,Dysarthric Speech Transformer: A Sequence-to-Sequence Dysarthric Speech Recognition System,"Automatic Speech Recognition (ASR) technologies can be life-changing for individuals who suffer from dysarthria, a speech impairment that affects articulatory muscles and results in incomprehensive speech. Nevertheless, the performance of the current dysarthric ASR systems is unsatisfactory, especially for speakers with severe dysarthria who most benefit from this technology. While transformer and neural attention-base sequences-to-sequence ASR systems achieved state-of-the-art results in converting healthy speech to text, their applications as a Dysarthric ASR remain unexplored due to the complexities of dysarthric speech and the lack of extensive training data. In this study, we addressed this gap and proposed our Dysarthric Speech Transformer that uses a customized deep transformer architecture. To deal with the data scarcity problem, we designed a two-phase transfer learning pipeline to leverage healthy speech, investigated neural freezing configurations, and utilized audio data augmentation. Overall, we trained 45 speaker-adaptive dysarthric ASR in our investigations. Results indicate the effectiveness of the transfer learning pipeline and data augmentation, and emphasize the significance of deeper transformer architectures. The proposed ASR outperformed the state-of-the-art and delivered better accuracies for 73% of the dysarthric subjects whose speech samples were employed in this study, in which up to 23% of improvements were achieved.",N/A
9247264,Investigation of Different Time-Frequency Representations for Intelligibility Assessment of Dysarthric Speech,"Speech disorders linked to neurological problems affect person's ability to communicate through speech. Dysarthria is one of the speech disorders caused due to muscle weakness producing slow, slurred and less intelligible speech. Automatic intelligibility assessment of dysarthria from speech can be used as a promising clinical tool in treatment. This paper explores the use of perceptually enhanced Fourier transform spectrograms and Constant-Q transform spectrograms with CNN to assess word level and sentence level intelligibility of dysarthric speech from UA and TORGO databases. Constant-Q transform and perceptually enhanced mel warped STFT spectrograms performed better in the classification task.",N/A
8943284,Voice Conversion for Persons with Amyotrophic Lateral Sclerosis,"Amyotrophic lateral sclerosis (ALS) results in progressive paralysis of voluntary muscles throughout the body. As speech deteriorates, individuals rely on pre-programmed messages available on commercial speech generating devices to communicate using one of the generic electronic voices on the device. To replace these generic voices and restore vocal identity, our aim is to develop personalized voices for people with ALS via the approach of voice conversion. The task is challenging because very few people have large quantities of their premorbid healthy speech recorded. Therefore, we have to rely on small quantities of dysarthric speech concomitant with an individual's disease stage. Further, progressive fatigue prohibits acquisition of large speech datasets and individuals display a range of dysarthria severities resulting from breathing, voice, articulation, resonance, and prosody disturbances. As the first step to address these problems, we use healthy source speakers and propose the approach of combining a structured sparse spectral transform with multiple linear regression-based frequency warping prediction for spectral conversion, and interpolating the transformed spectral frames for speech rate modification. Our experimental data included four healthy source speakers from the ARCTIC dataset, and four target ALS speakers with mild to severe dysarthria, forming 16 speaker pairs. Subjective listening evaluations showed that on average, (i) the proposed approach improved speech intelligibility by about 80% over the target speakers' speech, (ii) the converted voice was 3 times more similar to the target speakers' speech than to the source speakers' speech, and (iii) the converted speech quality was close to the MOS scale “good” relative to the source speakers' speech being “excellent.”",N/A
8578052,Optical force and distance sensing in intraoral devices for stroke rehabilitation: a distance calibration and force classification approach,"Stroke survivors often suffer from oro-facial impairments, affecting swallowing function and speech production. Measuring tongue pressure and position intraorally can help to improve therapy for both symptoms, but space inside the oral cavity is extremely limited and such devices can easily be prohibitively large and obstructive if too many sensors are needed. In this work, we present our efforts to sense the force of the tongue exerted against the hard palate and the tongue-palate distance, using only optical proximity sensors. To explore the feasibility and accuracy of this approach and to evaluate the selected sensor, we conducted a study with 10 subjects and measured the sensor's response to 10 discrete distances ranging from 0mm to 30mmbetween tongue and sensor, and to a continuously increasing tongue force against the sensor from 0.1N to 8N. For distance measurements, an existing in-situ calibration method was applied and verified that yielded errors of less than 2mm for the estimated distances in nearly every case. For force measurements, a Bayesian classification approach was adopted to map sensor data to two force regions (below and above a certain boundary value), where up to 84.1% (average: 71.7 %) of ADC values were classified correctly within-sample.",Date Added to IEEEXplore:16 December 2018
9754844,Abstract of Presentations on March 9th,Presents abstracts for the articles comprising the conference proceedings.,Date Added to IEEEXplore:14 April 2022
9179503,Speech task based automatic classification of ALS and Parkinson’s Disease and their severity using log Mel spectrograms,"We consider the task of speech based classification of patients with amyotrophic lateral sclerosis (ALS), Parkinson's disease (PD) and healthy controls (HC). Recent work in convolutional neural networks (CNN) to solve image classification problems raises the possibility of utilizing spectral representation of speech for detection of neurological diseases. In this paper, a spectrogram based approach is used. Feeding overlapping windows to the CNN makes sure that the temporal aspects are considered by using short signal segments or wide analysis filters. A three class (ALS, PD or HC) dysarthria classification is performed. In addition, we perform two severity classification experiments for ALS (5 class) and PD (3 class) respectively. Experiments are conducted on both baseline MFCC data [1] and log Mel spectrograms. Classification results show that for several audio lengths, models trained on log Mel spectrograms consistently outperform those of MFCC's. The ability of the network to accurately classify different classes is evaluated via the area under receiver operating characteristic curve [2],[3]. The findings from this study could aid in better detection and monitoring of ALS and PD diseases.",Date Added to IEEEXplore:28 August 2020
9380752,Detecting Effect of Levodopa in Parkinson’s Disease Patients Using Sustained Phonemes,"Background: Parkinson’s disease (PD) is a multi-symptom neurodegenerative disease generally managed with medications, of which levodopa is the most effective. Determining the dosage of levodopa requires regular meetings where motor function can be observed. Speech impairment is an early symptom in PD and has been proposed for early detection and monitoring of the disease. However, findings from previous research on the effect of levodopa on speech have not shown a consistent picture. Method: This study has investigated the effect of medication on PD patients for three sustained phonemes; /a/, /o/, and /m/, which were recorded from 24 PD patients during medication off and on stages, and from 22 healthy participants. The differences were statistically investigated, and the features were classified using Support Vector Machine (SVM). Results: The results show that medication has a significant effect on the change of time and amplitude perturbation (jitter and shimmer) and harmonics of /m/, which was the most sensitive individual phoneme to the levodopa response. /m/ and /o/ performed at a comparable level in discriminating PD-off from control recordings. However, SVM classifications based on the combined use of the three phonemes /a/, /o/, and /m/ showed the best classifications, both for medication effect and for separating PD from control voice. The SVM classification for PD-off versus PD-on achieved an AUC of 0.81. Conclusion: Studies of phonation by computerized voice analysis in PD should employ recordings of multiple phonemes. Our findings are potentially relevant in research to identify early parkinsonian dysarthria, and to tele-monitoring of the levodopa response in patients with established PD.",N/A
9887934,Parkinson’s Disease Detection Using Smartphone Recorded Phonemes in Real World Conditions,"Parkinson’s disease (PD) is a multi-symptom neurodegenerative disease. There are no biomarkers; the diagnosis and monitoring of the disease progression require clinical and functional symptom observation. Voice impairment is an early symptom of PD, and computerized analysis of voice has been proposed for early detection and monitoring of the disease. However, there is poor reproducibility of many studies, which is attributed to the experimental data having been collected under controlled conditions. To overcome the limitations of earlier works, this study has investigated three sustained phonemes: /a/, /o/, and /m/, which were recorded using an iOS-based smartphone from 72 participants (36 people with PD and 36 healthy) in a typical clinical setting. A number of signal features were obtained, statistically investigated, and ranked to identify the suitable feature sets. These were classified using machine learning models. The results show that a combination of phonemes /a/+/o/+/m/ was most suited to differentiate the voice of PD people from healthy control participants, with an average accuracy, sensitivity, and specificity of 100%, 100%, 100%, respectively, using leave-one-out validation. The findings of this study could assist in the clinical assessments and remote telehealth monitoring for people with parkinsonian dysarthria using smartphones.",N/A
10308048,A Review and Classification of Amyotrophic Lateral Sclerosis with Speech as a Biomarker,"Amyotrophic Lateral Sclerosis (ALS) is a motor system neurodegenerative disease that affects speech impairment, spinal, respiratory and swallowing difficulties in patients. It has gradually increased in elderly people in recent years and is not easy to diagnose. The ALS bulbar form system is based on detecting dysarthria speech classification in discriminating healthy subjects from ALS patients. To construct the classification model by using various machine learning techniques, the studies used datasets related to speech impairment recordings of ALS patients and healthy subjects. Early diagnosis of ALS can somewhat improve the patient’s quality of life to an extent. Sustained vowel phonation is very useful for classifying ALS and healthy control (HC). In this study, jitter and shimmer were used to extract features. A support vector machine classifier was applied, and it classified ALS/HC with an accuracy of 98.5%. This study presents a detailed review of various machine learning techniques applied to the speech signal for the diagnosis of ALS and their impact on future research in this direction.",Date Added to IEEEXplore:23 November 2023
9980322,Teager Energy Cepstral Coefficients For Classification of Dysarthric Speech Severity-Level,"Dysarthria is a neuro-motor speech impairment that renders speech unintelligibility, which is generally imperceptible to humans w.r.t severity-levels. Dysarthric speech classification acts as a diagnostic tool for evaluating the advancement in a patient's severity condition and also aids in automatic dysarthric speech recognition systems (an important assistive speech technology). This study investigates the significance of Teager Energy Cepstral Coefficients (TECC) in dysarthric speech classification using three deep learning architectures, namely, Convolutional Neural Network (CNN), Light-CNN (LCNN), and Residual Networks (ResNet). The performance of TECC is compared with state-of-the-art features, such as Short-Time Fourier Transform (STFT), Mel Frequency Cepstral Coefficients (MFCC), and Linear Frequency Cepstral Coefficients (LFCC). In addition, this study also investigate the effectiveness of cepstral features over the spectral features for this problem. The highest classification accuracy achieved using UA-Speech corpus is 97.18%, 94.63%, and 98.02% (i.e., absolute improvement of 1.98%, 1.41%, and 1.69%) with CNN, LCNN, and ResNet, respectively, as compared to the MFCC. Further, we evaluate feature discriminative capability usingF1-score, Matthew's Correlation Coefficient (MCC), Jaccard index, and Hamming loss. Finally, analysis of latency period w.r.t. state-of-the-art feature sets indicates the potential of TECC for practical deployment of the severity-level classification system.",Date Added to IEEEXplore:21 December 2022
10733600,Sequence-to-Sequence Models in Italian Atypical Speech Recognition,"In the domain of automatic speech recognition (ASR), we explore the usage of a state-of-the-art transformer-based sequence-to-sequence model to build a speaker-dependent isolated word recognizer for native Italian speakers with a speech disorder, such as dysarthria. In particular, this paper is concerned with a self-supervised learning approach, where the Wav2Vec2 has been fine-tuned on our private Italian corpus containing a total of 41 hours of speech contributions authored by 191 individuals with a disability and atypical speech. The discussed approach has been also evaluated thanks to collaboration of sixteen speakers with diverse degrees of speech disorders (mild, moderate, severe), and our analysis has shown a remarkable performance of our ASR system, with an overall word recognition accuracy of 97.4%.",Date Added to IEEEXplore:31 October 2024
8441307,Monitoring Progress of Parkinson's Disease Based on Changes in Phonation: a Pilot Study,"Hypokinetic dysarthria (HD) is a frequent symptom of idiopathic Parkinson's disease (PD). Although it is hypothesized its progress is tightly linked with changes in other motor/non-motor features of PD, it has not been proved yet. The aim of this work is to employ acoustic analysis of sustained phonation in order to identify significant correlates between phonatory measures and motor/non-motor deficits in a two-year follow-up study. For this purpose, we repeatedly quantified a sustained vowel/a/ in 51 PD patients who were also assessed by 5 common clinical scales. In addition, a multivariate regression model was trained to predict the motor/non-motor deficits in the horizon of two years. Results suggest that mainly instability in vocal folds oscillation increases with the progress of PD and with overall cognitive decline. Based on the acoustic analysis, the change in clinical scores could be predicted with the error in the range of 11.83-19.60 %.",Date Added to IEEEXplore:23 August 2018
7760933,An automatic diagnosis and assessment of dysarthric speech using speech disorder specific prosodic features,"To diagnose and classify the dysarthric speech, speech language pathologist (SLP) conducts a listening test. On the basis of the scores given by listeners the dysarthria is diagnosed and assessed. The above mentioned method is costly, time consuming and not very accurate. Unlike the traditional method, this research proposes an automatic diagnosis and assessment of dysarthria. The aim of this paper is to diagnose and classify the severity of dysarthria. The speech disorder specific prosodic features are selected by using genetic algorithm. The diagnosis and assessment of dysarthric speech is done by support vector machines. During diagnosis the classification accuracy of 98% has been achieved. And 87% of the dysarthric speech utterances are correctly classified. The standard UASPEECH database has been used in this work.",Date Added to IEEEXplore:01 December 2016
8031207,Signal Analysis for Voice Evaluation in Parkinson’s Disease,"Parkinson's Disease (PD) is a neurodegenerative disorder that is frequently correlated with vowel articulation difficulties. The phonation problem arises in patients affected by PD is commonly known as Parkinsonian Dysarthria and identifiedby vocal signal analysis. The analysis supporte physicians and specialists in early detection and monitoring of dysarthria aiming, to increase patients life quality and to evaluate the efficacy of treatments. We investigate on vocal signal analysis correlation with speech patterns related to PD. Vowel parameters are considered as discriminant elements among PD patients and healthy subjects. Aim of this work is to define possible indicators for dysarthria in PD patients.",Date Added to IEEEXplore:14 September 2017
10661160,Two-stage and Self-supervised Voice Conversion for Zero-Shot Dysarthric Speech Reconstruction,"Dysarthria is a motor speech disorder commonly associated with conditions such as cerebral palsy, Parkinson’s disease, amyotrophic lateral sclerosis, and stroke. Individuals with dysarthria typically exhibit significant speech difficulties, including imprecise articulation, lack of fluency, slow speech rate, and decreased volume and clarity, which can hinder their ability to communicate effectively with others. We propose a two-stage Voice Conversion method to enhance the reconstruction of dysarthric speech. In the first stage, we develop a KNN-VC approach based on a same-gender-retrieval strategy to preliminarily repair the dysarthric speech. In this stage, we match the dysarthric speech only with normal speech of the same gender. In the second stage, we adapt so-vits-svc to restore the speaker’s timbre and improve the sound quality of the speech repaired in the first stage. Both objective and subjective evaluations were conducted on the dataset of the Low Resource Dysarthria Wake-Up Word Spotting Challenge (LRDWWS Challenge) shows that the proposed approach can achieve some improvements in terms of speaker similarity, speech intelligibility and naturalness for unknown speakers, and these evaluations also show our method has a good Zero-shot performance. Our audio samples can be accessed online1.",Date Added to IEEEXplore:10 September 2024
95920,Computer aided methods for diagnosis and therapy of speech breathing disorders,"Computer-aided methods for the diagnosis and therapy of speech breathing disorders caused by brain damage (dysarthrias) are described. These methods make it possible to evaluate a speech disorder for diagnostic purposes using objective parameters. Special therapeutic tasks which proceed in accordance with the principle of biofeedback make it possible for the patient to carry out self-therapy through individualized exercise packages. Pathological speech patterns can be readily recognized, classified and quantified using the methods presented. It is concluded that these methods are clinically practical and show promise of becoming a useful clinical tool for the treatment of dysarthrias.<>",Date Added to IEEEXplore:06 August 2002
7178790,Automatic detection of voice onset time in dysarthric speech,"Although a number of speech disorders reflect varying involvement of brain areas, recently published automatic speech analyses have primarily been limited to hypokinetic dysarthria in Parkinson's disease (PD). Therefore, the aim of the present study was to provide an automatic algorithm suitable for the assessment of voice onset time (VOT) in various dysarthria types. Twenty-four PD participants with hypokinetic dysarthria and 40 Huntington's disease (HD) subjects with hyperkinetic dysarthria were included. These two types of dysarthria were selected in the design of a robust algorithm as they contain most of the dysarthric patterns found among all dysarthria subtypes. For a 10 ms threshold, the proposed algorithm reached approximately 90% accuracy in PD speakers and 80% accuracy in HD speakers. The accuracy of 80% obtained in HD was superior to the performance of 55% achieved by a previous algorithm designed particularly for hypokinetic dysarthria in PD.",Date Added to IEEEXplore:06 August 2015
10915057,Hybrid CNN-GRU Model for Predicting Dysarthric Speech Using Deep Learning Approaches,"This research study presents the novel method for dysarthria speech detection and classification. In order to create systems that are capable of accurately categorizing speech patterns that are impacted by dysarthria, it is vital to have speech detection and classification capabilities for dysarthria. The condition known as dysarthria, which is characterized by difficulties in articulation, phonation, resonance, and prosody, is a disorder that leads to a reduction in the intelligibility of speech. This method of classification necessitates the development of machine learning or deep learning models, more specifically a combination of Convolutional Neural Networks (CNN) and Gated Recurrent Units (GRU), in order to accomplish the objective of automatically classifying dysarthric speech into categories such as severity or presence/absence of dysarthria. The primary purpose of dysarthria speech categorization is to bring about the development of a reliable instrument that can be used for identifying, categorizing, and managing dysarthric speech. As an added benefit, the classification of dysarthric speech allows for the adaptation of treatment methods and the acquisition of information about the underlying causes of the condition. The individuals who are affected by this speech difficulty are the target audience for this strategy, which ultimately aims to enhance the quality of life of those individual. The study addresses the challenge of classifying dysarthric speech caused by neurological impairments using a Hybrid CNN-GRU model, achieving accuracies of 70.3% (high-severity), 38% (severe-moderate), 34.5% (moderate), and 10.3% (mild dysarthria).",Date Added to IEEEXplore:13 March 2025
8283503,Improving Acoustic Models in TORGO Dysarthric Speech Database,"Assistive speech-based technologies can improve the quality of life for people affected with dysarthria, a motor speech disorder. In this paper, we explore multiple ways to improve Gaussian mixture model and deep neural network (DNN) based hidden Markov model (HMM) automatic speech recognition systems for TORGO dysarthric speech database. This work shows significant improvements over the previous attempts in building such systems in TORGO. We trained speaker-specific acoustic models by tuning various acoustic model parameters, using speaker normalized cepstral features and building complex DNN-HMM models with dropout and sequence-discrimination strategies. The DNN-HMM models for severe and severe-moderate dysarthric speakers were further improved by leveraging specific information from dysarthric speech to DNN models trained on audio files from both dysarthric and normal speech, using generalized distillation framework. To the best of our knowledge, this paper presents the best recognition accuracies for TORGO database till date.",N/A
8846717,Automated Assessment of Oral Diadochokinesis in Multiple Sclerosis Using a Neural Network Approach: Effect of Different Syllable Repetition Paradigms,"Slow and irregular oral diadochokinesis represents an important manifestation of spastic and ataxic dysarthria in multiple sclerosis (MS). We aimed to develop a robust algorithm based on convolutional neural networks for the accurate detection of syllables from different types of alternating motion rate (AMR) and sequential motion rate (SMR) paradigms. Subsequently, we explored the sensitivity of AMR and SMR paradigms based on voiceless and voiced consonants in the detection of speech impairment. The four types of syllable repetition paradigms including /ta/, /da/, /pa/-/ta/-/ka/, and /ba/-/da/-/ga/ were collected from 120 MS patients and 60 matched healthy control speakers. Our neural network algorithm was able to correctly identify the position of individual syllables with a very high average accuracy of 97.8%, with the correct temporal detection of syllable position of 87.8% for 10 ms and 95.5% for 20 ms tolerance value. We found significantly altered diadochokinetic rate and regularity in MS compared to controls across all types of investigated tasks (p <; 0.001 ). MS patients showed slower speech for SMR compared to AMR tasks, whereas voiced paradigms were more irregular. Objective evaluation of oral diadochokinesis using different AMR and SMR paradigms may provide important information regarding speech severity and pathophysiology of the underlying disease.",N/A
9317735,Utterance Verification-Based Dysarthric Speech Intelligibility Assessment Using Phonetic Posterior Features,"In the literature, the task of dysarthric speech intelligibility assessment has been approached through development of different low-level feature representations, subspace modeling, phone confidence estimation or measurement of automatic speech recognition system accuracy. This paper proposes a novel approach where the intelligibility is estimated as the percentage of correct words uttered by a speaker with dysarthria by matching and verifying utterances of the speaker with dysarthria against control speakers’ utterances in phone posterior feature space and broad phonetic posterior feature space. Experimental validation of the proposed approach on the UA-Speech database, with posterior feature estimators trained on the data from auxiliary domain and language, obtained a best Pearson's correlation coefficient (r) of 0.950 and Spearman's correlation coefficient (ρ) of 0.957. Furthermore, replacing control speakers’ speech with speech synthesized by a neural text-to-speech system obtained a bestrof 0.931 andρof 0.961.",N/A
9740922,Smart Voice Assistance for Speech disabled and Paralyzed People,"People who are paralyzed, confront numerous challenges in meeting their basic necessities on a daily basis. It is very difficult to understand the speech of people with dysarthria, amyotrophic lateral sclerosis (ALS) and similar conditions. Automatic speech command recognition system will enhance the lifestyle of people with voice disorder like dysarthria and paraplegics. The proposed work will convert the speech command of paralyzed people into text and send it to the care taker's mobile with the help of Twilio message services. Algorithms like Support Vector Machine (SVM) and Convolutional Neural network (CNN) model is used for speech command identification and speech to text conversion. CNN model yields an accuracy of 90.62%, whereas the SVM algorithm gives a very low accuracy. The developed TensorFlow model is deployed in the flask server.",Date Added to IEEEXplore:31 March 2022
8645286,A Tool for Training Speech Imitation Accuracy,"Dysarthria is a neurological motor speech disorder that commonly results in reduced intelligibility. Communication partners can learn to better understand the speech of someone with dysarthria through perceptual training. Vocal imitation of the degraded speech during perceptual training has been shown to elevate this learning. A tool that provides the learner with real-time feedback regarding the accuracy of their imitation attempts during training may further enhance this learning. We describe a training tool that compares dysarthric speech productions with the imitation attempts of healthy subjects, using a two-level dynamic warp that accounts for both spectral and temporal degradation. Feature vectors derived from both the spectrogram and LPC are examined.",Date Added to IEEEXplore:21 February 2019
10673628,Leveraging OpenAI Whisper Model to Improve Speech Recognition for Dysarthric Individuals,"Automatic Speech Recognition (ASR) systems are pivotal in facilitating human-technology interactions through voice commands. However, individuals with dysarthria face significant challenges in benefiting from these technologies due to their speech disorder. This paper proposes finetuning the Whisper model for Dysarthric Speech Recognition (DSR) by incorporating additional features extracted from Mel-frequency cepstral coefficients (MFCCs). By combining spectrograms and MFCCs within an attention mechanism, the model creates a richer feature representation, with spectrograms providing broader context and MFCCs highlighting crucial formant frequencies. The attention mechanism dynamically weighs the importance of each feature based on specific speech segments and dysarthric speech characteristics. Furthermore, a hierarchical attention approach is adopted, which encompasses a two-stage attention mechanism. This mechanism directs attention at both local and global levels, facilitating the capture of both fine-grained details and broader contextual information within the speech signal. This study involved the development and training of 45 speaker-adaptive dysarthric ASR systems. The proposed model achieves an average Word Recognition Accuracy (WRA) of 74.08%, showing a notable enhancement compared to the benchmark of 69.23%. The findings underscore the efficacy of the proposed approach in addressing dysarthria-related challenges in ASR systems.",Date Added to IEEEXplore:18 September 2024
10482963,Empirical Analysis of Machine Learning Models on Parkinson’s Speech Dataset,"Parkinson’s disease (PD) is a chronic and progressive neurodegenerative disorder that worsens over time. Diagnosing PD primarily relies on clinical assessments, which can be costly, time-consuming, and invasive. These evaluations may also be subjective and vulnerable to inaccuracy. Dysarthria, a common condition characterised by delayed and distorted speech, frequently coexists with PD. This opens up the possibility of using speech features for diagnostic reasons. This research paper explores different machine learning models trained on numerical data of changes in speech patterns due to Dysarthria. These models are based on classifiers such as Artificial Neural Networks (ANN), Multi-Layer Perceptron (MLP), Random Forests, and Decision Trees. Additionally, we compare the performance of a newly introduced HyperTab classifier with the existing models. Our findings demonstrate the significant potential of machine learning in diagnosing PD based on speech analysis. This progress holds the promise of creating a cost-effective tool to expedite disease detection. Furthermore, this research is of utmost importance in offering essential support to regions with limited access to specialized medical facilities.",Date Added to IEEEXplore:02 April 2024
1556613,"Experiments with fast Fourier transform, linear predictive and cepstral coefficients in dysarthric speech recognition algorithms using hidden Markov model","In this study, a hidden Markov Model was constructed and conditions were investigated that would provide improved performance for a dysarthric speech (isolated word) recognition system. The speaker dependant system was intended to act as an assistive/control tool. A small size vocabulary spoken by three cerebral palsy subjects was chosen. Fast Fourier transform, linear predictive, and Mel frequency cepstral coefficients extracted from data provided training input to several whole-word hidden Markov model configurations. The effect of model structure, number of states, and frame rates were also investigated. It was noted that a 10-state ergodic model using 15 msec frames was better than other configurations. Furthermore, it was found that a Mel cepstrum based model outperformed a fast Fourier transform and linear prediction based model. The system offers effective and robust application as a rehabilitation and/or control tool to assist dysarthric motor impaired individuals.",N/A
9747205,Experimental Investigation on STFT Phase Representations for Deep Learning-Based Dysarthric Speech Detection,"Mainstream deep learning-based dysarthric speech detection approaches typically rely on processing the magnitude spectrum of the short-time Fourier transform of input signals, while ignoring the phase spectrum. Although considerable insight about the structure of a signal can be obtained from the magnitude spectrum, the phase spectrum also contains inherent structures which are not immediately apparent due to phase discontinuity. To reveal meaningful phase structures, alternative phase representations such as the modified group delay (MGD) and instantaneous frequency (IF) spectra have been investigated in several applications. The objective of this paper is to investigate the applicability of the unprocessed phase, MGD, and IF spectra for dysarthric speech detection. Experimental results show that dysarthric cues are present in all considered phase representations. Further, it is shown that using phase representations as complementary features to the magnitude spectrum is beneficial for deep learning-based dysarthric speech detection, with the combination of magnitude and IF spectra yielding a high performance. The presented results should raise awareness in the research community about the potential of the phase spectrum for dysarthric speech detection and motivate research into novel architectures which optimally exploit magnitude and phase information.",Date Added to IEEEXplore:27 April 2022
9915287,Consonant-Vowel Transition Models Based on Deep Learning for Objective Evaluation of Articulation,"Spectro-temporal dynamics of consonant-vowel (CV) transition regions are considered to provide robust cues related to articulation. In this work, we propose an objective measure of precise articulation, dubbed the objective articulation measure (OAM), by analyzing the CV transitions segmented around vowel onsets. The OAM is derived based on the posteriors of a convolutional neural network pre-trained to classify between different consonants using CV regions as input. We demonstrate that the OAM is correlated with perceptual measures in a variety of contexts including (a) adult dysarthric speech, (b) the speech of children with cleft lip/palate, and (c) a database of accented English speech from native Mandarin and Spanish speakers.",N/A
10832261,Speech Recognition-Based Feature Extraction For Enhanced Automatic Severity Classification in Dysarthric Speech,"Due to the subjective nature of current clinical evaluation, the need for automatic severity evaluation in dysarthric speech has emerged. DNN models outperform ML models but lack user-friendly explainability. ML models offer explainable results at a feature level, but their performance is comparatively lower. Current ML models extract various features from raw waveforms to predict severity. However, existing methods do not encompass all dysarthric features used in clinical evaluation. To address this gap, we propose a feature extraction method that minimizes information loss. We introduce an ASR transcription as a novel feature extraction source. We finetune the ASR model for dysarthric speech, then use this model to transcribe dysarthric speech and extract word segment boundary information. It enables capturing finer pronunciation and broader prosodic features. These features demonstrated an improved severity prediction performance to existing features: balanced accuracy of 83.72%.",Date Added to IEEEXplore:16 January 2025
10463016,"Dysarthric Speech Recognition Using Pseudo-Labeling, Self-Supervised Feature Learning, and a Joint Multi-Task Learning Approach","In this paper, we investigate the use of the spontaneous speech of dysarthric people for training an automatic speech recognition (ASR) model for them. Although the spontaneous speech of dysarthric people can be collected relatively easily compared to script-reading speech, which is obtained by having them read a prepared script, labeling the spontaneous speech of dysarthric people is very difficult and costly. For training an ASR model using unlabeled speech data, pseudo-labeling and self-supervised feature learning have been studied as effective approaches; however, the effectiveness of these approaches has not been clear when they are applied to the unlabeled dysarthric speech. In addition, pseudo-labeling may not be effective since the pseudo-labels of dysarthric speech include many errors and are not reliable. In this paper, we evaluate the above two approaches for the dysarthric speech recognition, and we propose a multi-task learning approach, which combines these approaches to train an ASR model that is robust against the errors in the pseudo-labels. Experimental results using Japanese and English datasets demonstrated that all approaches are effective, but among them, the proposed multi-task learning approach showed the best performance.",N/A
10900367,VASIR: Open-Source Android Application for Visual Analog Scale Intelligibility Rating of Dysarthric Speech,"We present an open-source application for mobile devices called VASIR that provides a reliable and sensitive tool for accessible intelligibility rating using visual analog scales. VASIR is validated by examining the intelligibility in 23 Parkinson’s disease (PD) patients treated with subthalamic nucleus deep brain stimulation (DBS) in on and off stimulation states, and 23 healthy controls. Two rater groups of naive and expert listeners performed intelligibility ratings using the provided application. An excellent intraclass correlation was found in both the naive (ICC = 0.97) and expert (ICC = 0.95) groups. Both listener groups were able to confidently differentiate between the controls and DBS groups (p < 0.001) whereas only the expert group was able to find statistically significant differences in intelligibility between on and off stimulation states (p = 0.031).",N/A
608020,The Nemours database of dysarthric speech,"The Nemours database is a collection of 814 short nonsense sentences; 74 sentences spoken by each of 11 male speakers with varying degrees of dysarthria. Additionally, the database contains two connected-speech paragraphs produced by each of the 11 speakers. The database was designed to test the intelligibility of dysarthric speech before and after enhancement by various signal processing methods, and is available on CD-ROM. It can also be used to investigate general characteristics of dysarthric speech such as production error patterns. The entire database has been marked at the word level and sentences for 10 of the 11 talkers have been marked at the phoneme level as well. The paper describes the database structure and techniques adopted to improve the performance of a Discrete Hidden Markov Model (DHMM) labeler used to assign initial phoneme labels to the elements of the database. These techniques may be useful in the design of automatic recognition systems for persons with speech disorders, especially when limited amounts of training data are available.",Date Added to IEEEXplore:06 August 2002
9621959,Corpus Design and Automatic Speech Recognition for Deaf and Hard-of-Hearing People,"This study describes automatic speech recognition (ASR) for the deaf and hard-of-hearing people. In the relevant literature, ASR for the deaf has been studied in a manner similar to the recognition of speech by people with dysarthria. However, prior studies have been conducted over a small number of deaf speakers. Therefore, to date, it remains unclear how the performance of ASR varies with different speakers. We conducted phoneme recognition experiments using speech from 12 deaf students to obtain analytical results from the perspective of ASR performance.",Date Added to IEEEXplore:01 December 2021
10123751,Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals,"Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based “AI Listener” that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.",Date Added to IEEEXplore:19 May 2023
10447681,Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition,"Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)",Date Added to IEEEXplore:18 March 2024
7062754,Contemporary speech/speaker recognition with speech from impaired vocal apparatus,"Speech is the effective form of communication between human and its environment. Speech also has potential of being important mode of interaction with computer. This review paper deals with both speech and speaker recognition of persons with speech motor disorders. Normally speaker recognition consists of speaker verification and speaker identification. Speaker identification is the process of determining which registered speaker provides a given input sample. Speaker verification is the process of accepting or rejecting the identity claim of a speaker. On the other hand, the speech recognition system deals with the following challenges such as speech representation, feature extraction techniques, speech classifiers, databases and performance evaluation. Motor speech disorders are a class of speech disorder that disturbs the body's natural ability to speak. These disturbances vary in their etiology based on the integrity and integration of cognitive, neuromuscular, and musculoskeletal activities. There are various types of speech disorders like Apraxia, Cluttering (similar to stuttering), Dyspraxia, Dysarthria, Dysprosody and so on. The main objective of this review paper is to summarize and compare the well known methods used in various stages of speech and speaker recognition system.",Date Added to IEEEXplore:19 March 2015
8512796,Analysis of adverse effects of stimulation during DBS surgery by patient-specific FEM simulations,"Deep brain stimulation (DBS) represents today a well-established treatment for movement disorders. Nevertheless the exact mechanism of action of DBS remains incompletely known. During surgery, numerous stimulation tests are frequently performed in order to evaluate therapeutic and adverse effects before choosing the optimal implantation site for the DBS lead. Anatomical structures responsible for the induced adverse effects have been investigated previously, but only based on stimulation data obtained with the implanted DBS lead. The present study introduces a methodology to identify these anatomical structures during intraoperative stimulation tests based on patient-specific electric field simulations and visualization on the patient specific anatomy. The application to 4 patients undergoing DBS surgery and presenting dysarthria, paresthesia or pyramidal effects shows the different anatomical structures, which might be responsible for the adverse effects. Several of the identified structures have been previously described in the literature. To draw any statistically significant conclusions, the methodology has to be applied to further patients. Together with the visualization of the therapeutic effects, this new approach could assist the neurosurgeons in the future in choosing the optimal implant position.",Date Added to IEEEXplore:28 October 2018
10816941,Targeting Friedreich Ataxia: A Sustainable Path to Safer and Smarter Therapeutics Through Integrated Docking and Toxicology,"Friedreich's ataxia (FA) is an autosomal recessive disorder affecting the nervous and cardiovascular systems, characterized by progressive ataxia, dysarthria, and muscle weakness. It results from a GAA trinucleotide repeat expansion in the FXN gene, leading to epigenetic silencing via heterochromatin formation, which reduces frataxin production-a mitochondrial protein essential for iron-sulphur cluster biogenesis and cellular energy production. Frataxin deficiency causes oxidative stress and mitochondrial dysfunction. Current treatments offer limited effectiveness, primarily addressing symptoms. Our study aims to develop novel therapeutic candidates by leveraging computational protein-ligand docking through the Galaxy EU platform, targeting frataxin deficiencies. High-throughput docking and toxicology studies have identified several promising compounds with potential therapeutic efficacy, offering hope for more effective and sustainable treatments for FA.",Date Added to IEEEXplore:01 January 2025
10800580,Check Your Audio Data: Nkululeko for Bias Detection,"We present a new release of the software tool Nkululeko. New additions enable users to automatically perform sanity checks, data cleaning, and bias detection in the data based on machine learning predictions. Two open-source databases from the medical domain are investigated: the Androids de-pression corpus and the UASpeech dysarthria corpus. Results show that both databases have some bias, but not in a severe manner.",Date Added to IEEEXplore:20 December 2024
10781648,Extraction of patients subpopulations with psychiatric symptoms using a transformer architecture,"In this paper, we demonstrate a novel pipeline for identifying and extracting patient subpopulations from unstructured physician’s notes. We validate the method by extracting patients with psychiatric issues from a general patient population. This method first uses a clinical metathesaurus to select terms of interest from reports, then vectorizes the terms using a transformer model. These vectors’ dimensions are reduced using Uniform Manifold Approximation and Projection (UMAP), and the results grouped by optimal cluster selection methods. We demonstrate this technique on a freely-available collection of deidentified patient notes (MIMIC IV), extracting and clustering “mental or behavioral dysfunctions”. Our results show that it is possible to select user-defined groups of patients from unstructured text with minimal model oversight to group patients with similar profiles. In our study cohort, the models automatically segmented the patients into two groups: patients with more physical symptoms (alcohol/drug abuse, dysarthria, tongue-biting, eating disorders) and patients with mental/emotional symptoms. By detecting the underlying similarities in patient profiles, we believe this method can be utilized for symptom prediction tasks, as well as curating treatment plans based on their cluster profiles. Such a system can assist in clinical decision-making without the need for individually-created NLP models.",Date Added to IEEEXplore:17 December 2024
7034078,Notice of Violation of IEEE Publication Principles: A voice-input voice-output communication assists in favor of people with harsh verbal communication destruction,"Notice of Violation of IEEE Publication PrinciplesA Voice Input Voice Output Communication Assists in Favor of People with Harsh Verbal Communication Destruction by M. Babu and V. Dhilip Kumar in the Proceedings of the International Conference on Information Communication and Embedded Systems, February 2014 After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles. This paper is loosly paraphrased copy of the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission. Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article: A Voice-Input Voice-Output Communication Aid for People with Severe Speech Impairment by Mark S. Hawley, Stuart P. Cunningham, Phil D. Green, Pam Enderby, Rebecca Palmer, Siddharth Sehgal, and Peter O'Neill in the IEEE Transactions on Neural Systems and Rehabilitation Engineering, Vol 21, No 1 January 2013, pp. 23-31  An actual VOICE INPUT VOICE OUTPUT Correspondence Help identifies the actual disordered talk from the person as well as develops communications that are changed into artificial talk. Tests demonstrated this technique works within producing great acknowledgement overall performance (mean precision 98 percentage) upon extremely disordered talk, even if acknowledgement perplexity is actually elevated. The actual VOICE INPUT VOICE OUTPUT Communication Assists (VIVOCA) had been examined inside an area test through people with reasonable in order to serious dysarthria as well as verified that they'll utilize the gadget to create intelligible talk result through disordered talk enter. The actual test outlined a few problems that restrict the actual overall performance as we...",Date Added to IEEEXplore:09 February 2015
10782047,A Wearable System for Monitoring Neurological Disorder Events with Multi-Class Classification Model in Daily Life,"Dysphagia and dysarthria are the prominent sequelae of neurological disorders. Treatment and rehabilitation of these impairments necessitate continuously monitoring symptoms related to swallowing and speaking. However, current medical technologies require large and diverse equipment to record these symptoms, which are predominantly limited to clinical environments. In this study, we propose an innovative wearable system for distinguishing neurological disorder events using a mechano-acoustic (MA) sensor and multi-class ensemble classification model. The MA sensor exhibits a high sensitivity to neck vibration without any interference from ambient sounds. A multi-class classification model was also developed to discern the symptoms from the recorded signals accurately. The proposed classification model is an ensemble neural network trained on waveforms and mel spectrograms. As a result, we achieve a high classification accuracy of 91.94%, surpassing the performance of previous single neural networks.",Date Added to IEEEXplore:17 December 2024
10780592,Tongue-Trackpad: Tongue Rehabilitation Through Quantitative Personalized Visual-Feedback Intervention,"Effective rehabilitation of tongue movement relies on delivering personalized interventions and quantitatively assessing the intervention's impact. Current approaches primarily focus on enhancing tongue movement control and strength through oromotor exercises and audio feedback. Here, we introduce the Tongue- Trackpad a novel solution for tongue movement rehabilitation and progress tracking. The Tongue- Trackpad is a wireless intra-oral device that provides real-time visual feedback of tongue movement while quantifying the movement, thus enabling personalized interventions. In this preliminary feasibility study, a stroke survivor diag-nosed with dysarthria participated in eighteen sessions of per-sonalized visual-feedback pursuit intervention using the Tongue-Trackpad. The intervention was customized to the participant's tongue movement deficit areas identified during the initial evaluation. Despite the participant's severe speech disorder, characterized by a Diadochokinetic rate of zero, the results indicated a modest positive change in tongue movement both within a single session and over the intervention period. The results showed an average increase of 3.5±4.7%in coverage area, a reduction of 1.5±1.9%in the deficit area, and a 1.7±3.1 % reduction in the excess area within a single session. Similar modest trends were observed throughout the entire intervention period. Although further studies with more participants are needed for robust conclusions, this preliminary feasibility study suggests provided preliminary that providing personalized visual-feedback intervention using the Tongue- Trackpad holds promise for tongue movement rehabilitation.",Date Added to IEEEXplore:11 December 2024
4292395,Speech Formation Objectivisation Methods Development and Analysis for Correct Phonation Diagnostics,"In connection with various infringements speech production people have requirement for development of methods of an objective estimation of the vocal apparatus functional condition. Methods should be directed on revealing of the attributes forming articulation-acoustic characteristics of correct harmonization of themes. The given problem is especially important for children for whom increase speech pathologies (legasthenia, dysgraphia, dysarthria, phonasthenia, hoarseness, etc.) is observed. As results of the carried out research, it is possible to come to a conclusion that for the analysis of voice pathologies it is expedient to use the offered method.",Date Added to IEEEXplore:20 August 2007
10340263,The Change of Vocal Tract Length in People with Parkinson’s Disease,"Hypokinetic dysarthria is one of the early symptoms of Parkinson’s disease (PD) and has been proposed for early detection and also for monitoring of the progression of the disease. PD reduces the control of vocal tract muscles such as the tongue and lips and, therefore the length of the active vocal tract is altered. However, the change in the vocal tract length due to the disease has not been investigated. The aim of this study was to determine the difference in the apparent vocal tract length (AVTL) between people with PD and age-matched control healthy people. The phoneme, /a/ from the UCI Parkinson’s Disease Classification Dataset and the Italian Parkinson’s Voice and Speech Dataset were used and AVTL was calculated based on the first four formants of the sustained phoneme (F1-F4). The results show a correlation between Parkinson’s disease and an increase in vocal tract length. The most sensitive feature was the AVTL calculated using the first formants of sustained phonemes (F1). The other significant finding reported in this article is that the difference is significant and only appeared in the male participants. However, the size of the database is not sufficiently large to identify the possible confounding factors such as the severity and duration of the disease, medication, age, and comorbidity factors.Clinical relevance—The outcomes of this research have the potential to improve the identification of early Parkinsonian dysarthria and monitor PD progression.",Date Added to IEEEXplore:11 December 2023
10023085,Weak-Supervised Dysarthria-Invariant Features for Spoken Language Understanding Using an Fhvae and Adversarial Training,"The scarcity of training data and the large speaker variation in dysarthric speech lead to poor accuracy and poor speaker generalization of spoken language understanding systems for dysarthric speech. Through work on the speech features, we focus on improving the model generalization ability with limited dysarthric data. Factorized Hierarchical Variational Auto-Encoders (FHVAE) trained unsupervisedly have shown their advantage in disentangling content and speaker representations. Earlier work showed that the dysarthria shows in both feature vectors. Here, we add adversarial training to bridge the gap between the control and dysarthric speech data domains. We extract dysarthric and speaker invariant features using weak supervision. The extracted features are evaluated on a Spoken Language Understanding task and yield a higher accuracy on unseen speakers with more severe dysarthria compared to features from the basic FHVAE model or plain filterbanks.",Date Added to IEEEXplore:27 January 2023
