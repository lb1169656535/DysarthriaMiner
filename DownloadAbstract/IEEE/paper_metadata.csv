Title,Abstract,Keywords,Links,Citation
"Quantitative Assessment of Syllabic Timing Deficits in Ataxic Dysarthria,","Parametric analysis of Cerebellar Dysarthria (CD) may be valuable and more informative compared to its clinical assessment. A quantifiable estimation of the timing deficits in repeated syllabic utterance is described in the current study. Thirty-five individuals were diagnosed with cerebellar ataxia to varying degrees and twenty-six age-matched healthy controls were recruited. To automatically detect the local maxima of each syllable in the recorded speech files, a topographic prominence incorporated concept is designed. Subsequently, four acoustic features and eight corresponding parametric measurements are extracted to identify articulatory deficits in ataxic dysarthria. A comparative study on the behaviour of these measures for dysarthric and non-dysarthric subjects is presented in this paper. The results are further explored using a dimensionreduction tool (Principal Component Analysis) to emphasize variation and bring out the strongest discriminating patterns in our feature dataset.","keywords: {Feature extraction;Principal component analysis;Acoustic measurements;Timing;Eigenvalues and eigenfunctions;Rhythm;dysarthria;speech disorder;repeated syllable;cerebellar ataxia;topographic prominence},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512311&isnumber=8512178,"B. Kashyap, P. N. Pathirana, M. Horne, L. Power and D. Szmulewicz, ""Quantitative Assessment of Syllabic Timing Deficits in Ataxic Dysarthria,"" 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, USA, 2018, pp. 425-428, doi: 10.1109/EMBC.2018.8512311."
"Non-invasive stroke diagnosis using speech data from dysarthria patients,","Acute Ischemic Stroke (AIS) is a major cause of disability and can lead to death in severe cases. A common symptom of AIS, dysarthria, significantly impacts the quality of life of patients. In this study, we developed a deep learning model using dysarthria data for cost-effective and non-invasive brain stroke diagnosis. We utilized models such as ResNet50, InceptionV4, ResNeXt50, SEResNeXt18, and AttResNet50 to effectively extract and classify speech features indicative of stroke symptoms. These models demonstrated high performance, with Sensitivity, Specificity, Precision, Accuracy, and F1-score values reaching 96.77%, 96.08%, 92.82%, 95.52%, and 93.82%, respectively. Our approach offers a non-invasive, cost-effective alternative for early stroke detection, with potential for further accuracy improvements through additional research. This method promises rapid, economical early diagnosis, which could positively impact long-term treatment and healthcare options.","keywords: {Deep learning;Accuracy;Sensitivity;Medical services;Learning (artificial intelligence);Stroke (medical condition);Brain modeling;Feature extraction;Data models;Residual neural networks;Artificial intelligence;Stroke;Dysarthria;Diagnosis;Deep learning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10781716&isnumber=10781494,"S. B. Mun, Y. J. Kim and K. G. Kim, ""Non-invasive stroke diagnosis using speech data from dysarthria patients,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10781716."
"Fine-Tuning Pre-Trained Audio Models for Dysarthria Severity Classification: A Second Place Solution in the Multimodal Dysarthria Severity Classification Challenge,","Dysarthria, a neurological disorder affecting speech, poses significant challenges for automatic diagnosis and severity classification due to its complexity and the scarcity of relevant datasets. This study addresses these challenges by leveraging the dataset provided by the ISCSLP 2024 Multimodal Dysarthria Severity Classification Challenge. Our approach primarily involves two key steps: splitting the training set into training and validation subsets, and fine-tuning pre-trained au-dio models to adapt them to the task. To ensure robust evaluation and prevent label leakage, we employed the StratifiedGroupKFold function from sklearn for dataset splitting, ensuring that training and validation sets do not share participants. This method allowed us to maintain consistent statistical distributions across folds. For model finetuning, we selected two pre-trained audio models, w2v-bert-2.0 and whisper-Iarge-v3, and fine-tuned them on different folds of the dataset. Our results indicate that the w2v-bert-2.0 model fine-tuned on fold-2 achieved the highest performance, with a validation Fl-score of 0.699 and an online score of 7.7452. The experimental results, including confusion matrices and embedding distributions, highlight the model's ability to distinguish between different severity levels of dysarthria. Our approach achieved a commendable second place in the competition’ demonstrating the effectiveness of fine-tuning pre-trained audio models for this task. Future work will explore the integration of multimodal data and multi-task learning strategies to further enhance model performance.","keywords: {Training;Neurological diseases;Adaptation models;Magnetic resonance imaging;Statistical distributions;Metadata;Multitasking;Complexity theory;Optimization;Overfitting;Dysarthria Severity Classification;Dataset Splitting;Pre-trained Audio Models;Fine-Tuning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800228&isnumber=10799969,"W. Dai, M. Li, Y. He and Y. Zhu, ""Fine-Tuning Pre-Trained Audio Models for Dysarthria Severity Classification: A Second Place Solution in the Multimodal Dysarthria Severity Classification Challenge,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 151-153, doi: 10.1109/ISCSLP63861.2024.10800228."
"Automated Detection and Severity Assessment of Dysarthria using Raw Speech,","Dysarthria is a medical condition that impairs an individual’s ability to speak clearly due to muscle weakness or paralysis. To diagnose and monitor dysarthria severity, this article proposes the use of a deep learning model that utilizes raw speech waveforms. This approach eliminates the need for feature engineering and enhances the model’s ability to handle noise and speech variability. The proposed system was compared to a standard convolutional neural network (CNN) model and was found to perform better in both dysarthria severity classification and dysarthria/healthy control classification tasks. The results indicate that the SincNet model achieved an accuracy of 95.7% and 99.6% in these tasks, respectively. The proposed system can aid clinicians in diagnosing and monitoring dysarthria severity and could have broader applications in speech-related disorders. The study emphasizes the importance of using raw waveform-based models for speech analysis and demonstrates the effectiveness of SincNet in automatic dysarthria/healthy control classification and dysarthria severity assessment.","keywords: {Deep learning;Speech analysis;Neural networks;Speech enhancement;Muscles;Paralysis;Convolutional neural networks;Dysarthria severity level assessment;Speech-related disorders;Deep learning;SincNet;Raw waveforms},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10307923&isnumber=10306339,"K. Radha and M. Bansal, ""Automated Detection and Severity Assessment of Dysarthria using Raw Speech,"" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-7, doi: 10.1109/ICCCNT56998.2023.10307923."
"Investigation on articulatory and acoustic characteristics of dysarthria,","Direct measurement of articulation contributes to figure out the mechanism of dysarthric speech production accurately, comparing to analyzing acoustic signal alone. This study makes a statistical comparison between dysarthric and normal speech, and investigates and analyzes dysarthric characteristics based on articulatory and acoustic data of continuous English utterance. The distribution of the articulatory point is calculated for each vowel, where the vowels show a relatively smaller region for dysarthria than for normal speech. This implies that the speakers with dysarthria may have some difficulties to fully use the articulatory space as the speakers without dysarthria. The rhythm and sound source are analyzed using autocorrelation function, power spectrum and linear prediction coding. The results reveal that the speakers without dysarthria can keep steady rhythm and energy in uttering consonant-vowel repetition sequences but it is hard for the speakers with dysarthria to maintain the stability. Meanwhile, the dysarthric sound source shows unstable period of the fundamental frequency, which reflects that the speakers with dysarthria could not control the glottis movement well.","keywords: {Speech;Speech recognition;Rhythm;Correlation;Standards;Tongue;dysarthria;speech analysis;articulator movement},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6936631&isnumber=6936571,"C. Zhang, J. Dang, J. Zhang and J. Wei, ""Investigation on articulatory and acoustic characteristics of dysarthria,"" The 9th International Symposium on Chinese Spoken Language Processing, Singapore, 2014, pp. 326-330, doi: 10.1109/ISCSLP.2014.6936631."
"Evolving Diagnostic Techniques for Speech Disorders: Investigating Dysarthria Classification Through DenseNet201 CNN Framework,","The primary subject of this research work pertains to dysarthria, a prevalent speech impairment observed in individuals diagnosed with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Prompt detection of dysarthria is essential for effective therapies and improved patient results, as it significantly impacts communication proficiency. The research employed a DenseNet201 Convolutional Neural Network (CNN) to categorize speech using the TORGO database, which has 2000 samples of individuals with and without dysarthria, representing various genders. The dataset comprises individuals of both genders, encompassing both dysarthric and non-dysarthric individuals. Each dataset consists of 500 samples, which were collected during distinct sessions. The DenseNet201 convolutional neural network (CNN) technique has a notable accuracy rate of 96% in distinguishing between cases of dysarthria and non-dysarthria. This outcome underscores the potential of deep learning technology in aiding the timely identification of dysarthria and facilitating fast interventions for affected individuals. This work provides valuable insights on the progress of diagnostic capabilities and offers a potential avenue for enhancing the well-being of those experiencing speech impairments.","keywords: {Deep learning;Databases;Computational modeling;Medical treatment;Speech enhancement;Robustness;Telecommunication computing;Artificial Intelligence;Deep Learning;DenseNet201 Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10550236&isnumber=10550194,"K. Mittal, K. S. Gill, S. Malhotra and S. Devliyal, ""Evolving Diagnostic Techniques for Speech Disorders: Investigating Dysarthria Classification Through DenseNet201 CNN Framework,"" 2024 International Conference on Communication, Computing and Internet of Things (IC3IoT), Chennai, India, 2024, pp. 1-6, doi: 10.1109/IC3IoT60841.2024.10550236."
"Analysis of Features for Dysarthria Severity Classification from Speech,","Dysarthria is a disorder that affects the ability of an individual to speak clearly. Diagnosis of the severity of dysarthria can aid in providing appropriate therapy to the individual. Therefore, the current work focuses on identifying features that can be used to estimate the severity of dysarthria. In this regard, two feature sets are considered, namely DisVoice and OpenSMILE, and the genetic algorithm is used to identify the most suitable list of features from both feature sets using 3 speech corpora, namely SSN-TDSC, Torgo, and UA Speech corpora. In order to test the effectiveness of these features, classifiers are trained on each feature set as a whole and on the features identified by the genetic algorithm, and their performance compared. It is observed that articulatory and prosodic features are most important in the diagnosis of severity of dysarthria. A maximum accuracy of 85% is obtained with the shortlisted DisVoice features and 97% with the OpenSMILE features.","keywords: {Accuracy;Medical treatment;Classification algorithms;Genetic algorithms;IEEE Regions;dysarthria;feature selection;genetic algorithm;severity classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10902941&isnumber=10902666,"P. H. Keerthi, P. Vijayalakshmi, A. R. Gladston and T. Nagarajan, ""Analysis of Features for Dysarthria Severity Classification from Speech,"" TENCON 2024 - 2024 IEEE Region 10 Conference (TENCON), Singapore, Singapore, 2024, pp. 335-338, doi: 10.1109/TENCON61640.2024.10902941."
"Multi-Modal Dysarthria Severity Assessment Using Dual-Branch Feature Decoupling Network and Mixed Expert Framework,","Dysarthria, a motor speech disorder resulting from neurological damage, significantly impairs an individual's ability to articulate words. Assessing the severity of dysarthria is crucial for effective therapeutic interventions and rehabilitation strategies. However, current methods are time-consuming and subjective, leading to potential inconsistencies. This study proposed a dual-branch feature decoupling network and mixed expert framework for the automatic diagnosis and assessment of dysarthria. The proposed hybrid network architecture integrates a ResNet-based branch for audio data with an attention module and a 3D ResNet branch for video data, and was evaluated on the Multimodal Dysarthria Severity Assessment Challenge (MDSA) dataset, achieving significant performance improvements and ranking first in the challenge with a best score of 8.7404. The study's findings highlight the efficacy of the proposed framework in capturing essential features from multimodal data, contributing to more accurate and reliable assessments of dysarthria.","keywords: {Measurement;Visualization;Three-dimensional displays;Accuracy;Neural networks;Network architecture;Motors;Acoustics;Complexity theory;Reliability;Multimodal Fusion;Dysarthria Severity Assessment;Convolutional Neural Network;Feature Decoupling Network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800159&isnumber=10799969,"S. Liang and Y. Gu, ""Multi-Modal Dysarthria Severity Assessment Using Dual-Branch Feature Decoupling Network and Mixed Expert Framework,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 126-130, doi: 10.1109/ISCSLP63861.2024.10800159."
"Spectral Analysis of Vowels and Fricatives at Varied Levels of Dysarthria Severity for Amyotrophic Lateral Sclerosis,","Dysarthria due to Amyotrophic Lateral Sclerosis (ALS) affects the acoustic characteristics of different speech sounds. The effects intensify with increasing severity leading to the collapse of the acoustic space of the affected individuals. With an aim to characterize such changes in the acoustic space, this paper studies the variations in band-specific and full-band spectral properties of 4 sustained vowels (/a/, /i/, /o/, /u/) and 3 sustained fricatives (/s/, /sh/, /f/) at different dysarthria severity levels. Effect of dysarthria on spectral features of these phonemes are not well explored. Statistical comparison of these features among different severities for the phonemes considered and among different vowels/fricatives for every severity level using speech data from 119 ALS and 40 healthy subjects indicate the followings. Though all band-specific and full-band features of the three fricatives and most of those features for the four vowels become statistically similar at high severity levels, certain features remain distinguishable. Spectral differences in 0-2 kHz band between /a/ and the other vowels and in the 2-6 kHz band between /a/ and /o/, /u/ persist through all severity levels. Moreover, properties of /f/ remain mostly unchanged with increasing dysarthria severity levels.","keywords: {Signal processing;Acoustics;Speech processing;Spectral analysis;Diseases;Amyotrophic Lateral Sclerosis;dysarthria;severity;vowels;fricatives},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10448175&isnumber=10445803,"C. V. Thirumala Kumar et al., ""Spectral Analysis of Vowels and Fricatives at Varied Levels of Dysarthria Severity for Amyotrophic Lateral Sclerosis,"" ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 12767-12771, doi: 10.1109/ICASSP48485.2024.10448175."
"Advancing Speech Disorder Diagnostics: A Comprehensive Study on Dysarthria Classification with CNN,","This study article focuses on dysarthria, a speech problem that is often seen in patients with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early identification of dysarthria is crucial for successful treatments and better patient outcomes, since it has a substantial influence on communication ability. Using the TORGO database, which consists of 2000 samples of persons with and without dysarthria, spanning different genders, the research use a Convolutional Neural Network (CNN) to classify speech. The dataset includes both dysarthric and non-dysarthric individuals of both genders, with 500 samples each, captured during separate sessions. The CNN-based technique demonstrates an impressive 96% accuracy in differentiating between dysarthric and non-dysarthric instances, highlighting the promise of deep learning technology in assisting with the early detection of dysarthria and enabling prompt therapies for afflicted people. This study makes significant contributions to the advancement of diagnostic capacities and presents a potential opportunity to improve the quality of life for those with speech difficulties.","keywords: {Deep learning;Cerebral palsy;Technological innovation;Accuracy;Databases;Medical treatment;Market research;Artificial Intelligence;Deep Learning;Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10673651&isnumber=10673418,"K. Mittal, K. Singh Gill, P. Aggarwal, R. Singh Rawat and G. Sunil, ""Advancing Speech Disorder Diagnostics: A Comprehensive Study on Dysarthria Classification with CNN,"" 2024 Asia Pacific Conference on Innovation in Technology (APCIT), MYSORE, India, 2024, pp. 1-5, doi: 10.1109/APCIT62007.2024.10673651."
"Progressing Speech Disorder Identification: A Thorough Investigation into Dysarthria Categorization Utilizing ResNet50 CNN,","The speech issue of dysarthria, which is frequently observed in individuals with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS), is the subject of this research article. Timely detection of dysarthria is essential for effective interventions and improved patient results, as it significantly impacts communication proficiency. The study employs the TORGO database, which has 2000 samples of individuals with and without dysarthria, encompassing various genders. The researchers utilize a ResNet50 Convolutional Neural Network (CNN) to categorize speech. The dataset comprises individuals of both genders, encompassing both dysarthric and non-dysarthric individuals. Each group consists of 500 samples, which were recorded over distinct sessions. The CNN-based technique achieves a remarkable 96% accuracy in distinguishing between dysarthric and non-dysarthric instances. This underscores the potential of deep learning technology in aiding the early identification of dysarthria and facilitating timely treatments for affected individuals. This study provides valuable contributions to the enhancement of diagnostic capabilities and offers a potential avenue for enhancing the quality of life for individuals experiencing speech impairments.","keywords: {Deep learning;Cerebral palsy;Accuracy;Databases;Medical treatment;Speech enhancement;Robustness;Artificial Intelligence;Deep Learning;ResNet50 Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10581063&isnumber=10580998,"M. Singla, K. S. Gill, D. Upadhyay and S. Devliyal, ""Progressing Speech Disorder Identification: A Thorough Investigation into Dysarthria Categorization Utilizing ResNet50 CNN,"" 2024 International Conference on Intelligent Systems for Cybersecurity (ISCS), Gurugram, India, 2024, pp. 1-5, doi: 10.1109/ISCS61804.2024.10581063."
"Enhancing the Diagnosis of Speech Disorders: An In-Depth Investigation into Dysarthria Classification Using the ResNet18 Model,","This research article concentrates on dysarthria, a speech impediment commonly observed in individuals with conditions such as cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early detection of dysarthria is essential for effective interventions and improved patient outcomes, as it significantly impacts communication abilities. Employing the TORGO database, comprising 2000 samples from individuals with and without dysarthria, encompassing diverse genders, the study utilizes a Convolutional Neural Network (ResNet18 Model) for speech classification. The dataset encompasses both dysarthric and non-dysarthric subjects of both genders, with 500 samples each, recorded in separate sessions. The ResNet18 Model-based approach demonstrates an impressive 96% accuracy in distinguishing between dysarthric and non-dysarthric instances, underscoring the potential of deep learning technology in aiding early dysarthria detection and facilitating timely interventions. This investigation significantly contributes to advancing diagnostic capabilities and presents a promising avenue to enhance the quality of life for individuals grappling with speech difficulties.","keywords: {Deep learning;Cerebral palsy;Accuracy;Databases;Speech enhancement;Market research;Robustness;Artificial Intelligence;Deep Learning;Convolutional Neural Network (ResNet18 Model);Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10625627&isnumber=10624740,"K. Mittal, K. S. Gill, K. Rajput and V. Singh, ""Enhancing the Diagnosis of Speech Disorders: An In-Depth Investigation into Dysarthria Classification Using the ResNet18 Model,"" 2024 IEEE International Conference on Information Technology, Electronics and Intelligent Communication Systems (ICITEICS), Bangalore, India, 2024, pp. 1-5, doi: 10.1109/ICITEICS61368.2024.10625627."
"Statistical Analysis of Speech Disorder Specific Features to Characterise Dysarthria Severity Level,","Poor coordination of the speech production subsystems due to any neurological injury or a neuro-degenerative disease leads to dysarthria, a neuro-motor speech disorder. Dysarthric speech impairments can be mapped to the deficits caused in phonation, articulation, prosody, and glottal functioning. With the aim of reducing the subjectivity in clinical evaluations, many automated systems are proposed in the literature to assess the dysarthria severity level using these features. This work aims to analyse the suitability of these features in determining the severity level. A detailed investigation is done to rank these features for their efficacy in modelling the pathological aspects of dysarthric speech, using the technique of paraconsistent feature engineering. The study used two dysarthric speech databases, UA-Speech and TORGO. It puts light into the fact that both the prosody and articulation features are best useful for dysarthria severity estimation, which was supported by the classification accuracies obtained on using different machine learning classifiers.","keywords: {Pathology;Statistical analysis;Databases;Estimation;Production;Machine learning;Signal processing;dysarthria severity estimation;paraconsistent feature engineering;statistical analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10095366&isnumber=10094560,"A. A. Joshy, P. N. Parameswaran, S. R. Nair and R. Rajan, ""Statistical Analysis of Speech Disorder Specific Features to Characterise Dysarthria Severity Level,"" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095366."
"Identification of Cerebellar Dysarthria with SISO Characterisation,","Quantitative identification of dysarthria plays a major role in the classification of its severity. This paper quantitatively analyses several components of cerebellar dysarthria. The methodology described in this study will be extended to other types of dysarthria via systematic analysis. The speech production model is characterized as a second-order singleinput and single-output (SISO), linear, time-invariant (LTI) system in our study. A comparative study on the behavior of the damping ratio and resonant frequency for dysarthric and non-dysarthric subjects is presented. The results are further analyzed using the Principal component analysis (PCA) technique to emphasize the variation and uncover strong patterns in the selected features. The effects of some other related factors like decay time and Q-factor are also highlighted.","keywords: {Speech;Damping;Feature extraction;Resonant frequency;Production;Diseases;Transfer functions;dysarthria;speechdisorder;damping;resonantfrequency;secondordermodel;decayrate;vocaltract},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8251336&isnumber=8251248,"B. Kashyap, D. Szmulewicz, P. N. Pathirana, M. Horne and L. Power, ""Identification of Cerebellar Dysarthria with SISO Characterisation,"" 2017 IEEE 17th International Conference on Bioinformatics and Bioengineering (BIBE), Washington, DC, USA, 2017, pp. 479-485, doi: 10.1109/BIBE.2017.000-8."
"CNN-Driven Innovations in Dysarthria Classification and Speech Disorder Management,","This research article addresses dysarthria, a speech disorder frequently observed in individuals with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early detection of dysarthria is essential for effective treatment and improved patient outcomes, as it significantly impacts communication abilities. Utilizing the TORGO database, which comprises 2000 speech samples from both dysarthric and non-dysarthric individuals across various genders, this study employs a Convolutional Neural Network (CNN) for classification. The dataset includes 500 samples each from both groups, collected in different sessions. The CNN approach achieves an impressive 96% accuracy in distinguishing dysarthric from non-dysarthric speech, showcasing the potential of deep learning technology in facilitating early dysarthria diagnosis and enabling timely interventions. This study makes substantial contributions to enhancing diagnostic capabilities and offers a promising avenue for improving the quality of life for individuals with speech disorders.","keywords: {Deep learning;Computers;Cerebral palsy;Technological innovation;Electric potential;Accuracy;Databases;Speech enhancement;Convolutional neural networks;Medical diagnosis;Artificial intelligence;deep learning;convolutional neural network (CNN) model;model training;dysarthria classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10739183&isnumber=10738866,"A. Kheterpal, K. S. Gill, D. Upadhyay and S. Devliyal, ""CNN-Driven Innovations in Dysarthria Classification and Speech Disorder Management,"" 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT), Greater Noida, India, 2024, pp. 1-5, doi: 10.1109/ICEECT61758.2024.10739183."
"Constant Q Transform for Audio-Visual Dysarthria Severity Assessment,","This paper describes an automatic dysarthria severity assessment system submitted for the MDSA2024 challenge. The system utilizes audio and visual features to classify the severity of dysarthria. We investigate the effectiveness of different acoustic feature combinations, such as Mel spectrogram, filterbank and constant Q transform. Different encoders for audio data processing are compared in the experiments. The results demonstrate that the Conformer encoder achieves superior performance compared to the Densenet encoder on pure audio features. We further explore model-level fusion by combining audio and visual embeddings, leading to improvements in precision’ recall, and accuracy metrics. The best-achieved score on the test set is 7.5579, with an individual F1-score of 0.7028. The findings suggest that the proposed system with Conformer encoder and model-level fusion holds promise for automatic dysarthria assessment.","keywords: {Measurement;Visualization;Accuracy;Filter banks;Transforms;Data processing;Acoustics;Data models;Spectrogram;dysarthria severity assessment;acoustic feature combination;multimodalities model-level fusion},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800051&isnumber=10799969,"G. Sun and L. Wang, ""Constant Q Transform for Audio-Visual Dysarthria Severity Assessment,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 146-150, doi: 10.1109/ISCSLP63861.2024.10800051."
"Dysarthria Detection with Deep Representation Learning for Patients with Parkinson’s Disease,","Dysarthria is a very common motor speech symptom in Parkinson’s disease impairing normal communications of patients. Detection of dysarthria could assist clinical diagnosis and intervention of Parkinson’s disease, provide monitoring approach for treatment-related side effects, and lead to effective speech therapy to prevent further communication and social deficits. Applying machine learning techniques to speech analysis for patients offers more resource-efficient, accessible and objective tools for screening and assessment of dysarthria. In this study, we constructed a multi-task speech dataset recorded from 600 participants with high data diversity to facilitate the development of detection models. We established a remote data acquisition and end-to-end prediction pipeline with deep representation learning, compared the performance with different feature-based learning and classification methods, and achieved a superior accuracy of over 90%. Different affecting factors were analyzed for model performance. Our proposed framework demonstrates the potential of developing and deploying an automated self-monitoring approach of dysarthria for patients with Parkinson’s disease, which could benefit a large-scale population and their disease managements.","keywords: {Representation learning;Speech analysis;Pipelines;Data acquisition;Medical treatment;Multitasking;Motors;Data models;Monitoring;Diseases;Parkinson’s disease;dysarthria;deep representation learning;speech;detection},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10781584&isnumber=10781494,"C. Zhang, C. Gong and Y. Sui, ""Dysarthria Detection with Deep Representation Learning for Patients with Parkinson’s Disease,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-5, doi: 10.1109/EMBC53108.2024.10781584."
"Automatic assessment of dysarthria severity level using audio descriptors,","Dysarthria is a motor speech impairment, often characterized by speech that is generally indiscernible by human listeners. Assessment of the severity level of dysarthria provides an understanding of the patient's progression in the underlying cause and is essential for planning therapy, as well as improving automatic dysarthric speech recognition. In this paper, we propose a non-linguistic manner of automatic assessment of severity levels using audio descriptors or a set of features traditionally used to define timbre of musical instruments and have been modified to suit this purpose. Multitapered spectral estimation based features were computed and used for classification, in addition to the audio descriptors for timbre. An Artificial Neural Network (ANN) was trained to classify speech into various severity levels within Universal Access dysarthric speech corpus and the TORGO database. An average classification accuracy of 96.44% and 98.7% was obtained for UA speech corpus and TORGO database respectively.","keywords: {Speech;Databases;Harmonic analysis;Estimation;Timbre;Speech recognition;Dysarthria;Severity level;Automatic assessment;Audio descriptors;Multi-taper},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7953122&isnumber=7951776,"C. Bhat, B. Vachhani and S. K. Kopparapu, ""Automatic assessment of dysarthria severity level using audio descriptors,"" 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), New Orleans, LA, USA, 2017, pp. 5070-5074, doi: 10.1109/ICASSP.2017.7953122."
"Automated Dysarthria Severity Classification Using Deep Learning Frameworks,","Dysarthria is a neuro-motor speech disorder that renders speech unintelligible, in proportional to its severity. Assessing the severity level of dysarthria, apart from being a diagnostic step to evaluate the patient's improvement, is also capable of aiding automatic dysarthric speech recognition systems. In this paper, a detailed study on dysarthia severity classification using various deep learning architectural choices, namely deep neural network (DNN), convolutional neural network (CNN) and long short-term memory network (LSTM) is carried out. Mel frequency cepstral coefficients (MFCCs) and its derivatives are used as features. Performance of these models are compared with a baseline support vector machine (SVM) classifier using the UA-Speech corpus and the TORGO database. The highest classification accuracy of 96.18% and 93.24% are reported for TORGO and UA-Speech respectively. Detailed analysis on performance of these models shows that a proper choice of a deep learning architecture can ensure better performance than the conventionally used SVM classifier.","keywords: {Deep learning;Support vector machines;Neural networks;Speech recognition;Signal processing;Reliability;Mel frequency cepstral coefficient;dysarthria;intelligibility;automatic assessment;deep learning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9287741&isnumber=9287310,"A. A. Joshy and R. Rajan, ""Automated Dysarthria Severity Classification Using Deep Learning Frameworks,"" 2020 28th European Signal Processing Conference (EUSIPCO), Amsterdam, Netherlands, 2021, pp. 116-120, doi: 10.23919/Eusipco47968.2020.9287741."
"Deep Learning Based Speech Recognition for Hyperkinetic Dysarthria Disorder,","Speech recognition is a technology that aims to transform human speech into text and has applications in a variety of fields, including information technology, healthcare, automobiles, and others. This research explores the advantages of employing deep learning methods to provide individualized assistance technology solutions for people with dysarthria. We proposed a Convolutional Temporal Bidirectional Network (CTBNet) model for translating Russian Hyperkinetic Dysarthria Disorder (RHDD) speech into text. CTBNet encodes input audio features using 1D convolution layers, and BidirectionalGRU (Bi-GRU) layers. The encoder effectively captures both short-term and long-term spatial temporal information. More importantly, CTBNet includes with an attention-CTC decoder, that is responsible for producing output texts. We utilized a dataset of 2797 recordings of RHDD speech, with a cumulative recorded duration of 2 hours and 33 minutes, for training purposes. The CTBNet model has achieved a 15% Character Error Rate (CER) and a 59% Word Error Rate (WER) on the dataset proposed. Our experimental findings show superior performance compared to state-of-the-art models, namely, 3xCNN, and 3xLSTM, when evaluated on the same dataset.","keywords: {Training;Deep learning;Accuracy;Error analysis;Biological system modeling;Speech recognition;Transforms;dysarthria;recognition;encoder-decoder;deep learning;attention mechanism},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10584052&isnumber=10583970,"A. M. Hashan, C. R. Dmitrievich, M. A. Valerievich, D. D. Vasilyevich, K. N. Alexandrovich and B. B. Andreevich, ""Deep Learning Based Speech Recognition for Hyperkinetic Dysarthria Disorder,"" 2024 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT), Yekaterinburg, Russian Federation, 2024, pp. 012-015, doi: 10.1109/USBEREIT61901.2024.10584052."
"Developing a Hyperkinetic Dysarthria Speech Classification System using Residual Learning,","Hyperkinetic dysarthria abnormalities are exceedingly widespread across the worldwide populace. One of the most effective methodologies for disordered audio speech identification is classical deep learning technique. However, the efficiency of classification is impacted by the limitations imposed by the quality of the training dataset, such as expense and limited resources, data imbalance, and data annotation difficulties. Accordingly, we suggest implementing a residual learning framework (ResN et architecture) with various conditions and hyperparameters (batch size and learning rate) that improve the training process of deeper networks compared to earlier approaches. Pre-emphasis, windowing, and lifting techniques are applied to improve the quality of Mel-Frequency Cepstral Coefficients. In addition, we have introduced the hyperkinetic dysarthria speech dataset in the Russian language, consisting of 4 hours and 4 minutes of recorded speech. The empirical findings demonstrate that ResNet40 has an overall validation accuracy of 86.7%, surpassing other research models. It is anticipated that it will be recognized as an alternative diagnostic instrument for physicians in the future as research continues to progress.","keywords: {Training;Measurement;Deep learning;Accuracy;Cepstral analysis;Annotations;Instruments;Medical services;Real-time systems;Informatics;dysarthria;classification;feature extraction;deep learning;residual network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10805075&isnumber=10804862,"M. H. Antor, N. A. Khlebnikov and B. A. Bredikhin, ""Developing a Hyperkinetic Dysarthria Speech Classification System using Residual Learning,"" 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE), Novosibirsk, Russian Federation, 2024, pp. 1020-1023, doi: 10.1109/PIERE62470.2024.10805075."
"Multilingual Speaker-Invariant Dysarthria Severity Assessment Using Adversarial Domain Adaptation and Self-Supervised Learning,","Traditional assessments for dysarthria are subjective and time-consuming, highlighting the need for automated, objective approaches that can be scaled for remote and resource-constrained environments. This paper introduces an adversarial domain adaptation framework tailored for dysarthria severity assessment, addressing the challenges of high intra-class variability and limited availability of dysarthric speech data. By framing speaker variability as a domain adaptation problem, we utilise an adversarially trained feature extractor to derive speaker-invariant yet discriminatively powerful representations utilising speech features learned through self-supervised learning. Experiments on previously unseen and diverse speakers reveal that the proposed approach yields a 7.86% average improvement across multilingual datasets compared to traditional severity-discriminative training, outperforming competitive baselines by 12.33% on average. Additionally, the method inherently supports privacy-preserving applications by minimising reliance on speaker-specific information. The results demonstrate strong alignment with clinical assessments, reinforcing our model’s clinical relevance and effectiveness.","keywords: {Training;Deep learning;Adaptation models;Self-supervised learning;Signal processing;Feature extraction;Robustness;Multilingual;Speech processing;Overfitting;Dysarthria severity level classification;Adversarial domain adaptation;Self-supervised learning;Wav2vec},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10889800&isnumber=10887541,"L. Stumpf, B. Kadirvelu and A. A. Faisal, ""Multilingual Speaker-Invariant Dysarthria Severity Assessment Using Adversarial Domain Adaptation and Self-Supervised Learning,"" ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025, pp. 1-5, doi: 10.1109/ICASSP49660.2025.10889800."
"AI-Enabled Speech Monitoring for Dysarthria Detection in Consumer Electronics,","Speech is essential in human communication, and recent advancements in artificial intelligence (AI) have enabled new applications for voice data, particularly in health monitoring. Building on this progress, a home-based health detection system that utilizes voice signals provides an accessible method for users to independently identify and respond to conditions such as dysarthria, without the need for hospital or clinic visits. In this study, we developed and evaluated convolutional neural network (CNN) models to classify audio data from both healthy individuals and individuals with dysarthria. The dataset comprised command-based speech samples, which were segmented into sentence-specific input. Preprocessing steps included segmentation, downsampling, and feature extraction tailored for CNN input. The models exhibited high potential in distinguishing between normal and dysarthric states, with the most effective model achieving an f1-score of 96.18±4.7% and an accuracy of 96.01±5.02%. Furthermore, sentence structure and composition significantly influenced model performance, with specific sentences demonstrating higher classification accuracy. These findings suggest that AI-driven speech analysis may support detecting and managing specific health conditions in non-clinical environments. However, further work is needed to address data imbalance and sentence variability limitations. Future research could expand on real-world applications, particularly in settings with limited traditional health monitoring.","keywords: {Accuracy;Speech analysis;Hospitals;Data models;Convolutional neural networks;Reliability;Object recognition;Artificial intelligence;Monitoring;Consumer electronics;Audio classification;convolutional neural network;deep learning;dysarthria detection;healthcare},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10929844&isnumber=10929768,"S. Lee et al., ""AI-Enabled Speech Monitoring for Dysarthria Detection in Consumer Electronics,"" 2025 IEEE International Conference on Consumer Electronics (ICCE), Las Vegas, NV, USA, 2025, pp. 1-4, doi: 10.1109/ICCE63647.2025.10929844."
"Dysarthria Severity Classification Using Phase Based Features of LP Residual,","Classifying the severity of speech impairment due to dysarthria is crucial for optimizing care and enhancing communication abilities for affected individuals. This study explores the use of the Modified Group Delay Function (MGDF) of LP residual signal in classifying dysarthria severity-levels. Evaluations were conducted using standard UA-Speech and TORGO datasets. A stratified Convolutional Neural Network (CNN) with 5-fold cross-validation validated the results. Baseline features included Linear Frequency Cepstral Coefficients (LFCC), Mel Frequency Cepstral Coefficients (MFCC), and Whisper module. Key performance evaluation metrics were accuracy, precision, recall, and F1-score. Finally, the latency period was analyzed for practical deployment of the system, system’s ability to accurately recognize and process speech from any speaker, without needing to be specifically trained or adapted to individual voice characteristics.","keywords: {Performance evaluation;Asia;Speech recognition;Information processing;Speech enhancement;Delays;Convolutional neural networks;Character recognition;Mel frequency cepstral coefficient;Standards;Dysarthria;LP residual;Modified Group Delay Function},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10848959&isnumber=10848533,"R. S. Mannepalli, A. Pusuluri and H. A.Patil, ""Dysarthria Severity Classification Using Phase Based Features of LP Residual,"" 2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Macau, Macao, 2024, pp. 1-5, doi: 10.1109/APSIPAASC63619.2025.10848959."
"Automatic Early Detection of Dysarthria using Deep Neural Network,","Dysarthria is dis-coordination among articulatory muscles responsible for articulation in the production of speech. A person with dysarthric disorder cannot control their tongue, hence, are prone to slurred speech or swallowing words which lead to improper pronunciation and less intelligibility of speech. This work focuses on the automatic early detection of Dysarthria. To elucidate the main idea, it must be understood that the whole detection procedure revolves around the speech intelligibility of a person. It becomes an arduous job to distinguish between a healthy person's speech and a person who is slowly evolving with the disorder. The prime motive of the work is to provide a proper classification of the disorder in its earliest stages. The common words and phrases recorded in the Universal Access database are used for this work. Mel Frequency Cepstral Coefficients (MFCC) are extracted from both controlled and high intelligible speech data, and fed to a Deep Neural Network (DNN) classifier. MFCC fed to the DNN classifier has provided the best classification accuracy for the above experimental setup.","keywords: {Training;Recurrent neural networks;Tongue;Databases;Artificial neural networks;Production;Muscles;Dysarthria;speech intelligibility;Mel Frequency Cepstral Co-efficient;classifier;Deep Neural network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10200180&isnumber=10199221,"A. K, L. T N, S. U. Bhat, S. R and C. H M, ""Automatic Early Detection of Dysarthria using Deep Neural Network,"" 2023 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES), Tumakuru, India, 2023, pp. 1-4, doi: 10.1109/ICSSES58299.2023.10200180."
"Learning to Detect Dysarthria from Raw Speech,","Speech classifiers of paralinguistic traits traditionally learn from diverse hand-crafted low-level features, by selecting the relevant information for the task at hand. We explore an alternative to this selection, by learning jointly the classifier, and the feature extraction. Recent work on speech recognition has shown improved performance over speech features by learning from the waveform. We extend this approach to paralinguistic classification and propose a neural network that can learn a filterbank, a normalization factor and a compression power from the raw speech, jointly with the rest of the architecture. We apply this model to dysarthria detection from sentence-level audio recordings. Starting from a strong attention-based baseline on which mel-filterbanks outperform standard low-level descriptors, we show that learning the filters or the normalization and compression improves over fixed features by 10% absolute accuracy. We also observe a gain over OpenSmile features by learning jointly the feature extraction, the normalization, and the compression factor with the architecture. This constitutes a first attempt at learning jointly all these operations from raw audio for a speech classification task.","keywords: {Task analysis;Neural networks;Time-domain analysis;Speech recognition;Databases;Standards;Computational modeling;dysarthria;paralinguistic;classification;waveform;lstm},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8682324&isnumber=8682151,"J. Millet and N. Zeghidour, ""Learning to Detect Dysarthria from Raw Speech,"" ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 5831-5835, doi: 10.1109/ICASSP.2019.8682324."
"Modeling fundamental frequency dynamics in hypokinetic dysarthria,","Hypokinetic dysarthria (Hd), which often accompanies Parkinson's Disease (PD), is characterized by hypernasality and by compromised phonation, prosody, and articulation. This paper proposes automated methods for detection of Hd. Whereas most such studies focus on measures of phonation, this paper focuses on prosody, specifically on fundamental frequency (F0) dynamics. Prosody in Hd is clinically described as involving monopitch, which has been confirmed in numerous studies reporting reduced within-utterance pitch variability. We show that a new measure of F0 dynamics, based on a superpositional pitch model that decomposes the F0 contour into a declining phrase curve and (generally, single-peaked) accent curves, performs more accurate Hd vs. Control classification than simpler versions of the model or than conventional variability statistics.","keywords: {Speech;Feature extraction;Mathematical model;Accuracy;Foot;Equations;Support vector machines;Hypokinetic dysarthria;Parkinson's Disease;Pitch decomposition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7078586&isnumber=7078533,"M. S. E. Langarani and J. van Santen, ""Modeling fundamental frequency dynamics in hypokinetic dysarthria,"" 2014 IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoe, NV, USA, 2014, pp. 272-276, doi: 10.1109/SLT.2014.7078586."
"Automatic detection of speech disorder in dysarthria using extended speech feature extraction and neural networks classification,","This paper presents an automatic detection of Dysarthria, a motor speech disorder, using extended speech features called Centroid Formants. Centroid Formants are the weighted averages of the formants extracted from a speech signal. This involves extraction of the first four formants of a speech signal and averaging their weighted values. The weights are determined by the peak energies of the bands of frequency resonance, formants. The resulting weighted averages are called the Centroid Formants. In our proposed methodology, these centroid formants are used to automatically detect Dysarthric speech using neural network classification technique. The experimental results recorded after testing this algorithm are presented. The experimental data consists of 200 speech samples from 10 Dysarthric speakers and 200 speech samples from 10 age-matched healthy speakers. The experimental results show a high performance using neural networks classification. A possible future research related to this work is the use of these extended features in speaker identification and recognition of disordered speech.","keywords: {Dysarthria;speech disorder;Centroid Formants;Neural Networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8361563&isnumber=8329307,"T. B. Ijitona, J. J. Soraghan, A. Lowit, G. Di-Caterina and H. Yue, ""Automatic detection of speech disorder in dysarthria using extended speech feature extraction and neural networks classification,"" IET 3rd International Conference on Intelligent Signal Processing (ISP 2017), London, 2017, pp. 1-6, doi: 10.1049/cp.2017.0360."
"Analysis of Time Domain Features of Dysarthria Speech,","In general, Speech can be described as the ability to express thoughts and feelings by articulating sounds in order to communicate with the person who understand the speech and accordingly interpret their action. The types of communication disorders that affects a person’s ability to produce speech sounds due to the lack of control over motor functions and articulators are speech disorders. Many people of varying age groups suffer from speech disorders and require early treatment in order to correct them. This paper analyses the time domain features such as jitter and shimmer that are extracted from the speech samples of dysarthria and healthy people. This is done by looking for dissimilarities between the different speech features preset in the normal healthy person and the disordered one by employing steps such as pre-processing followed by feature extraction.","keywords: {Jitter;Feature extraction;Time-domain analysis;Pitch;jitter;shimmer;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9076507&isnumber=9076357,"A. Singh, A. Kittur, K. Sonawane, A. Singh and S. Upadhya, ""Analysis of Time Domain Features of Dysarthria Speech,"" 2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC), Erode, India, 2020, pp. 122-125, doi: 10.1109/ICCMC48092.2020.ICCMC-00025."
"Emotional Communication Assist Interface App for People with Dysarthria,","Text to speech (TTS) helps people who have speech disorders communicate with other people. The current TTS can express emotion, but this requires extra time and effort. We developed a TTS application by which users can render messages with emotion with ease by changing the pitch angle of a smartphone. We designed two types of TTS: free input and phrase selection. Also, we calculated the TTS editing parameters for each degree of emotion on the basis of the Online Game Voice Chat Corpus (OGVC).","keywords: {Communication aids;Conferences;Natural languages;Games;Consumer electronics;TTS;dysarthria;emotional message;smartphone;pitch angle},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9291830&isnumber=9291703,"T. Kawase and M. Iwaki, ""Emotional Communication Assist Interface App for People with Dysarthria,"" 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), Kobe, Japan, 2020, pp. 568-571, doi: 10.1109/GCCE50665.2020.9291830."
"Fuzzy Expectation Maximization Phoneme Prediction in Diffusion Model-based Dysarthria Voice Conversion,","In this paper proposed an effective fuzzy expectation-maximization phoneme prediction method in diffusion model-based dysarthria voice conversion (FEMPPDM-DVC) which is accessible to (i) training without parallel data (ii) converting a longer duration of the audio data (iii) preserves speaker identity. By integrating Fuzzy Expectation-Maximization (FEM) clustering and Diffusion model-based dysarthria voice conversion approach, the proposed method is able gradually generate normal utterances. Feature extraction is performed using Mel-frequency cepstral coefficients (MFCCs), a robust method in acoustic feature extraction in Text-to-Speech systems. Ensures the effectiveness and accuracy, through repeated parameter adjustment using the FEM clustering algorithm. The conversion network is a diffusion model-based structure, which can convert normal utterances to dysarthria utterances in the forward diffusion process. After forward diffusion process, the dysarthria utterance will go through a reverse diffusion process to convert dysarthria voice to normal utterance. Once converted, a GAN-based vocoder is applied to convert mel-frequency spectrogram to waveform. Objective evaluation is conducted on the Saarbrücken Voice Database (SVD) dataset show that FEMDDPM-DVC is able to improve the intelligibility and naturalness of dysarthria utterances in 15 kinds of dysarthria voice. The proposed method is compared with five other dysarthria voice conversion methods, show that the proposed method has the most performance among other method.","keywords: {Training;Adaptation models;Vocoders;Diffusion processes;Predictive models;Feature extraction;Transformers;voice conversion;diffusion probabilistic model;dysarthria voice;fuzzy expectation maximization;phoneme prediction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10661368&isnumber=10661367,"G. Lin, W. Hsu, G. Liu and S. Chen, ""Fuzzy Expectation Maximization Phoneme Prediction in Diffusion Model-based Dysarthria Voice Conversion,"" 2024 International Conference on Fuzzy Theory and Its Applications (iFUZZY), Kagawa, Japan, 2024, pp. 1-4, doi: 10.1109/iFUZZY63051.2024.10661368."
"Revolutionary Dysarthria Analysis Through Convolutional Neural Networks,","Dysarthria occurs when there are impairments in the muscles necessary for speech, including those in the lips, tongue, voice box, and breathing muscles. This condition can result in unclear speech, even in individuals who are skilled communicators. Dysarthria can range from mild speech difficulties, where speech is hard to understand, to severe cases, where speech is completely unclear. A method for distinguishing between different types of speech involves using recordings and advanced computational techniques such as convolutional neural networks (CNNs). The strengths of CNNs are leveraged to construct a highly effective model for detecting dysarthria in speech samples. The dataset utilized includes recordings from individuals with and without dysarthria. The results indicate that the CNN model performs exceptionally well, achieving precision, recall, and F1-scores of 0.99 for both categories. With an accuracy of 0.99, the model demonstrates a high level of proficiency in differentiating between speech affected by dysarthria and normal speech. These findings highlight the effectiveness and utility of this CNN-based approach in diagnosing and assessing dysarthria. It offers valuable potential for assisting healthcare professionals and enhancing personalized speech therapy interventions.","keywords: {Training;Accuracy;Tongue;Computational modeling;Medical treatment;Speech recognition;Speech enhancement;Muscles;Recording;Convolutional neural networks;Dysarthria;Speech Disorder;Convolutional Neural Networks (CNNs);Audio Classification;Speech Recognition;Machine Learning;Neural Networks;Speech Therapy;Automated Diagnosis;Clinical Applications},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10912315&isnumber=10912086,"A. Verma, K. S. Gill, N. Thapliyal and R. S. Rawat, ""Revolutionary Dysarthria Analysis Through Convolutional Neural Networks,"" 2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT), Faridabad, India, 2024, pp. 832-837, doi: 10.1109/ICAICCIT64383.2024.10912315."
"A Study of Speech Recognition Techniques for Dysarthria Speeches Based on Digit Recognition,","With the introduction of speech recognition technology into people's lives, the application of which is no longer limited to ordinary people, but begins to target special populations, such as patients with dysarthria. It has become a major research challenge to build a high-performance speech recognition system for patients with dysarthria. To address this problem, in this paper the speech data of four patients with dysarthria and four healthy speakers on digital commands are firstly collected, secondly several speech recognition experiments are separately conducted by using Aishell-2 and Wenetspeech, two pre-trained models based on the WeNet open-source architecture, as well as the four major commercial recognition API systems, at the end a method to optimize the dysarthria data by using the so-vits-svc tool is proposed. The experiments show that the current state-of-the-art speech recognition model has a high recognition rate for speech data from the general population, but there is still room for improvement in the recognition performance of speech data from individuals with dysarthria. Later the optimization method for the dysarthria data proposed in this paper effectively improves the quality of speech data. For the two above open-source pre-trained models, the recognition rates have increased by 60% and 67.64 % compared to the original data, respectively.","keywords: {Human computer interaction;Analytical models;Sequences;Error analysis;Optimization methods;Speech recognition;Data models;dysarthria;automatic speech recognition;speech conversion;open source models;commercial models;pre-trained models;digital data recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10675073&isnumber=10674745,"J. Yang, H. Guo, X. Xu and H. Bu, ""A Study of Speech Recognition Techniques for Dysarthria Speeches Based on Digit Recognition,"" 2024 5th International Conference on Electronic Communication and Artificial Intelligence (ICECAI), Shenzhen, China, 2024, pp. 382-386, doi: 10.1109/ICECAI62591.2024.10675073."
"Fractal features for automatic detection of dysarthria,","Amytrophic lateral sclerosis (ALS) is an incurable neurodegenerative disease. Difficulty articulating speech, dysarthria, is a common early symptom of ALS. Detecting dysarthria currently requires manual analysis of several different speech tasks by pathology experts. This is time consuming and can lead to misdiagnosis. Many existing automatic classification approaches require manually preprocessing recordings, separating individual spoken utterances from a repetitive task. In this paper, we propose a fully automated approach which does not rely on manual preprocessing. The proposed method uses novel features based on fractal analysis. Acoustic and associated articulatory recordings of a standard speech diagnostic task, the diadochokinetic test (DDK), are used for classification. This study's experiments show that this approach attains 90.2% accuracy with 94.2% sensitivity and 85.1% specificity.","keywords: {Fractals;Speech;Jitter;Feature extraction;Time series analysis;Acoustics;Lips},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7897299&isnumber=7897179,"T. Spangler, N. V. Vinodchandran, A. Samal and J. R. Green, ""Fractal features for automatic detection of dysarthria,"" 2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), Orlando, FL, USA, 2017, pp. 437-440, doi: 10.1109/BHI.2017.7897299."
"Assessing Dysarthria severity using global statistics and boosting,","A new method for automatic assessment of Dysarthria severity is described. It uses the forward selection method (FSM) on global statistics of low-complexity features to find effective feature sets. FSM is embedded in a boosting algorithm that combines multiple weak classifiers to achieve a single strong classifier. Unlike standard boosting, this uses nonlinear class boundaries and unique feature sets per iteration. Results on a 39 speaker dysarthria database are described.","keywords: {Boosting;Speech;Classification algorithms;Training;Training data;Speech processing;Frequency measurement},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6190184&isnumber=6189941,"A. DeMino, R. Kubichek and K. Caves, ""Assessing Dysarthria severity using global statistics and boosting,"" 2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR), Pacific Grove, CA, USA, 2011, pp. 1103-1106, doi: 10.1109/ACSSC.2011.6190184."
"An Intelligent System for Dysarthria Classification of Male and Female Processed Dataset using Sequential Model Parameters,","There are several current studies including various methods for treating dysarthria in various developing countries. Research-based dysarthria treatments take a multidisciplinary approach that encompasses speech therapy, assistive technology, neurosurgery, pharmaceutical therapies, non-invasive brain stimulation, machine learning and AI methods. These methods seek to raise the general quality of life, communication and speaking abilities of dysarthria sufferers. With a 90% accuracy rate, the proposed sequential model was found to have good classification performance for identifying Dysarthria. Researchers have also been looking at the use of Machine Learning (ML) and Artificial Intelligence (AI) approaches to create tools and systems that can help people with dysarthria to speak and communicate for social welfare and provide access to best remedies.","keywords: {Multiple sclerosis;Parkinson's disease;Machine learning;Production;Assistive technologies;Speech enhancement;Brain modeling;CNN;Model Training;Dysarthria Classification;Sequential Model;Deep Learning;Confusion matrix},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10250600&isnumber=10250450,"K. S. Gill, V. Anand and R. Gupta, ""An Intelligent System for Dysarthria Classification of Male and Female Processed Dataset using Sequential Model Parameters,"" 2023 Second International Conference on Augmented Intelligence and Sustainable Systems (ICAISS), Trichy, India, 2023, pp. 815-820, doi: 10.1109/ICAISS58487.2023.10250600."
"Dysarthria diagnosis via respiration and phonation,",This report discusses the implementation of a computerized application - the Computerised Frenchay Dysarthria Assessment Procedure (CFDA) - which uses digital signal processing (DSP) techniques to objectively evaluate digitised speech recordings in order to detect any symptoms of dysarthria (a type of motor speech disorder). This investigation focuses specifically on two CFDA diagnostic sub-applications which assess a patient's ability to maintain a prolonged exhalation (the “Respiration at Rest” task) as well as execute a steady state phonation (i.e. the “Sustained Phonation” task). It is demonstrated that combining the functionality of these two sub-applications substantially increases their diagnostic effectiveness in the context of identifying a particular variety of dysarthria known as spastic dysarthria. It is further demonstrated that certain patterns of energy fluctuations are closely correlated with manifestations of spastic dysarthria. The CFDA is intended for use in real-world conditions to assist speech and language therapists when conducting clinical evaluations.,"keywords: {Speech;Steady-state;Digital signal processing;Fluctuations;Speech processing;Context;Production;DSP;diagnosis;dysarthria;respiration;phonation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7344537&isnumber=7344420,"J. Carmichael, ""Dysarthria diagnosis via respiration and phonation,"" 2015 International Conference and Workshop on Computing and Communication (IEMCON), Vancouver, BC, Canada, 2015, pp. 1-5, doi: 10.1109/IEMCON.2015.7344537."
"Deep Learning Based Dysarthria Detection: A Comprehensive Approach,","Making a model that can analyze numerous input features and predict whether a person will develop dysarthria is the first step in utilizing machine learning to forecast diseases like dysarthria. A motor speech disorder called dysarthria can be caused by a variety of underlying conditions, such as degenerative disorders, traumatic brain injuries, or neurological disorders. The authors used the 2000 audio signals from males and females with and without dysarthria from the TORGO data set. To extract the important features from the audio signals, the authors used the MFCC approach. To determine if dysarthria is present or absent, a machine learning model or algorithm will be fed with these MFCC coefficients.","keywords: {Training;Machine learning algorithms;Predictive models;Feature extraction;Prediction algorithms;Motors;Robustness;Mel frequency cepstral coefficient;Testing;Overfitting;Dysarthria;motor speech impairment;degenerative diseases;brain traumas;neurological disorders;TORGO;MFCC},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10892620&isnumber=10892612,"S. Patil, S. Borude, C. Budhwani, A. Bhandare and P. Bhujbal, ""Deep Learning Based Dysarthria Detection: A Comprehensive Approach,"" 2024 Fourth International Conference on Multimedia Processing, Communication & Information Technology (MPCIT), Shivamogga, India, 2024, pp. 262-266, doi: 10.1109/MPCIT62449.2024.10892620."
"Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition,","In this paper, we propose a deep learning-based algorithm to improve the performance of automatic speech recognition (ASR) systems for aphasia, apraxia, and dysarthria speech by utilizing electroencephalography (EEG) features recorded synchronously with aphasia, apraxia, and dysarthria speech. We demonstrate a significant decoding performance improvement by more than 50% during test time for isolated speech recognition task and we also provide preliminary results indicating performance improvement for the more challenging continuous speech recognition task by utilizing EEG features. The results presented in this paper show the first step towards demonstrating the possibility of utilizing non-invasive neural signals to design a real-time robust speech prosthetic for stroke survivors recovering from aphasia, apraxia, and dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will be released to the public to help further advance this interesting and crucial research.","keywords: {Stroke (medical condition);Electroencephalography;Real-time systems;Biology;Decoding;Task analysis;Prosthetics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9629802&isnumber=9629471,"G. Krishna et al., ""Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition,"" 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Mexico, 2021, pp. 6008-6014, doi: 10.1109/EMBC46164.2021.9629802."
"Hmm-Based and Svm-Based Recognition of the Speech of Talkers With Spastic Dysarthria,","This paper studies the speech of three talkers with spastic dysarthria caused by cerebral palsy. All three subjects share the symptom of low intelligibility, but causes differ. First, all subjects tend to reduce or delete word-initial consonants; one subject deletes all consonants. Second, one subject exhibits a painstaking stutter. Two algorithms were used to develop automatic isolated digit recognition systems for these subjects. HMM-based recognition was successful for two subjects, but failed for the subject who deletes all consonants. Conversely, digit recognition experiments assuming a fixed word length (using SVMs) were successful for two subjects, but failed for the subject with the stutter.","keywords: {Hidden Markov models;Speech recognition;Automatic speech recognition;Birth disorders;Natural languages;Microphone arrays;Speech analysis;Muscles;Brain injuries;Automatic control},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1660840&isnumber=34759,"M. Hasegawa-Johnson, J. Gunderson, A. Perlman and T. Huang, ""Hmm-Based and Svm-Based Recognition of the Speech of Talkers With Spastic Dysarthria,"" 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, Toulouse, France, 2006, pp. III-III, doi: 10.1109/ICASSP.2006.1660840."
"Design of a dysarthria classifier using global statistics of speech features,","Dysarthria is a neurological disorder in which the speech production system is impaired. There are five main types of dysarthrias depending on the location of the lesion in the nervous system. There is evidence suggesting a relationship between the location of the lesion and the resulting speech characteristics. This paper describes a non-intrusive classifier to identify the dysarthria type in a person using global statistics, e.g., mean, variance, etc., of speech features. A tree-based classifier was developed using multiple low-level maximum likelihood classifiers as inputs. An error of 10.5% was achieved in the classification of three types of dysarthrias.","keywords: {Statistics;Speech analysis;Classification tree analysis;Lesions;Decision trees;Neural networks;Cepstral analysis;Hidden Markov models;Speech coding;Speech processing;Speech disorders;dysarthria diagnosis;objective speech quality analysis;global speech statistics;decision trees},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5495563&isnumber=5494886,"M. V. Mujumdar and R. F. Kubichek, ""Design of a dysarthria classifier using global statistics of speech features,"" 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, Dallas, TX, USA, 2010, pp. 582-585, doi: 10.1109/ICASSP.2010.5495563."
"Dysarthria Speech Disorder Classification Using Traditional and Deep Learning Models,","Dysarthria is a motor speech disorder that results in speech difficulties due to the weakness of associated muscles. This unclear speech makes it difficult for dysarthric patients to present himself understood. This neurological limitation is usually occurs due to damages to the brain or central nervous system. Speech therapy can be effectively employed to enhance the range and consistency of voice production and improve intelligibility and communicative effectiveness. Assessing the degree of severity of dysarthria provides vital information on the patient's progress which inturn assists pathologists in arriving at a treatment plan that includes developing automated voice recognition system suitable for dysarthria patients. This work performs an exhaustive study on dysarthria severity level classification using deep neural network (DNN) and convolution neural network (CNN) architectures. Mel Frequency Cepstral Coefficients (MFCCs) and their derivatives constitute feature vectors for classification. Using the UA-Speech database, the performance metrics of DNN/CNN based learning models have been compared to baseline classifiers like support vector machine (SVM) and Random Forest (RF). The highest classification accuracy of 97.6\% is reported for DNN under UA speech database. A detailed examination of the performance from the models discussed above reveal that appropriate choice of deep learning architecture ensures better results than traditional classifiers like SVM and Random Forest.","keywords: {Support vector machines;Radio frequency;Deep learning;Databases;Computational modeling;Speech enhancement;Indexes;CNN;deep learning;DNN;Dysarthria;motor speech disorder;Random Forest;SVM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10157285&isnumber=10156874,"M. Suresh, R. Rajan and J. Thomas, ""Dysarthria Speech Disorder Classification Using Traditional and Deep Learning Models,"" 2023 Second International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT), Trichirappalli, India, 2023, pp. 01-06, doi: 10.1109/ICEEICT56924.2023.10157285."
"Harnessing Deep Learning Techniques for Dysarthria Detection,","Dysarthria, a motor speech disorder resulting from neurological impairments. This study explores various approaches for the automated detection of dysarthria, integrating both traditional and emerging technologies. Speech assessments by acoustic analysis, machine learning models, and deep learning techniques are considered. Machine learning models are trained on datasets containing normal and dysarthric speech for males and females, while deep learning methods, including convolutional and recurrent neural networks, are employed to automatically learn features from speech data. Our goal is to develop a reliable and accessible dysarthria detection system thatcomplements the expertise of healthcare professionals. Validation using diverse datasets is emphasized, acknowledging the importance of early detection and intervention in improving outcomes for individuals with dysarthria.","keywords: {Deep learning;Analytical models;Recurrent neural networks;Collaboration;Medical services;Motors;Feature extraction;Acoustic analysis;Convolutional neural net-works;Deep learning techniques;Dysarthria;Machine learning models;Recurrent neural networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10527645&isnumber=10527410,"P. Varalakshmi, V. S. Sanmitha, S. B. Dhivyadharshni and S. Saravanan, ""Harnessing Deep Learning Techniques for Dysarthria Detection,"" 2024 Third International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS), Krishnankoil, Virudhunagar district, Tamil Nadu, India, 2024, pp. 01-06, doi: 10.1109/INCOS59338.2024.10527645."
"Next-Gen Speech Disorder Diagnostics: CNN Methods for Dysarthria Classification,","Neurological disorders such as amyotrophic lateral sclerosis (ALS) and cerebral palsy (CP) are common causes of dysarthria, the subject of this research article. Dysarthria has a significant impact on communication capacity, making early diagnosis essential for effective therapies and improved patient outcomes. The research used a Convolutional Neural Network (CNN) to categorise speech using the TORGO database, which includes 2000 samples of individuals with and without dysarthria, representing different genders. There are 500 samples from each gender in the collection, representing dysarthric and non-dysarthric people who were recorded during different sessions. Highlighting the promise of deep learning technology in aiding early identification of dysarthria and enabling rapid therapy for affected individuals, the CNN-based approach exhibits an amazing 96% accuracy in discriminating between dysarthric and non-dysarthric cases. This study offers a promising chance to enhance the quality of life for those with speech impairments while also making substantial contributions to the development of diagnostic capabilities.","keywords: {Deep learning;Neurological diseases;Cerebral palsy;Accuracy;Databases;Medical treatment;Speech enhancement;Convolutional neural networks;Medical diagnosis;Stress;Artificial Intelligence;Deep Learning;Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10696797&isnumber=10695957,"G. Verma, K. S. Gill, M. Kumar and R. Rawat, ""Next-Gen Speech Disorder Diagnostics: CNN Methods for Dysarthria Classification,"" 2024 First International Conference on Pioneering Developments in Computer Science & Digital Technologies (IC2SDT), Delhi, India, 2024, pp. 365-369, doi: 10.1109/IC2SDT62152.2024.10696797."
"Advancing Speech Disorder Diagnostics: CNN Innovations in Dysarthria Recognition,","Neurological disorders such as amyotrophic lateral sclerosis (ALS) and cerebral palsy (CP) are common causes of dysarthria, the subject of this research article. The capacity to communicate is greatly affected by dysarthria, thus early diagnosis is critical for effective therapies and improved patient outcomes. The study employed a Convolutional Neural Network (CNN) for speech classification using the TORGO database, which includes 2000 samples of individuals with and without dysarthria, representing different genders. With 500 samples each, collected over distinct sessions, the dataset contains both dysarthric and non-dysarthric people of both sexes. Distinguishing between dysarthric and non-dysarthric occurrences, the CNN-based approach achieves an astonishing 96% accuracy. This shows how deep learning technology may help with early dysarthria identification and enable rapid therapy for affected individuals. The findings of this study have important implications for the development of diagnostic tools and may lead to better living conditions for those who struggle with speaking.","keywords: {Deep learning;Neurological diseases;Cerebral palsy;Technological innovation;Accuracy;Databases;Medical treatment;Speech recognition;Convolutional neural networks;Stress;Artificial Intelligence;Deep Learning;Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10730941&isnumber=10730804,"A. Kaur, K. S. Gill, M. Kumar and R. Rawat, ""Advancing Speech Disorder Diagnostics: CNN Innovations in Dysarthria Recognition,"" 2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC), Gwalior, India, 2024, pp. 766-770, doi: 10.1109/AIC61668.2024.10730941."
"A preliminary study on self-care telemonitoring of dysarthria in spinal muscular atrophy,","Spinal muscular atrophy (SMA) is a rare neuromuscular disease which may cause impairments in oro-facial musculature. Most of the individuals with SMA present bulbar signs such as flaccid dysarthria which mines their abilities to speak and, as consequence, their psychic balance. To support clinicians, recent work has demonstrated the feasibility of video-based techniques for assessing the oro-facial functions in patients with neurological disorders such as amyotrophic lateral sclerosis. However, no work has so far focused on automatic and quantitative monitoring of dysarthria in SMA. To overcome limitations this work’s aim is to propose a cloud-based store-and-forward telemonitoring system for automatic and quantitative evaluation of oro-facial muscles in individuals with SMA. The system integrates a convolutional neural network (CNN) aimed at identifying the position of facial landmarks from video recordings acquired via a web application by an SMA patient.Clinical relevance— The proposed work is in the preliminary stage, but it represents the first step towards a better understanding of the bulbar-functions’ evolution in patients with SMA.","keywords: {Atrophy;Neurological diseases;Evolution (biology);Neuromuscular;Medical services;Stroke (medical condition);Convolutional neural networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10340908&isnumber=10339939,"L. Migliorelli et al., ""A preliminary study on self-care telemonitoring of dysarthria in spinal muscular atrophy,"" 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Sydney, Australia, 2023, pp. 1-4, doi: 10.1109/EMBC40787.2023.10340908."
"Automatic And Perceptual Discrimination Between Dysarthria, Apraxia of Speech, and Neurotypical Speech,","Automatic techniques in the context of motor speech disorders (MSDs) are typically two-class techniques aiming to discriminate between dysarthria and neurotypical speech or between dysarthria and apraxia of speech (AoS). Further, although such techniques are proposed to support the perceptual assessment of clinicians, the automatic and perceptual classification accuracy has never been compared. In this paper, we investigate a three-class automatic technique and a set of handcrafted features for the discrimination of dysarthria, AoS and neurotypical speech. Instead of following the commonly used One-versus-One or One-versus-Rest approaches for multi-class classification, a hierarchical approach is proposed. Further, a perceptual study is conducted where speech and language pathologists are asked to listen to recordings of dysarthria, AoS, and neurotypical speech and decide which class the recordings belong to. The proposed automatic technique is evaluated on the same recordings and the automatic and perceptual classification performance are compared. The presented results show that the hierarchical classification approach yields a higher classification accuracy than baseline One-versus-One and One-versus-Rest approaches. Further, the presented results show that the automatic approach yields a higher classification accuracy than the perceptual assessment of speech and language pathologists, demonstrating the potential advantages of integrating automatic tools in clinical practice.","keywords: {Databases;Conferences;Support vector machine classification;Tools;Signal processing;Feature extraction;Acoustics;dysarthria;apraxia of speech;support vector machine;hierarchical classification;perceptual classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414283&isnumber=9413350,"I. Kodrasi, M. Pernon, M. Laganaro and H. Bourlard, ""Automatic And Perceptual Discrimination Between Dysarthria, Apraxia of Speech, and Neurotypical Speech,"" ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Toronto, ON, Canada, 2021, pp. 7308-7312, doi: 10.1109/ICASSP39728.2021.9414283."
"Adversarial Auto-Encoders Based Model for Classification of Speech Dysarthria,","Communication is effective based on various parameters, out of which phonetic or oral communication plays a vital role. Slurred speech or improper speech will lead to misunderstanding in speech, which could toss up any situation. There are many people, ranging from children to adults, who are affected with slurred speech, which is technically termed as Speech Dysarthria, a disease which tampers effective oral communication. Distinguishing between people affected with dysarthria and people with normal speech will be tedious process manually. Machine Learning (ML), and Artificial Intelligence (AI), can be pitched in to solve the problem. There are existing methodologies which classify people affected with speech dysarthria and people who communicate in a normal way. Some of the existing technologies used are Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and so on. This paper aims at distinguishing between people affected with speech dysarthria and people with normal speech, using Adversarial Auto Encoders (AAE), a model which has its roots from Variational Auto Encoders (VAE) and Generative Adversarial Networks (GAN). This paper brings out a good result and proves to be effective.","keywords: {Recurrent neural networks;Computational modeling;Machine learning;Phonetics;Speech;Generative adversarial networks;Distance measurement;Convolutional neural networks;Speech processing;Diseases;Speech Dysarthria;Artificial Intelligence;Machine Learning;Adversarial Auto Encoders;Variational Auto Encoders;Generative Adversarial Networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10724410&isnumber=10723316,"V. Kanchana Devi, R. Sreenivas, E. Umamaheshwari and N. Bacanin, ""Adversarial Auto-Encoders Based Model for Classification of Speech Dysarthria,"" 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT), Kamand, India, 2024, pp. 1-7, doi: 10.1109/ICCCNT61001.2024.10724410."
"Summary of Low-Resource Dysarthria Wake-Up Word Spotting Challenge,","In recent years, the rapid advancement and widespread adoption of speech technology have made smart home systems a common feature in many households. However, individuals with dysarthria face difficulties using these technologies due to inconsistent speech patterns. This paper summarizes the Low-Resource Dysarthria Wake-Up Word Spotting (LRDWWS) Challenge at SLT 2024, which aimed to develop effective voice wake-up systems for individuals with dysarthria. The challenge attracted 25 teams from 4 countries, with 7 teams submitting results and 5 providing detailed system descriptions. This paper presents an overview of the dataset, evaluation metrics, and key innovations from participating teams. Our findings highlight the potential of these systems to enhance the accessibility and usability of smart home technologies for individuals with dysarthria. The challenge results underscore the importance of developing specialized solutions to meet the unique needs of this user group.","keywords: {Measurement;Training;Technological innovation;Pathology;Smart homes;Speech enhancement;Feature extraction;Data models;Usability;Faces;Dysarthria;Wake-up Word Spotting;Speaker-dependent Systems;Speech Disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832239&isnumber=10830793,"M. Gao et al., ""Summary of Low-Resource Dysarthria Wake-Up Word Spotting Challenge,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 592-599, doi: 10.1109/SLT61566.2024.10832239."
"Transformer-based Transfer Learning for Enhanced Speech Dysarthria Severity Assessment,","Dysarthria, a neuromuscular communications disorder presented with impaired pronunciation, is a daunting task to identify and quantify the level of dysfunction. Through this paper, a critical study of automated dysarthria severity classification using transformer-based deep learning methods will be discussed. Transfer learning is facilitated by employing three variations of Vision Transformer (ViT) models: ViT-L-16, ViT-L-32, and ViT-B-16, on UA-Speech Corpus and TORGO datasets, achieving remarkable results with 99.01% accuracy for the UA-Speech dataset and 99.39% accuracy for the TORGO database. The study evaluates the models’ performance focusing on accuracy, precision, recall, and F1-scores. Experimental results emphasise the potential for automated diagnostic units in neurology clinical practice and establish a baseline for leading work in dysarthria severity classification with the selection of an efficient ViT model.","keywords: {Deep learning;Neurology;Accuracy;Neuromuscular;Computational modeling;Transfer learning;Speech recognition;Speech enhancement;Transformers;Real-time systems;Transfer Learning;Speech Processing;Dysarthria Severity Detection;Transformers},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10724295&isnumber=10723316,"A. Venkata Siva Manoj, V. Lakshman, A. Kamuju, V. Pulagam and G. Jyothish Lal, ""Transformer-based Transfer Learning for Enhanced Speech Dysarthria Severity Assessment,"" 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT), Kamand, India, 2024, pp. 1-6, doi: 10.1109/ICCCNT61001.2024.10724295."
"Enhancing speech rate estimation techniques to improve dysarthria diagnosis,","This report discusses the implementation of a computerized algorithm specifically designed to measure the syllables-per-minute rate of abnormal speech typically produced by persons suffering from an articulatory disorder known as dysarthria. This speech rate measurement application - which can also serve as a diagnostic tool in itself - has been integrated into the computerised Frenchay Dysarthria Assessment (CFDA) suite of diagnostic tests. It is demonstrated that, when processing dysarthric speech, syllables-per-minute measurements are more accurate when based on vowel transition detection techniques as opposed to using spectral moment computations.","keywords: {Speech;Speech processing;Signal processing algorithms;Production;Estimation;Software algorithms;Time measurement;DSP;diagnosis;dysarthria;speech rate;intelligibility},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8117233&isnumber=8117121,"J. N. Carmichael, ""Enhancing speech rate estimation techniques to improve dysarthria diagnosis,"" 2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), Vancouver, BC, Canada, 2017, pp. 309-313, doi: 10.1109/IEMCON.2017.8117233."
"Personalizing TTS Voices for Progressive Dysarthria,","Amyotrophic lateral sclerosis (ALS) patients experience progressive speech deterioration due to muscle paralysis, leading to eventual loss of verbal communication capability. Text-to-speech synthesis (TTS) is an important technology for speech generating devices, enabling users to communicate using generic electronic voices, but often without the vocal identity of the users. Our work is aimed at personalizing TTS voices for people with ALS induced dysarthria by integrating machine learning and speech processing techniques of voice conversion (VC) and TTS. This is challenging as only small quantities of dysarthric speech are available from individual patients. Our system includes both timbre and prosody conversion for VC, neural TTS to generate TTS speech, and neural feature converter to interface VC and TTS. We collected speech data from 4 ALS target speakers with mild to severe dysarthria. Subjective listening tests showed that on average, our approach improved speech intelligibility by about 72% over the target speakers’ speech, the converted voice was 2 to 3 times more similar to ALS targets than to TTS sources, and the converted speech quality was in the MOS scale of fair to good.","keywords: {Conferences;Natural languages;Machine learning;Muscles;Paralysis;Timbre;Speech processing;voice conversion;feature conversion;neural TTS;dysarthria;amyotrophic lateral sclerosis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9508522&isnumber=9508478,"Y. Zhao, M. Song, Y. Yue and M. Kuruvilla-Dugdale, ""Personalizing TTS Voices for Progressive Dysarthria,"" 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI), Athens, Greece, 2021, pp. 1-4, doi: 10.1109/BHI50953.2021.9508522."
"A Phonological Control Method on A Speech Compensation System for Dysarthria Using A Standardized Space,","We have developed a speech compensation system for dysarthria. The system aims at improving the phonological properties of vowels without losing speaker individuality. We propose a method for phonological control of vowels using a standardized space to control vowels in the normalized articulation space, normalized for speaker individuality. The method maps an original dysarthric speaker's normalized articulation space to a standardized space, then from the standardized space to the target speaker's normalized articulation space assuming normality to improve the phonological properties of vowels. We confirm phonological control of vowels by performing a processing simulation, comparison different target speakers and a processing simulation using a dummy original speaker as a dysarthria.","keywords: {Frequency synthesizers;Process control;Aerospace electronics;Speech synthesis;Informatics;Frequency control;Gravity;vowel formant;articulation space;phonological control;vowel normalization;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9336404&isnumber=9336196,"Y. Hetsugi, T. Sakata and Y. Ueda, ""A Phonological Control Method on A Speech Compensation System for Dysarthria Using A Standardized Space,"" 2020 5th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS), Okinawa, Japan, 2020, pp. 158-162, doi: 10.1109/ICIIBMS50712.2020.9336404."
"Japanese Vowel-mora Visualization for Dysarthria Rehabilitation with Variational Autoencoder,","This work proposes a Variational AutoEncoder (VAE)-based rehabilitation framework that visualizes the vowel-mora for Japanese dysarthria. Traditionally, Speech-Language Pathology (SLP) has shown the guideline of rehabilitation for dysarthria, but they should rely only on clinical experience and case-by-case adaption, highlighting the urgent necessity to push the boundary for showing a subjective guideline, which does not depend on the perspective of SLPs. The proposed framework takes advantage of two-dimensional latent representations of vowel-mora, which is assumed to be pre-processed by mel-spectrogram, via VAE. The experiments highlight the effectiveness of our proposed framework.","keywords: {Training;Pathology;Accuracy;Circuits and systems;Autoencoders;Asia;Data visualization;Speech recognition;Feature extraction;Guidelines;Dysarthria;Variational Autoencoder;Mora visualization},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10808415&isnumber=10808208,"R. Michizoe et al., ""Japanese Vowel-mora Visualization for Dysarthria Rehabilitation with Variational Autoencoder,"" 2024 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS), Taipei, Taiwan, 2024, pp. 494-498, doi: 10.1109/APCCAS62602.2024.10808415."
"Machine Learning Approaches for Automated Detection and Classification of Dysarthria Severity,","Dysarthria, a speech disorder caused by neuro-motor problems resulting in impaired articulation, requires an assessment of its severity for diagnostic and monitoring purposes. Additionally, accurate severity classification facilitates the development of automated dysarthric speech detection and classification systems. This paper presents a comprehensive investigation into detecting dysarthric voices within a collection of normal voice samples, followed by the dysarthria severity classification utilizing neural network frameworks, specifically long short-term memory network (LSTM) and recurrent neural network (RNN). The study employs various features including Mel frequency cepstral coefficients (MFCC), formants, prosodic parameters, and voice quality. The performance of these models is evaluated against a baseline support vector machine (SVM) classifier using the Nemours corpus database. Remarkably, the highest classification accuracy achieved for this corpus is 99.69%. Detailed analysis demonstrates that selecting an appropriate neural network architecture yields superior performance compared to the conventional SVM classifier.","keywords: {Support vector machines;Voice activity detection;Pathology;Recurrent neural networks;Supervised learning;Speech enhancement;Mel frequency cepstral coefficient;Dysarthria classification;SVM;LSTM;RNN;acoustic parameters;automatic speech assessment},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10419588&isnumber=10419198,"A. Hamza, D. Addou and H. Kheddar, ""Machine Learning Approaches for Automated Detection and Classification of Dysarthria Severity,"" 2023 2nd International Conference on Electronics, Energy and Measurement (IC2EM), Medea, Algeria, 2023, pp. 1-6, doi: 10.1109/IC2EM59347.2023.10419588."
"The ISCSLP 2024 Multimodal Dysarthria Severity Assessment (MDSA) Challenge: Dataset, Tracts, Baseline and Results,","To advance multimodal speech assessment and related research in developing objective diagnostic methods, we are launching the Multimodal Dysarthria Severity Assessment (MDSA) Challenge. This paper summarizes the outcomes from the ISCSLP 2024 MDSA Challenge. We first address the necessity of the challenge and then introduce the associated audio-video dataset selected from the MSDM database, including 62 subacute stroke patients and 25 normal controls. We then describe the challenge arrangement and the baseline system. Specifically, we set up a four-classification task for the severity of dysarthria (normal, mild, moderate and severe) with the aim of developing an objective and accurate automatic assessment method to assist in clinical diagnosis and treatment. Finally we summarize the challenge results and provide the major observations from the submitted systems. We hope the open data and challenge will serve as a benchmark and common test-bed for pathological speech assessment.","keywords: {Measurement;Pathology;Image analysis;Data analysis;Databases;Speech recognition;Stroke (medical condition);Clinical diagnosis;Speech processing;Open data;Multimodal;Dysarthria severity;Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800436&isnumber=10799969,"J. Liu et al., ""The ISCSLP 2024 Multimodal Dysarthria Severity Assessment (MDSA) Challenge: Dataset, Tracts, Baseline and Results,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 136-140, doi: 10.1109/ISCSLP63861.2024.10800436."
"An Analysis of Degenerating Speech Due to Progressive Dysarthria on ASR Performance,","Although personalized automatic speech recognition (ASR) models have recently been improved to recognize even severely impaired speech, model performance may degrade over time for persons with degenerating speech. The aims of this study were to (1) analyze the change of performance of ASR over time in individuals with degrading speech, and (2) explore mitigation strategies to optimize recognition throughout disease progression. Speech was recorded by four individuals with degrading speech due to amyotrophic lateral sclerosis (ALS). Word error rates (WER) across recording sessions were computed for three ASR models: Unadapted Speaker Independent (U-SI), Adapted Speaker Independent (A-SI), and Adapted Speaker Dependent (A-SD or personalized). The performance of all models degraded significantly over time as speech became more impaired, but the A-SD model improved markedly when updated with recordings from the severe stages of speech progression. Recording additional utterances early in the disease before significant speech degradation did not improve the performance of A-SD models. This emphasizes the importance of continuous recording (and model retraining) when providing personalized models for individuals with progressive speech impairments.","keywords: {Degradation;Adaptation models;Error analysis;Computational modeling;Acoustics;Recording;Speech processing},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10097195&isnumber=10094560,"K. Tomanek, K. Seaver, P. -P. Jiang, R. Cave, L. Harrell and J. R. Green, ""An Analysis of Degenerating Speech Due to Progressive Dysarthria on ASR Performance,"" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10097195."
"Dysarthria Voice Disorder Detection Using Mel Frequency Logarithmic Spectrogram and Deep Convolution Neural Network,","Dysarthric speech recognition (DSR), often known as DSR, is an important tool that enables persons with vocal impairments to participate in voice-based automation systems and human-computer interaction. As a result of the poor intelligibility of handicapped speakers, the limited availability of datasets, and the low intra-class and inter-class variability in the speech samples, DSR is an essential component. This research provides a DSR based on the Mel Frequency Logarithmic Spectrogram (MFLS) and Deep Convolutional Neural Network (DCNN). The suggested MFLS+DCNN offers an improved representation of the voice signal in terms of its spectral and temporal characteristics. The results of the MFLS-DCNN scheme are validated using the UASpeech dataset, which was developed based on accuracy, recall, precision, and F1-score. With an accuracy of 96.83%, a recall performance of 0.97, a precision performance of 0.96, and an F1-score of 0.97, the scheme that is proposed has shown a considerable improvement above the conventional state-of-the-art.","keywords: {Human computer interaction;Pathology;Accuracy;Automation;Convolution;Neural networks;Speech recognition;Convolutional neural networks;Speech processing;Spectrogram;Deep learning;automatic speech recognition;affective computing;deep convolution neural networks;dysarthric speech recognition;voice pathology are some of the topics that are being discussed},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10698732&isnumber=10697984,"S. Yadav and D. Yadav, ""Dysarthria Voice Disorder Detection Using Mel Frequency Logarithmic Spectrogram and Deep Convolution Neural Network,"" 2024 First International Conference on Electronics, Communication and Signal Processing (ICECSP), New Delhi, India, 2024, pp. 1-6, doi: 10.1109/ICECSP61809.2024.10698732."
"Wav2vec-Based Detection and Severity Level Classification of Dysarthria From Speech,","Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).","keywords: {Databases;Cepstral analysis;Feature extraction;Medical diagnosis;Speech processing;Task analysis;Spectrogram;Dysarthria;Severity level classification;Wav2vec 2.0;MFCCs},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10094857&isnumber=10094560,"F. Javanmardi, S. Tirronen, M. Kodali, S. R. Kadiri and P. Alku, ""Wav2vec-Based Detection and Severity Level Classification of Dysarthria From Speech,"" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10094857."
"Improved Speaker Independent Dysarthria Intelligibility Classification Using Deepspeech Posteriors,","Individuals with dysarthria are unable to control rapid movement of the velum leading to reduction in intelligibility, audibility, naturalness and efficiency of vocal communication. Automatic intelligibility assessment of dysarthric patients allows clinicians diagnose the impact of therapy and medication and also to plan future course of action. Earlier works have concentrated on building speaker dependent machine learning systems for intelligibility assessment, due to limited availability of data. However, a speaker independent assessment system is of greater use by clinicians. Motivated by this observation, we propose a speaker independent intelligibility assessment system which relies on a novel set of features obtained by processing the output of DeepSpeech, an end to end Speech-to-Text engine. All experiments have been performed on the Universal Access Speech database. An accuracy of 53.9% was obtained using Support Vector Machine based four-class classification system for the speaker independent scenario while the accuracy obtained for the speaker dependent scenario is 97.4%.","keywords: {Support vector machines;Medical treatment;Machine learning;Signal processing;Speech processing;Medical diagnostic imaging;Engines;Dysarthria;intelligibility assessment;openS-MILE;deepspeech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9054492&isnumber=9052899,"A. Tripathi, S. Bhosale and S. K. Kopparapu, ""Improved Speaker Independent Dysarthria Intelligibility Classification Using Deepspeech Posteriors,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 6114-6118, doi: 10.1109/ICASSP40776.2020.9054492."
"PB-LRDWWS System For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge,","For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting (LRDWWS) Challenge, we introduce the PB-LRDWWS system. This system combines a dysarthric speech content feature extractor for prototype construction with a prototype-based classification method. The feature extractor is a fine-tuned HuBERT model obtained through a three-stage fine-tuning process using cross-entropy loss. This fine-tuned HuBERT extracts features from the target dysarthric speaker’s enrollment speech to build prototypes. Classification is achieved by calculating the cosine similarity between the HuBERT features of the target dysarthric speaker’s evaluation speech and prototypes. Despite its simplicity, our method demonstrates effectiveness through experimental results. Our system achieves second place in the final Test-B of the LRDWWS Challenge.","keywords: {Conferences;Buildings;Prototypes;Feature extraction;Keyword spotting;wake-up word detection;dysarthria;fine-tuning;prototype-based classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832235&isnumber=10830793,"S. Wang, J. Zhou, S. Zhao and Y. Qin, ""PB-LRDWWS System For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 586-591, doi: 10.1109/SLT61566.2024.10832235."
"Automated assessment and treatment of speech rate and intonation in dysarthria,","Prosody assessment and treatment in dysarthria is clinically relevant, since prosodic impairment can have a negative impact on speech intelligibility and thus on participation in daily life conversation. We propose a speech-technology based software tool that provides automated numerical and visual feedback on two important aspects of prosody: speech rate and intonation. The tool includes speech rate and intonation algorithms, both specifically developed for the analysis of Dutch dysarthric speech. The tool enables speech-language pathologists to obtain objective measures of these prosodic aspects in a standardized and fast way, and enables dysarthric speakers to practise their prosodic skills intensively without the presence of a speech-language pathologist being required.","keywords: {Classification algorithms;Speech;dysarthria;speech rate;intonation;sentence modality;automated assessment;automated treatment},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6563972&isnumber=6563889,"H. Martens, G. Van Nuffelen, M. De Bodt, T. Dekens, L. Latacz and W. Verhelst, ""Automated assessment and treatment of speech rate and intonation in dysarthria,"" 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops, Venice, Italy, 2013, pp. 382-384, doi: 10.4108/icst.pervasivehealth.2013.252366."
"Identification of hypokinetic dysarthria using acoustic analysis of poem recitation,","Up to 90% of patients with Parkinson's disease (PD) suffer from hypokinetic dysarthria (HD). In this work, we analysed the power of conventional speech features quantifying imprecise articulation, dysprosody, speech dysfluency and speech quality deterioration extracted from a specialized poem recitation task to discriminate dysarthric and healthy speech. For this purpose, 152 speakers (53 healthy speakers, 99 PD patients) were examined. Only mildly strong correlation between speech features and clinical status of the speakers was observed. In case of univariate classification analysis, sensitivity of 62.63 % (imprecise articulation), 61.62% (dysprosody), 71.72% (speech dysfluency) and 59.60% (speech quality deterioration) was achieved. Multivariate classification analysis improved the classification performance. Sensitivity of 83.42% using only two features describing imprecise articulation and speech quality deterioration in HD was achieved. We showed the promising potential of the selected speech features and especially the use of poem recitation task to quantify and identify HD in PD.","keywords: {Speech;High definition video;Feature extraction;Correlation;Parkinson's disease;acoustic analysis;binary classification;hypokinetic dysarthria;Parkinson's disease;poem recitation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8076086&isnumber=8075917,"J. Mucha et al., ""Identification of hypokinetic dysarthria using acoustic analysis of poem recitation,"" 2017 40th International Conference on Telecommunications and Signal Processing (TSP), Barcelona, Spain, 2017, pp. 739-742, doi: 10.1109/TSP.2017.8076086."
"Respiratory and laryngeal influences on voice in post-stroke dysarthria: a pilot study,","Dysarthria is a common disorder among stroke-patients that affects a wide range of speech and voice production processes. While voice disorders in post-stroke dysarthric patients have been well documented, few research has set out to investigate the complicated cause of the disorders, which potentially involve altered respiratory and laryngeal functions and their interaction. In this paper, we report a pilot study that preliminarily examined the respiratory and laryngeal influences on vocal performances in post-stroke dysarthric patients and healthy controls. Respiratory, laryngeal and vocal measures were collected in a maximum phonation time task and analyzed with linear mixed-effect regressions. The results suggested that pathways of influence may be established from respiratory to laryngeal functions and from laryngeal to vocal functions. Patients demonstrated increased laryngeal effort in response to reduced respiratory volumes and expiratory speed, which in turn may have contributed to a reduced voice quality and stability among patients. In addition, a few other findings suggested that reduced respiratory functions may also influence vocal performance through alternative pathways, although more work is needed to establish a clear chain of influence in these cases by unveiling the precise relations between the respiratory, laryngeal and vocal abnormalities in question.","keywords: {Production;Stability analysis;Time measurement;Behavioral sciences;Task analysis;Speech processing;post-stroke dysarthria;voice disorder;respiratory function;laryngeal function},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10037959&isnumber=10037573,"T. Zhao, X. Du, J. Liu, R. Su, N. Yan and L. Wang, ""Respiratory and laryngeal influences on voice in post-stroke dysarthria: a pilot study,"" 2022 13th International Symposium on Chinese Spoken Language Processing (ISCSLP), Singapore, Singapore, 2022, pp. 364-368, doi: 10.1109/ISCSLP57327.2022.10037959."
"Assessing freezing of gait in parkinson's disease using analysis of hypokinetic dysarthria,","Hypokinetic dysarthria (HD) and freezing of gait (FOG) are frequent symptoms of Parkinson's disease (PD). The aim of this work is to reveal pathological mechanisms common for HD and FOG, and use acoustic analysis of dysarthric speech to assess the gait difficulties in PD. We used a correlation analysis to investigate a relationship between speech features and FOG evaluated by freezing of gait questionnaire (FOG-Q). We found speech features quantifying reduced mobility of the articulatory organs significantly correlated with all parts of the questionnaire. Next, we built multivariate regression models to estimate the FOG-Q total score. With this approach, mean estimation error rate of 14.71% was achieved. We confirmed the previous findings of a close relationship between HD and FOG in PD. Furthermore, we showed it is possible to accurately (with the error of approximately 0.5 points) estimate FOG-Q using a reasonable number of conventional speech features.","keywords: {Speech;High definition video;Correlation;Legged locomotion;Turning;Intellectual property;Parkinson's disease;acoustic analysis;freezing of gait;hypokinetic dysarthria;Parkinson's disease;regression},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8076085&isnumber=8075917,"Z. Galaz et al., ""Assessing freezing of gait in parkinson's disease using analysis of hypokinetic dysarthria,"" 2017 40th International Conference on Telecommunications and Signal Processing (TSP), Barcelona, Spain, 2017, pp. 735-738, doi: 10.1109/TSP.2017.8076085."
"SARNet: Speaker-Attentive ResNet for Quantification of Dysarthria Severity,","Analyzing differences in audio data of individuals with articulation disorders from the perspective of human speech and employing objective methods for automated dysarthria evaluation can significantly aid doctors in early patient screening and diagnosis. This proactive approach enables timely intervention and treatment during the initial stages of the condition. This paper presents a speech recognition network(named Speaker-Attentive ResNet (SARNet)) that aggregates and propagates features from different hierarchical levels using an attention-based statistical pooling module, built upon the ResNet architecture. This network aims to extract subtle features specific to individual speakers. The proposed method is evaluated using the TORGO dysarthric speech database, employing eight different acoustic features as well as features aggregated from these eight. Comparative experiments with two other models, Time-Delay Neural Network (TDNN) and Panns-CNN10(Large-Scale Pretrained Audio Neural Networks), demonstrate that due to the superior ability of MFCC(Mel-scaleFrequency Cepstral Coefficients) to emulate human auditory features, the proposed approach achieves a classification accuracy of $98 \%-99 \%$ in measuring speech intelligibility in healthy people, patients, and the severity of patient’s articulation disorders.","keywords: {Accuracy;Databases;Neural networks;Speech recognition;Medical services;Feature extraction;Robustness;Dysarthria;Quantification;Speaker-Attentive ResNet},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10662779&isnumber=10661196,"H. Yin, X. Zhang, Y. Yu and N. Yu, ""SARNet: Speaker-Attentive ResNet for Quantification of Dysarthria Severity,"" 2024 43rd Chinese Control Conference (CCC), Kunming, China, 2024, pp. 8465-8470, doi: 10.23919/CCC63176.2024.10662779."
"Data Augmentation for Dysarthric Speech Recognition Based on Text-to-Speech Synthesis,","In the field of automatic speech recognition (ASR) for people with dysarthria, it is problematic that not enough training speech data can be collected from people with dysarthria. To solve this problem, we propose a method of data augmentation using text-to-speech (TTS) synthesis. In the proposed data augmentation method, a deep neural network (DNN)-based TTS model is trained by utilizing speech data recorded from a speaker with dysarthria, and the trained TTS model is then used to generate the speaker’s speech data for training the ASR model for the speaker. The results of a speech recognition experiment on a person having spinal muscular atrophy (SMA) showed that the speech recognition error rate was improved by using the proposed data augmentation.","keywords: {Training;Deep learning;Error analysis;Conferences;Neural networks;Training data;Speech recognition;speech recognition;data augmentation;dysarthria;speaking disorder;speech synthesis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754798&isnumber=9754592,"Y. Matsuzaka, R. Takashima, C. Sasaki and T. Takiguchi, ""Data Augmentation for Dysarthric Speech Recognition Based on Text-to-Speech Synthesis,"" 2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech), Osaka, Japan, 2022, pp. 399-400, doi: 10.1109/LifeTech53646.2022.9754798."
"Analysis and Classification of Dysarthric Speech,","Classifying dysarthria using neural networks is challenging due to several factors inherent to the nature of dysarthria, the complexity of speech signals, and the requirements of effective machine learning models. Impaired speech classification is challenging for two main reasons: firstly, the data is scarce, and secondly, it is heterogeneous. In this paper, we have trained two different architectures on a dysarthric speech database. A comparison of results shows that according to precision, recall, f1-score, and accuracy, the Deep Neural Network (DNN) model outperforms the classical Convolutional Neural Network (CNN) model, even with a small database. For people with dysarthria, a DNN can improve the performance metrics during the classification of impaired speech by 99% compared to the classical architecture. This improvement is more than that provided by CNN. We have used the TORGO database for this work.","keywords: {Measurement;Databases;Machine learning;Complexity theory;Convolutional neural networks;Deep Neural Network;Convolutional Neural Network;speech classification;dysarthria;impaired speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10482956&isnumber=10482911,"V. Tyagi, A. Dev and P. Bansal, ""Analysis and Classification of Dysarthric Speech,"" 2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Delhi, India, 2023, pp. 1-6, doi: 10.1109/O-COCOSDA60357.2023.10482956."
"Speech Recognition for a Person With Cerebral Palsy Using Whisper Fine-Tuned on Japanese and English Dysarthric Speech,","People with cerebral palsy often have dysarthria, and this makes it hard for them to speak as they wish. In this paper, we present an automatic speech recognition (ASR) model for a person with cerebral palsy based on Whisper. Whisper is highly accurate when performing speech recognition for Japanese speech, but recognition accuracy for Japanese dysarthric speech tends to be low. One possible solution to this problem is to fine-tune Whisper using Japanese dysarthric speech. However, there is a problem in that it is difficult to record a large amount of speech for people with dysarthria. Therefore, in our proposed method, English dysarthric speech is utilized for training Whisper in addition to Japanese dysarthric speech. The results of our proposed method showed an improvement in the error rate of about 1% compared to using only Japanese dysarthric speech as training data.","keywords: {Training;Cerebral palsy;Accuracy;Error analysis;Training data;Speech recognition;Consumer electronics;Automatic speech recognition;speech recognition;multilingual;dysarthria;cerebral palsy;Whisper},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10760680&isnumber=10760245,"K. Haze, R. Takashima and T. Takiguchi, ""Speech Recognition for a Person With Cerebral Palsy Using Whisper Fine-Tuned on Japanese and English Dysarthric Speech,"" 2024 IEEE 13th Global Conference on Consumer Electronics (GCCE), Kitakyushu, Japan, 2024, pp. 419-420, doi: 10.1109/GCCE62371.2024.10760680."
"A joint-feature learning-based voice conversion system for dysarthric user based on deep learning technology,","Dysarthria speakers suffer from poor communication, and voice conversion (VC) technology is a potential approach for improving their speech quality. This study presents a joint feature learning approach to improve a sub-band deep neural network-based VC system, termed J_SBDNN. In this study, a listening test of speech intelligibility is used to confirm the benefits of the proposed J_SBDNN VC system, with several well-known VC approaches being used for comparison. The results showed that the J_SBDNN VC system provided a higher speech intelligibility performance than other VC approaches in most test conditions. It implies that the J_SBDNN VC system could potentially be used as one of the electronic assistive technologies to improve the speech quality for a dysarthric speaker.","keywords: {Speech processing;Feature extraction;Training;Biological system modeling;Artificial neural networks;Task analysis;Mel frequency cepstral coefficient},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8856560&isnumber=8856280,"K. -C. Chen, H. -W. Yeh, J. -Y. Hang, S. -H. Jhang, W. -Z. Zheng and Y. -H. Lai, ""A joint-feature learning-based voice conversion system for dysarthric user based on deep learning technology,"" 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Berlin, Germany, 2019, pp. 1838-1841, doi: 10.1109/EMBC.2019.8856560."
"A multi-smartwatch system for assessing speech characteristics of people with dysarthria in group settings,","Speech-language pathologists (SLPs) frequently use vocal exercises in the treatment of patients with speech disorders. Patients receive treatment in a clinical setting and need to practice outside of the clinical setting to generalize speech goals to functional communication. In this paper, we describe the development of technology that captures mixed speech signals in a group setting and allows the SLP to analyze the speech signals relative to treatment goals. The mixed speech signals are blindly separated into individual signals that are preprocessed before computation of loudness, pitch, shimmer, jitter, semitone standard deviation and sharpness. The proposed method has been previously validated on data obtained from clinical trials of people with Parkinson disease and healthy controls.","keywords: {Speech;Speech processing;Monitoring;Random variables;Blind source separation;Acoustics;Estimation;dysarthria;jitter;knowledge-based speech processing;loudness;multi-smartwatch system;perceptual speech quality;pitch;semitone standard deviation;sharpness;shimmer},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7454559&isnumber=7454459,"H. Dubey, J. C. Goldberg, K. Mankodiya and L. Mahler, ""A multi-smartwatch system for assessing speech characteristics of people with dysarthria in group settings,"" 2015 17th International Conference on E-health Networking, Application & Services (HealthCom), Boston, MA, USA, 2015, pp. 528-533, doi: 10.1109/HealthCom.2015.7454559."
"Weak Speech Supervision: A case study of Dysarthria Severity Classification,","Machine Learning methodologies are making a remarkable contribution, and yielding state-of-the-art results in different speech domains. With this exceptionally significant achievement, a large amount of labeled data is the largest bottleneck in the deployment of these speech systems. To generate massive data, hand-labeling training data is an intensively laborious task. This is problematic for clinical applications where obtaining such data labeled by speech pathologists is expensive and time-consuming. To overcome these problems, we introduce a new paradigm called Weak Speech Supervision (WSS), a first-of-its-kind system that helps users to train state-of-the-art classification models without hand-labeling training data. Users can write labeling functions (i.e., weak rules) to generate weak data from the unlabeled training set. In this paper, we provide the efficiency of this methodology via showing the case study of the severity-based binary classification of dysarthric speech. In WSS, we train a classifier on trusted data (labeled with 100% accuracy) via utilizing the weak data (labeled using weak supervision) to make our classifier model more efficient. Analysis of the proposed methodology is performed on Universal Access (UA) corpus. We got on an average 35.68% and 43.83% relative improvement in terms of accuracy and F1-score w.r.t. baselines, respectively.","keywords: {Training;Training data;Machine learning;Signal processing;Data models;Task analysis;Speech processing;Dysarthria;Severity-based Classification;Data Scarcity;Weak Supervision;CNN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9287502&isnumber=9287310,"M. Purohit, M. Parmar, M. Patel, H. Malaviya and H. A. Patii, ""Weak Speech Supervision: A case study of Dysarthria Severity Classification,"" 2020 28th European Signal Processing Conference (EUSIPCO), Amsterdam, Netherlands, 2021, pp. 101-105, doi: 10.23919/Eusipco47968.2020.9287502."
"End-to-end Dysarthric Speech Recognition Using Multiple Databases,","We present in this paper an end-to-end automatic speech recognition (ASR) system for a person with an articulation disorder resulting from athetoid cerebral palsy. In the case of a person with this type of articulation disorder, the speech style is quite different from that of a physically unimpaired person, and the amount of their speech data available to train the model is limited because their burden is large due to strain on the speech muscles. Therefore, the performance of ASR systems for people with an articulation disorder degrades significantly. In this paper, we propose an end-to-end ASR framework trained by not only the speech data of a Japanese person with an articulation disorder but also the speech data of a physically unimpaired Japanese person and a non-Japanese person with an articulation disorder to relieve the lack of training data of a target speaker. An end-to-end ASR model encapsulates an acoustic and language model jointly. In our proposed model, an acoustic model portion is shared between persons with dysarthria, and a language model portion is assigned to each language regardless of dysarthria. Experimental results show the merit of our proposed approach of using multiple databases for speech recognition.","keywords: {Hidden Markov models;Acoustics;Data models;Databases;Speech recognition;Training;Computational modeling;Speech recognition;multilingual;assistive technology;end-to-end model;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8683803&isnumber=8682151,"Y. Takashima, T. Takiguchi and Y. Ariki, ""End-to-end Dysarthric Speech Recognition Using Multiple Databases,"" ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 6395-6399, doi: 10.1109/ICASSP.2019.8683803."
"On Using the UA-Speech and Torgo Databases to Validate Automatic Dysarthric Speech Classification Approaches,","Although the UA-Speech and TORGO databases of control and dysarthric speech are invaluable resources made available to the research community with the objective of developing robust automatic speech recognition systems, they have also been used to validate a considerable number of automatic dysarthric speech classification approaches. Such approaches typically rely on the underlying assumption that recordings from control and dysarthric speakers are collected in the same noiseless environment using the same recording setup. In this paper, we show that this assumption is violated for the UA-Speech and TORGO databases. Using voice activity detection to extract speech and non-speech segments, we show that the majority of state-of-the-art dysarthria classification approaches achieve the same or a considerably better performance when using the non-speech segments of these databases than when using the speech segments. These results demonstrate that such approaches trained and validated on the UA-Speech and TORGO databases are potentially learning characteristics of the recording environment or setup rather than dysarthric speech characteristics. We hope that these results raise awareness in the research community about the importance of the quality of recordings when developing and evaluating automatic dysarthria classification approaches.","keywords: {Voice activity detection;Databases;Acoustics;Recording;Automatic speech recognition;automatic dysarthria classification;TORGO;UA-Speech;noise;SNR},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10095981&isnumber=10094560,"G. Schu, P. Janbakhshi and I. Kodrasi, ""On Using the UA-Speech and Torgo Databases to Validate Automatic Dysarthric Speech Classification Approaches,"" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095981."
"Modification to correct distortions in stops of dysarthrie speech using TMS320C6713 DSK,","Communication is a bridge between people which enables them to share facts, ideas, feelings etc. with each other. Speech communication is easier and simpler compared to other types of communication. But speech disorders affect one's ability to communicate. Dysarthria is a neuro-motor disorder, where one losses his/her ability to articulate words normally due to tongue/muscle weakness or stroke etc. It results in distorted speech which is hard to understand. Dysarthria affect more on the articulation of consonants (stops in particular) than on the articulation of vowels. Intelligibility varies greatly depending on the extent of neurological damage. As consonants are important than vowels in measuring the intelligibility, distortions in consonant production of dysarthric speech are studied and modifications are performed. This work aims at correcting devoicing of voiced stop (voiced stops are pronounced as unvoiced stops with the same place of articulation) by detecting important time instants such as glottal onset, glottal offset and burst onset thereby modifying the distortions in dysarthric speech using TMS320C6713 DSK with CC Studio 5.5. After the dysarthric speech has been recorded on the DSK 6713 processor modification is done to improve its intelligibility. Two databases were used in the experiment such as Universal Access (UA) database and Torgo database. Out of 65 bursts expected from UA database and 128 bursts expected from Torgo database, 84.6% and 82.81% bursts were detected correctly within 0.3 s intervals respectively.","keywords: {Speech;Databases;Distortion;Spectrogram;Discrete Fourier transforms;Optimization;Speech processing;Landmarks;Dysarthria;Rate of Rise;Glottis;Bursts},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8284657&isnumber=8284511,"A. Raj, A. Anjum, Chethan, Lakshmi, V. Karjigi and M. Rao, ""Modification to correct distortions in stops of dysarthrie speech using TMS320C6713 DSK,"" 2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT), Mysuru, India, 2017, pp. 158-163, doi: 10.1109/ICEECCOT.2017.8284657."
"Prediction of Parkinson’s Disease Using Machine Learning Based on Vocal Frequency,","Parkinson’s disease (PD) is a neurological disorder that affects most people after Alzheimer’s.The ageing neurodegenerative condition leads to PD that reduces dopamine levels in the brain are a defining feature. Tremor, stiffness, bradykinesia, and postural instability are the core characteristics of PD. PD patient’s(PWP) quality of life is impacted by both motor and non-motor symptoms, which may also have an indirect impact on family and caregivers. The patient’s quality of life can be improved and maintained with an early PD diagnosis. There is currently no cure, however there are therapies to control the illness, such as dopaminergic medications. Speech problems associated with PD are categorized under the term hypokinetic dysarthria for the diagnosis of PD.The traditional approach is based on their clinical history and also with their physical examination. With the help of automatic analysis tools, practitioners may diagnose patients, monitor their progress, and conduct regular, economical assessments that are objective. This study uses a machine learning approach to conduct a pilot experiment to identify the existence of dysarthria in speech and gauge its severity. With the help of SVM Algorithm and HyperTunningdisplays 93% accuracy on KAGGLE database test samples.","keywords: {Support vector machines;Neurological diseases;Machine learning algorithms;Magnetic resonance imaging;Medical treatment;Machine learning;Prediction algorithms;Dysarthria detection;Machine learning;SVM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10128416&isnumber=10128171,"N. P. Saravanan, P. Deepika, P. Dhanush and P. Dhanvarsini, ""Prediction of Parkinson’s Disease Using Machine Learning Based on Vocal Frequency,"" 2023 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 2023, pp. 1-6, doi: 10.1109/ICCCI56745.2023.10128416."
"Acoustic Space in Motor Disorders of Speech: Two Case Studies,","Studies on acoustic space have strengthened the view that vowels are acoustically and perceptually defined in terms of their relative positioning in vowel space. Every speaker identifies an optimal vowel space within which perceptual, phonological contrast is maintained. This is an interdisciplinary study involving speech pathology, physics of speech and neurology of speech. Two case studies of dysarthria presented in this paper are -- one Parkinson's disease and one case of acute ischemic stroke with age-gender-language matched controls. A detailed acoustic analysis shows how acoustic space gets considerably reduced, in both PD and stroke, and in these two very different kinds of dysarthrias the acoustic space is also modified very differently. The study also examines the third formant to show that the higher formants are consistently lowered in both PD and stroke. Hypokinetic speech production in these cases is reflected in lower intensity. The results have significant applications in clinical acoustics and in the theoretical fields of neurology of speech, linguistics and phonology.","keywords: {Acoustics;Speech;Aerospace electronics;Production;Parkinson's disease;Nervous system;PD control;Acoustic Space;Dysarthria;Stroke;Parkinson's disease;Intensity;Formants},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6121505&isnumber=6121452,"V. Narang, D. Misra and G. Dalal, ""Acoustic Space in Motor Disorders of Speech: Two Case Studies,"" 2011 International Conference on Asian Language Processing, Penang, Malaysia, 2011, pp. 211-215, doi: 10.1109/IALP.2011.25."
"Robust Assessment of Dysarthrophonic Voice with RASTA-PLP Features: A Nonlinear Spectral Measures,",This paper presents an artificial intelligence based speech signal processing technique to identify dysarthrophonic voice with relative spectral-perceptual linear prediction (RASTA-PLP) features. Dysarthria is a neural motor speech disorder caused by muscular weakness. Voice analysis of dysarthrophonic patients is challenging as this disease has multidimensional effects on the human voice generation system. Conventional spectral analysis is unable to accurately characterize the pathology associated with nonlinear dynamicity of human voice. This work investigates the suitability of RASTA-PLP features excerpted from speech signals to identify dysarthrophonic patients. The speech samples of healthy and dysarthrophonic patients are collected from the Saarbrücken Voice Database (SVD). Several machine learning and Artificial neural network (ANN) based algorithms are developed to evaluate the classification performance of the proposed system. The designed system can achieve excellent performance in terms of accuracy (100%) considering female and male subjects separately.,"keywords: {Support vector machines;Pathology;Machine learning algorithms;Signal processing algorithms;Artificial neural networks;Machine learning;Feature extraction;Accuracy;ANN;classifier;dysarthria;dysarthophonia;deep learning;machine learning;pathology;PLP;RASTA-PLP;speech;vocal disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10126695&isnumber=10126146,"R. Islam and M. Tarique, ""Robust Assessment of Dysarthrophonic Voice with RASTA-PLP Features: A Nonlinear Spectral Measures,"" 2023 2nd International Conference on Mechatronics and Electrical Engineering (MEEE), Abu Dhabi, United Arab Emirates, 2023, pp. 74-78, doi: 10.1109/MEEE57080.2023.10126695."
"Comparison of English and Chinese Speech Recognition Using High-Density Electromyography,","Speaking different languages requires different ways of pronunciation, and the muscular activities associated with phonation show different articulation styles. Therefore, clarifying the contributions of the articulatory muscles in different regions, such as the face and neck, is helpful for automatic speech recognition. However, it remains unclear how the articulatory muscles at different positions affect the classification accuracies of speech recognition across different languages. In this study, the technique of high-density surface electromyography (HD sEMG) was proposed to investigate the role of different articulatory muscles in classifying English and Chinese speaking tasks, respectively. The HD sEMG signals were recorded by 120 electrodes evenly placed on the facial and neck muscles across six subjects while they were speaking five English and Chinese daily words. Four time-domain features were extracted from sEMG recordings and used to construct a linear-discriminant-analysis classifier for speech recognition. The results showed that the classification accuracies of using neck sEMG were higher than that of using facial sEMG in both English and Chinese recognition tasks. The accuracies for Chinese speaking tasks were significantly higher than that for English when using facial sEMG only. Moreover, there was no significant difference in accuracies between the two types of languages when using neck sEMG. This study might provide useful information about the contributions of different articulatory muscles, and pave the way for automatic speech recognition across different languages for patients with dysarthria.","keywords: {Muscles;Task analysis;Neck;Speech recognition;Electrodes;Facial muscles;Face recognition;high-density surface electromyography;speech recognition;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9047706&isnumber=9047668,"M. Zhu et al., ""Comparison of English and Chinese Speech Recognition Using High-Density Electromyography,"" 2019 13th International Conference on Sensing Technology (ICST), Sydney, NSW, Australia, 2019, pp. 1-5, doi: 10.1109/ICST46873.2019.9047706."
"Hybrid CNN-LSTM network to detect Dysarthria using Mel-Frequency Cepstral Coefficients,","Dysarthria is a speech problem acquired at birth due to cerebral palsy (CP) or developed after severe brain damage. Dysarthria affects more than 70% of Parkinson's patients and 10% to 65% of people with traumatic brain injury. It is critical to detect dysarthria and other voice speech difficulties early to diagnose the underlying cause. Intelligent systems capable of identifying dysarthria with incredible precision have been developed using audio processing techniques and various deep learning models. This paper presents a hybrid CNN-LSTM model for classifying patients with dysarthria using audio recordings. The CNN-LSTM combination helps capture spatial and temporal information where CNN acts as a feature extractor while LSTM functions as a classifier. The proposed model was trained on the publicly available 9184 audio recordings from the TORGO dataset, and various audio augmentation techniques were employed to generate synthetic data. A total of 128 features were extracted using Mel Frequency Cepstral Coefficients (MFCC) and fed into the architecture as inputs. The K-fold cross-validation technique was used to avoid overfitting and increase the generalization capability of the model. The proposed architecture achieved a state-of-the-art 99.59% accuracy on the dataset. The presented work will minimize the workload of speech pathologists and help them detect dysarthria precisely and effectively.","keywords: {Pediatrics;Parkinson's disease;Education;Feature extraction;Brain modeling;Audio recording;Convolutional neural networks;Dysarthria;Audio Processing;Feature Extraction;Convolution Neural Network;Long Short Term Network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10039514&isnumber=10039478,"K. Vora, D. Padalia, D. Mehta and D. Sharma, ""Hybrid CNN-LSTM network to detect Dysarthria using Mel-Frequency Cepstral Coefficients,"" 2022 5th International Conference on Advances in Science and Technology (ICAST), Mumbai, India, 2022, pp. 615-621, doi: 10.1109/ICAST55766.2022.10039514."
"Optimizing Dysarthria Wake-Up Word Spotting: an End-to-End Approach For SLT 2024 LRDWWS Challenge,","Speech has emerged as a widely embraced user interface across diverse applications. However, for individuals with dysarthria, the inherent variability in their speech poses significant challenges. This paper presents an end-to-end Pretrain-based Dual-filter Dysarthria Wake-up word Spotting (PD-DWS) system for the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge. Specifically, our system improves performance from two key perspectives: audio modeling and dual-filter strategy. For audio modeling, we propose an innovative 2 branch- d 2 v 2 model based on the pre-trained data2vec $2(\mathrm{~d} 2 \mathrm{v} 2)$, which can simultaneously model automatic speech recognition (ASR) and wake-up word spotting (WWS) tasks through a unified multi-task finetuning paradigm. Additionally, a dual-filter strategy is introduced to reduce the false accept rate (FAR) while maintaining the same false reject rate (FRR). Experimental results demonstrate that our PD-DWS system achieves an FAR of 0.00321 and an FRR of 0.005, with a total score of 0.00821 on the test-B eval set, securing first place in the challenge.","keywords: {Conferences;User interfaces;Multitasking;Data augmentation;Automatic speech recognition;LRDWWS challenge;2brach-d2v2;dualfilter;wake-up word spotting},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832263&isnumber=10830793,"S. Liu et al., ""Optimizing Dysarthria Wake-Up Word Spotting: an End-to-End Approach For SLT 2024 LRDWWS Challenge,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 578-585, doi: 10.1109/SLT61566.2024.10832263."
"Enhancing Dysarthria Diagnosis With Deep Learning Techniques,","Dysarthria is characterized by delayed, slurred speech that can be challenging to comprehend. This condition is frequently brought on by nerve injury that affects the muscles used to produce speech. Depending on which muscles are affected and the underlying cause, symptoms can differ greatly. Talk therapy can help with early detection and intervention, which can enhance treatment outcomes. Convolutional neural networks (CNNs) are the method suggested here for detecting dysarthria from audio data. The method highlights the distinctions in speech patterns between people with and without dysarthria by using feature extraction, notably Mel-frequency cepstral coefficients (MFCC), and audio visualization approaches. These properties are used in the development and training of several neural network models, such as CNN, Long Short Term Memory (LSTM), Gated Recurrent Units (GRU), Bidirectional LSTM, SimpleRNN, and Deep Neural Networks (DNN). These models’ performance is assessed with the use of confusion matrices and classification reports. This all-inclusive dysarthria detection pipeline includes advanced deep learning approaches for evaluation, model training, and data preprocessing. The objective is to develop a trustworthy dysarthria detection system that will help healthcare professionals identify and treat the ailment early on.","keywords: {Training;Deep learning;Accuracy;Telemedicine;Muscles;Feature extraction;Software;Convolutional neural networks;Object recognition;Testing;Dysarthria Detection;Deep Learning Techniques;MFCC;Spectrogram;Neural network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10725823&isnumber=10723316,"S. Keerthika, N. Abinaya, S. Santhiya, K. Nithika, T. Dhanush and C. B. Arvind, ""Enhancing Dysarthria Diagnosis With Deep Learning Techniques,"" 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT), Kamand, India, 2024, pp. 1-6, doi: 10.1109/ICCCNT61001.2024.10725823."
"Using Novel Hybrid Convolutional Neural Network for Dysarthria Diagnosis,","Dysarthria is a motor speech disorder characterized by articulation and phonation difficulties resulting from speech muscle weakness, paralysis, or incoordination. A precise and timely diagnosis of dysarthria is essential for effective treatment and management of the condition, as it may deteriorate over time or be a precursor to a much more serious disease. On the other hand, this is becoming a severe problem in recent times owing to the rising ageing population and the prevalence of neurological disorders among such people. Hence early detection of dysarthria is deemed essential for the timely management of the disease. In recent years, Artificial Intelligence (AI) applications have shown promising results in various audio processing tasks and incorporated into pathological voice analysis for disease diagnosis. The majority of previous studies on dysarthria detection employed Machine Learning (ML) and Deep Learning (DL) models as the disease classification models. In light of this, this study presents a novel hybrid approach for classifying dysarthria based on audio data using Convolutional Neural Network (CNN) and Support Vector Machine (SVM). According to the experimental results, the proposed classification schema achieves an accuracy of 98.25 % compared to previous research work. Overall, our proposed method aims to automate the classification process, enabling faster and more reliable diagnoses.","keywords: {Support vector machines;Analytical models;Soft sensors;Sociology;Motors;Feature extraction;Data models;Artificial Intelligence;Convolutional Neural Network;Dysarthria;Speech Classification;Machine Learning;Deep Learning;SVM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10487652&isnumber=10487139,"N. N. Thilakarathne, K. Galajit, J. Karnjana, W. P. Pa and H. Yassin, ""Using Novel Hybrid Convolutional Neural Network for Dysarthria Diagnosis,"" 2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE), Nadi, Fiji, 2023, pp. 01-06, doi: 10.1109/CSDE59766.2023.10487652."
"Breathiness Indices for Classification of Dysarthria Based on Type and Speech Intelligibility,","Dysarthria classification based on intelligibility level is useful for speech pathologists for deciding the therapy. However, intelligibility assessment also depends on perceptual attributes like hypernasality, breathiness, slow rate, short pauses etc. These perceptual attributes vary depending on the cause for dysarthria giving rise to different types of dysarthria. In this work, we explore the use of breathiness features for intelligibility assessment of dysarthria and for distinguishing type of dysarthria. Voiced segments from two controlled speakers, two dysarthric speakers with low and mid intelligibility level each from UA database are used in the work. Features were analysed for use in dysarthria intelligibility assessment vs. distinguishing type of dysarthria.","keywords: {Databases;Harmonic analysis;Feature extraction;Jitter;Perturbation methods;Correlation;Support vector machines;Intelligibilty;Breathiness;Cerebral palsy;Spastic;Athetoid},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9032852&isnumber=9032713,"H. M. Chandrashekar, V. Karjigi and N. Sreedevi, ""Breathiness Indices for Classification of Dysarthria Based on Type and Speech Intelligibility,"" 2019 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET), Chennai, India, 2019, pp. 266-270, doi: 10.1109/WiSPNET45539.2019.9032852."
"Cross-lingual Dysarthria Severity Classification for English, Korean, and Tamil,","Data scarcity hinders research on dysarthria severity classification due to the limited size of datasets. While the crosslingual approach has been applied to alleviate the problem, the roles of language-specific features have been underestimated. This paper proposes a cross-lingual classification method for English, Korean, and Tamil, which employs both language-independent features and language-unique features. First, we extract thirty-nine features from diverse speech dimensions such as voice quality, pronunciation, and prosody. Second, feature selections are applied to identify the optimal feature set for each language. A set of shared features and a set of distinctive features are distinguished by comparing the feature selection results of the three languages. Lastly, automatic severity classification is performed, utilizing the two feature sets. Notably, the proposed method removes different features by languages to prevent the negative effect of unique features for other languages. Accordingly, eXtreme Gradient Boosting (XGBoost) algorithm is employed for classification, due to its strength in imputing missing data. In order to validate the effectiveness of our proposed method, two baseline experiments are conducted: experiments using the intersection set of mono-lingual feature sets (Intersection) and experiments using the union set of monolingual feature sets (Union). According to the experimental results, our method achieves better performance with a 67.14% F1 score, compared to 64.52% for the Intersection experiment and 66.74% for the Union experiment. Further, the proposed method attains better performances than mono-lingual classifications for all three languages, achieving 17.67%, 2.28%, 7.79% relative percentage increases for English, Korean, and Tamil, respectively. The result specifies that commonly shared features and language-specific features must be considered separately for cross-language dysarthria severity classification.","keywords: {Training;Deep learning;Neural networks;Interference;Information processing;Feature extraction;Boosting},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9980124&isnumber=9979808,"E. J. Yeo, K. Choi, S. Kim and M. Chung, ""Cross-lingual Dysarthria Severity Classification for English, Korean, and Tamil,"" 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 2022, pp. 566-574, doi: 10.23919/APSIPAASC55919.2022.9980124."
"The Open-Access Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) Database for Intelligent Assessment,","Early objective identification and assessment of dysarthria due to neurological deficits are essential for neurorehabilitation. Developing a system to achieve this requires a large-scale database of pathological information with detailed labeling. In the present study, a high-quality Chinese multimodal audio-visual database, consisting of 64 subacute stroke patients and 25 healthy participants, named the “Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) database”, was established. The materials of MSDM include a series of speech tasks such as syllables, characters, words, sentences, and spontaneous speech. All audio-visual data in this database were manually annotated and simultaneously verified by experienced researchers. Additionally, comprehensive clinical assessments of speech-motor function (e.g., Frenchay Dysarthria Assessment) and cognitive function (e.g., Montreal Cognitive Assessment) for each individual were included in the database. In conclusion, the MSDM database is believed to provide sufficient data resources for developing automatic assessment and speech recognition methods and contribute to understanding the pathological mechanisms of dysarthria.","keywords: {Pathology;Databases;Speech recognition;Stroke (medical condition);Neurorehabilitation;Object recognition;Labeling;Mandarin;Subacute stroke;Dysarthria;Audio-video database},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10799983&isnumber=10799969,"J. Liu et al., ""The Open-Access Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) Database for Intelligent Assessment,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 131-135, doi: 10.1109/ISCSLP63861.2024.10799983."
"Significance of Filterbank Structure for Capturing Dysarthric Information through Cepstral Coefficients,","The short-term Fourier transform magnitude spectra (STFT-MS) computed from the dysarthric speech deviates nonlinearly from the normal speech in different frequency bands depending on underlying sound units. This discriminating information can be captured by segmenting the STFT-MS into different frequency bands following the power spectra of board categories of sound units. Motivated by this observation in this study, we have computed the cepstral coefficients by analyzing the STFT-MS in 0–500 Hz, 500–2000 Hz, 2000–4000 Hz, and 4000 – 8000Hz, respectively for 16 kHz sampled speech data. Each of the selected frequency bands is analyzed by using a 30 channel Mel filterbank. The log filterbank energies computed for each sub-band are then polled together and discrete cosine transform (DCT) is applied to compute the cepstral coefficients, here termed as sub-band enhanced Mel frequency cepstral coefficients (SE-MFCC). The i-vector based dysarthric intelligibility assessment system reported in this study shows that the SEMFCC outperforms the conventional Mel frequency cepstral coefficients (MFCC), and the cepstral coefficients computed using inverse-Mel filterbank (IMFCC), and linear filterbank (LFCC). The score level combination of SE-MFCC with the MFCC further improves the overall performance.","keywords: {Fourier transforms;Conferences;Filter banks;Signal processing;Discrete cosine transforms;Mel frequency cepstral coefficient;Task analysis;Cepstral coefficients;Dysarthria;Filterbank;Inverse-Mel scale;Linear scale;Mel scale;Sub-band spectra},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9840837&isnumber=9840511,"L. P. Sahu and G. Pradhan, ""Significance of Filterbank Structure for Capturing Dysarthric Information through Cepstral Coefficients,"" 2022 IEEE International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2022, pp. 1-5, doi: 10.1109/SPCOM55316.2022.9840837."
"Data Augmentation Based on Frequency Warping for Recognition of Cleft Palate Speech,","In this paper, we present an automatic speech recognition (ASR) system for the speech of a person with a cleft lip and palate (CLP). The accuracy of speech recognition for a person with CLP is lower than that of a physically-unimpaired (PU) person because the CLP speech has characteristics that differ from those of a PU person; moreover, the amount of available training data is quite limited. In the field of ASR for PU people, data augmentation and self-supervised learning have been studied to tackle this problem of data scarcity. In this paper, we evaluate the effectiveness of those approaches on CLP speech recognition, and propose a data augmentation technique based on frequency warping. The formant of CLP speech tends to fluctuate compared to that of PU people. In order to compensate for the large variety of formant components, our data augmentation method stretches or contracts the spectrogram through the frequency axis. The experimental results on an ASR task with two CLP subjects showed that both data augmentation and self-supervised learning were effective for CLP speech recognition, and our proposed method further improved the performance of those two approaches based on conventional SpecAugment techniques.","keywords: {Lips;Training data;Speech recognition;Information processing;Character recognition;Task analysis;Speech processing;speech recognition;data augmentation;self-supervised learning;cleft lip and palate;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9689552&isnumber=9689213,"K. Fujiwara et al., ""Data Augmentation Based on Frequency Warping for Recognition of Cleft Palate Speech,"" 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Tokyo, Japan, 2021, pp. 471-476."
"Improving Pronunciation Clarity of Dysarthric Speech Using CycleGAN with Multiple Speakers,","In this paper, we propose a method that improves pronunciation clarity of dysarthric speech using CycleGAN based non-parallel voice conversion. This method converts dysarthric speech into healthy speech using CycleGAN. We considered the use of single and multiple speakers as healthy speech. The subjective evaluations showed the effectiveness of using multiple speakers as healthy speech.","keywords: {Conferences;Training data;Consumer electronics;Dysarthria;Pronunciation clarity;Voice conversion;CycleGAN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292041&isnumber=9291703,"S. Imai, T. Nose, A. Kanagaki, S. Watanabe and A. Ito, ""Improving Pronunciation Clarity of Dysarthric Speech Using CycleGAN with Multiple Speakers,"" 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), Kobe, Japan, 2020, pp. 366-367, doi: 10.1109/GCCE50665.2020.9292041."
"Adaptation of a Pronunciation Dictionary for Dysarthric Speech Recognition,","In the general framework of an automatic speech recognition system, a pronunciation dictionary, that is a mapping table from a phoneme sequence to a word, is used both in the processes of training and recognition. However, this pronunciation dictionary is not always adequate in the case of dysarthric speech recognition because dysarthric people often have difficulty pronouncing words in the same way they are pronounced in the dictionary. In this paper, we investigate the adaptation of a pronunciation dictionary to an individual dysarthric person and evaluate the effectiveness of adapting the dictionary using a dysarthric speech recognition task. In the proposed method, in order to find rules we can use to modify a dictionary, we analyze the pattern of mis-recognition in the phoneme recognition results. By following the extracted rules, we add pronunciations to the dictionary for the target dysarthric person. We evaluate the effectiveness of the adapted pronunciation dictionary on a continuous speech recognition task and demonstrate that the adapted dictionary can decrease the word error rate.","keywords: {Training;Dictionaries;Error analysis;Conferences;Speech recognition;Life sciences;Task analysis;Speech recognition;dysarthria;pronunciation dictionary;lexicon},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754848&isnumber=9754592,"Y. Sawa, R. Takashima and T. Takiguchi, ""Adaptation of a Pronunciation Dictionary for Dysarthric Speech Recognition,"" 2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech), Osaka, Japan, 2022, pp. 631-635, doi: 10.1109/LifeTech53646.2022.9754848."
"Comparative Analysis of Glottal and Vocal Tract Features in Dysarthria,","Dysarthria is a neurological disorder associated with the muscles in the vocal tract system that is caused by problems with coordination. It affects the vocal tract system and the glottis with different levels of severity. This research deals with the role of vocal tract (i.e., filter) and glottal excitation source in dysarthric speech, using Glottal Flow Model with Iterative Adaptive Inverse Filtering (GFM-IAIF). By decomposing the vocal tract and glottal source into two separate components, we were able to identify which component (source or filter) is affected the most. Our research showed that the vocal tract system is the most affected part, determining a better classification of features the magnitude spectrum-based Mel frequency cepstral coefficient (MFCC) resulted in 95.75% accuracy for vocal tract components and lower accuracy of 86.5% for glottal source components. In the same way, Modified Group Delay Cepstral Coefficients (MGDCC) correlated with a test accuracy of 94.43% of the vocal tract in comparison with 88.70% of the glottal source. These outcomes reveal that the vocal tract gets most damaged due to dysarthria and thus, emphasis the use of specific diagnostic and therapy interventions targeted at this area. The research not only explores feature fusion but also recommends further efforts to refine dysarthria diagnosis and treatment.","keywords: {Neurological diseases;Adaptation models;Accuracy;Filtering;Medical treatment;Information processing;Muscles;Iterative algorithms;Delays;Mel frequency cepstral coefficient;Dysarthria;glottal flow model using the iterative adaptive inverse filtering algorithm;phase;magnitude-based components},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10849291&isnumber=10848533,"G. S. Sahasra, K. Swapna, A. Srivastava, A. Pusuluri and H. A. Patil, ""Comparative Analysis of Glottal and Vocal Tract Features in Dysarthria,"" 2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Macau, Macao, 2024, pp. 1-6, doi: 10.1109/APSIPAASC63619.2025.10849291."
"A Fuzzy Cognitive Map Hierarchical Model for Differential Diagnosis of Dysarthrias and Apraxia of Speech,","This paper presents a novel soft computing system for differential diagnosis of the dysarthrias and apraxia of speech based on well accepted dysarthrias' classification system used by speech and language pathologists. The dysarthrias and apraxia are complex disorders of speech because they represent a variety of neurological disturbances that can potentially affect every component of speech production. Since an accurate diagnosis is a very challenging task for the clinician, the under development system based on hierarchical fuzzy cognitive maps (FCMs) will be used as a ""second opinion"" or training system. The hierarchical FCM differential diagnosis system is capable of differentiating between the six types of dysarthria as well as apraxia. The system was tested using published case studies and real patients and examples are presented here","keywords: {Fuzzy cognitive maps;Speech;Parkinson's disease;Natural languages;Muscles;Production systems;Medical treatment;Educational technology;Auditory system;System testing;Fuzzy Cognitive Maps;Differential Diagnosis;Knowledge-Based Systems;Decision Support Systems},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1616954&isnumber=33900,"V. C. Georgopoulos and G. A. Malandraki, ""A Fuzzy Cognitive Map Hierarchical Model for Differential Diagnosis of Dysarthrias and Apraxia of Speech,"" 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference, Shanghai, China, 2005, pp. 2409-2412, doi: 10.1109/IEMBS.2005.1616954."
"A Novel Gamified Approach for Collecting Speech Data from Young Children with Dysarthria: Feasibility and Positive Engagement Evaluation,","Dysarthria is a common and treatable speech problem in children, and computer-assisted speech therapy is a promising way for children's speech therapy. However, data collection poses a significant challenge for computer-assisted therapy, especially when it comes to gathering speech data from young children, particularly those with dysarthria. Finding a better way to collect young children's speech data is, therefore, an urgent need. This paper prompted a gamified speech collection method and carried out an experiment to compare the participation time, error rate, and collection efficiency with the gamified method and with a traditional method where adults are imitated. Moreover, we also explore whether the gamified collection methods increase the children's positive engagement. A feasibility study including 10 children with dysarthria and 10 children without speech problems was conducted. Their participation duration, number of spoken utterances, number of mispronunciation utterances, and a questionnaire about children's engagement attitude were recorded. The findings indicate that the gamified collecting method reduces pronunciation mistake rates in children with dysarthria while also increasing their engagement to participate.","keywords: {Visualization;Error analysis;Medical treatment;Games;Data collection;Assistive technologies;Speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10776229&isnumber=10776063,"N. Liu, E. Barakova and T. Han, ""A Novel Gamified Approach for Collecting Speech Data from Young Children with Dysarthria: Feasibility and Positive Engagement Evaluation,"" 2024 17th International Convention on Rehabilitation Engineering and Assistive Technology (i-CREATe), Shanghai, China, 2024, pp. 1-5, doi: 10.1109/i-CREATe62067.2024.10776229."
"Edge Computing Solutions Supporting Voice Recognition Services for Speakers with Dysarthria,","In the framework of Automatic Speech Recognition (ASR), the synergism between edge computing and artificial intelligence has led to the development of intelligent objects that process and respond to human speech. This acts as a key enabler for multiple application scenarios, such as smart home automation, where the user’s voice is an interface for interacting with appliances and computer systems. However, for millions of speakers with dysarthria worldwide, such a voice interaction is impossible because nowadays ASR technologies are not robust to their atypical speech commands. So these people, who also live with severe motor disabilities, are unable to benefit from many voice assistant services that might support their everyday life. To cope with the above challenges, this paper proposes a deep learning approach to isolated word recognition in the presence of dysarthria conditions, along with the deployment of customized ASR models on machine learning powered edge computing nodes. In this way, we work toward a low-cost, portable solution with the potential to operate next to the user with a disability, e.g., in a wheelchair or beside a bed, in an always active mode. Finally, experiments show the goodness (in terms of word error rate) of our speech recognition solution in comparison with other studies on isolated word recognition for impaired speech.","keywords: {Deep learning;Home appliances;Automation;Error analysis;Computational modeling;Wheelchairs;Smart homes;artificial intelligence;dysarthria;edge computing;assistive technology;smart home automation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10181220&isnumber=10181105,"D. Mulfari, L. Carnevale, A. Galletta and M. Villari, ""Edge Computing Solutions Supporting Voice Recognition Services for Speakers with Dysarthria,"" 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW), Bangalore, India, 2023, pp. 231-236, doi: 10.1109/CCGridW59191.2023.00047."
"Dysarthria Diagnosis and Dysarthric Speaker Identification Using Raw Speech Model,","Dysarthria is a medical condition that causes difficulty in producing coherent speech due to muscle paralysis or weakness. This article presents a unique approach to identifying dysarthric speakers using a deep learning model that works directly with unprocessed speech waveforms. By eliminating the need for feature extractions, the model's resistance to noise and voice variability is increased. The proposed approach utilizes a SincNet layer model with multiple initializations including Mel, Erb, and Bark scales for dysarthria detection (DD) and dysarthric speaker identification (DSI). Bark scaling, aligning better with human auditory perception and capturing distinctive acoustic features, notably outperforms other initialization methods. When Bark scaling was employed, the study's results demonstrated outstanding accuracy rates of 97.0% for DD and 88.0% for DSI. The results demonstrated exceptional performance, that surpassing existing literature benchmarks.","keywords: {Deep learning;Noise;Muscles;Benchmark testing;Feature extraction;Acoustics;Paralysis;Diagnosis of dysarthria;dysarthric speaker identification;SincNet;raw waveforms;Bark scaling;deep learning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10485694&isnumber=10485661,"S. Sajiha, K. Radha, D. V. Rao, V. Akhila and N. Sneha, ""Dysarthria Diagnosis and Dysarthric Speaker Identification Using Raw Speech Model,"" 2024 National Conference on Communications (NCC), Chennai, India, 2024, pp. 1-6, doi: 10.1109/NCC60321.2024.10485694."
"Acoustic and Kinematic Examination of Dysarthria in Cantonese Patients of Parkinson’s Disease,","Hypokinetic dysarthria is one of the core symptoms of Parkinson's disease, characterized by reduced loudness, slurred speech and distorted consonant productions. Dopaminergic medication for Parkinson's disease (PD) has been proven to be effective in treating limb and gross motor movement problems. However, the literature sees contradictory findings regarding dopaminergic effects on speech problems associated with PD. Previous perceptual and acoustic studies of PD hypokinetic dysarthria mostly involved heterogeneous population and variations of speech tasks which complicated data interpretation. Also, the lack of kinematic data limited our understanding of the details of articulation errors associated with PD, as well as the medication effects. Electromagnetic articulography (EMA) enabled the examination of PD articulatory patterns with high degree of accuracy and safety. The aim of the present study was to address these inconsistencies by providing an integrative description of basic kinematic and acoustic parameters of speech production about the dopaminergic effect on early PD speech by using EMA. The results revealed a significant improvement of articulatory function on dopaminergic effects for PD speech, evidenced by increased vowel space, the velocity and accelerated velocity initiation and coordination of articulation during bilabial or alveolar productions.","keywords: {Kinematics;Acoustics;Auditory system;Sun;Parameter extraction;Schedules;Lips;Parkinson’;s disease, hypokinetic dysarthria, dopaminergic medication, kinematic analysis, acoustic analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8706615&isnumber=8706262,"Y. Sun, M. L. Ng, C. Lian, L. Wang, F. Yang and N. Yan, ""Acoustic and Kinematic Examination of Dysarthria in Cantonese Patients of Parkinson’s Disease,"" 2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP), Taipei, Taiwan, 2018, pp. 354-358, doi: 10.1109/ISCSLP.2018.8706615."
"In-Domain Data Augmentation to Enhance Severity Level Classification of Dysarthria from Speech,","In this paper, we present our endeavor to construct an automatic dysarthria severity level classification system tai-lored for low-resource dysarthric speech datasets. The scarcity of available speech data from dysarthric speakers poses a significant challenge to training an effective classification system. Addressing this challenge, we devised a robust baseline system by blending a distinctive set of features, encompassing temporal, prosodic, and spectral information, with the traditional MFCC features. This amalgamation aptly captures the nuanced characteristics of dysarthric speech, facilitating efficient model training. To tackle the constraints of low-resource conditions, we explored four prominent augmentation techniques: Speaking Rate, Pitch, Formant, and Vocal Tract Length Perturbation (VTLP) modification based data augmentation for the task of severity classification. The explored data augmentation gives a significant reduction in the classification error rate (CER), and VTLP-based data augmentation is superior among others. Further, we also investigated combinations of explored data augmentation methods, fortifying the reliability of our dysarthria severity classification system. The combined novel augmentation gives a noteworthy relative improvement of 42.86% over the baseline on the dysarthric severity classification.","keywords: {Training;Error analysis;System performance;Training data;Speech enhancement;Signal processing;Data augmentation;Dysarthria;severity classification;speech mod-ification;speech augmentation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10631597&isnumber=10631292,"B. Karumuru, P. Sapkota and H. Kathania, ""In-Domain Data Augmentation to Enhance Severity Level Classification of Dysarthria from Speech,"" 2024 International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2024, pp. 1-5, doi: 10.1109/SPCOM60851.2024.10631597."
"A modern approach to dysarthria classification,","This work deals with the assessment of neurological diseases known as dysarthrias, using a novel approach based on objective and perceptual features extracted from pathological speech signals. A methodology for the classification of dysarthria is developed in which digital signal processing algorithms are used to appraise the severity of those features less reliably judged by the clinicians, while the others are taken directly from perceptual judgments or medical records. The assessment process evaluates the performance of two different classifiers and compares them with the traditional assessment system. The first approach is based on the lineal discriminant analysis and the second is a non-lineal technique based on self-organizing maps. The non-lineal classifier provided the highest percent of correct classification and the most accurate information on the relevance of the features in the classifier decision. It also provided a bi-dimensional representation of de data that allows a better understanding of the correspondence between the speech deviations and the location of the damage in the peripheral or central nervous system.","keywords: {Speech processing;Speech analysis;Databases;Diseases;Lesions;Pathology;Appraisal;Medical diagnostic imaging;Central nervous system;Personal communication networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1280248&isnumber=28615,"E. Castillo Guerra and D. F. Lovey, ""A modern approach to dysarthria classification,"" Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No.03CH37439), Cancun, Mexico, 2003, pp. 2257-2260 Vol.3, doi: 10.1109/IEMBS.2003.1280248."
"Observations from a Simple Vocal-Tract-Model's Behaviour for PD-Dysarthric Speech: Applicability,","The uniform-element tube model of a speaker's vocal tract is a by-product from LPC speech analysis. It is used here to observe a speaker's articulation. The aim is an insight into possible articulatory weaknesses of PD (Parkinson-Disease) patients who suffer from dysarthria, i.e., speaking problems. Approved auditive methods exist for evaluation of the patients' speech handicaps, applying also instrumental signal features. But a direct view on vocal-tract movements can be an additional diagnostic aid. Due to its real-time potential, the simple estimation of vocal-tract areas from LPC analysis is regarded here, despite its known impreciseness. In a first measurement set, it is checked with fluent speech of healthy persons, whether any reaction appears on articulatory changes between clear and intentionally mumbled speech. Then, with sustained vowels of healthy and slightly as well as strongly handicapped speakers, the potential of the approach to display such differences is examined.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578023&isnumber=8577984,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578023&isnumber=8577984,"U. Heute and G. Schmidt, ""Observations from a Simple Vocal-Tract-Model's Behaviour for PD-Dysarthric Speech: Applicability,"" Speech Communication; 13th ITG-Symposium, Oldenburg, Germany, 2018, pp. 1-5."
"A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information,","Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses.","keywords: {Accuracy;Databases;Signal processing;Phonetics;Acoustics;Speech processing;Dysarthria;Multi-modal;Cross-Attention;Pronunciation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10889515&isnumber=10887541,"A. M, K. Gurugubelli, K. V and A. K. Vuppala, ""A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information,"" ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025, pp. 1-5, doi: 10.1109/ICASSP49660.2025.10889515."
"Processing of pathological changes in speech caused by dysarthria,","Computer analysis of voice isolated sounds may lead to identification of parameters correlated with neurological diseases. This paper presents results of preliminary research of voice pathological changes caused by dysarthria. The selection of linguistic material was characterized according to the place and manner of articulation in the phonetic system of Polish. Results of clinical examination allowed to determine simple markers of neurodegenerative diseases, which serves as a basis for construction of objective examination model.","keywords: {Pathology;Speech processing;Diseases;Tongue;Centralized control;Control systems;Nervous system;Isolation technology;Materials science and technology;Speech analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1595343&isnumber=33566,"T. Orzechowski, A. Izworski, R. Tadeusiewicz, K. Chmurzynska, P. Radkowski and I. Gatkowska, ""Processing of pathological changes in speech caused by dysarthria,"" 2005 International Symposium on Intelligent Signal Processing and Communication Systems, Hong Kong, China, 2005, pp. 49-52, doi: 10.1109/ISPACS.2005.1595343."
"Significance of Entropy Based Features For Dysarthric Severity Level Classification,","Dysarthria is a motor speech disorder arising from impairment of muscles that makes difficult to form or pronounce words while speaking. In this paper, we introduce an approach of multiband entropy based features extracted from dysarthric speech signals for dysarthric severity level classification. Generally, entropy is measured as the number of bits of information contained in each message signal. The information content of these signal measures how much randomness or uncertainity contains in a signal. Extending this, we use a frame-wise processing technique to divide the speech signal into short frames which allows for detailed analysis of different characteristics of speech signal. Furthermore we divide frames into equal sub bands and compute the entropy and zero mean entropy in each sub band using Gabor filterbank. It is expected that the mean entropy of very low dysarthric severity is lower compared to high dysarthric severity level indicating that randomness is increasing as the severity level of dysarthria is increasing. Experimental analysis were conducted on extensively used dataset namely UA Speech. Results were carried out by Convolutional Neural Network (CNN), along with 5-cross validation. The results are compared against standard MFCC, LFCC, and glottal source based LFRCC. It was observed that the addition of entropy information boosted the performance of MFCC by 4.38%, LFCC by 2.54%, and LFRCC by 1.88% compared to their traditional techniques indicating the crucial information captured by the entropy for dysarthria severity classification.","keywords: {Filter banks;Muscles;Feature extraction;Speech;Entropy;Noise robustness;Convolutional neural networks;Speech processing;Mel frequency cepstral coefficient;Standards;Dysarthria;Entropy;Gabor Filterbank},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10848870&isnumber=10848533,"M. Avula, A. Pusuluri and H. A. Patil, ""Significance of Entropy Based Features For Dysarthric Severity Level Classification,"" 2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Macau, Macao, 2024, pp. 1-6, doi: 10.1109/APSIPAASC63619.2025.10848870."
"Automated Acoustic Analysis in Parkinson’s Disease Using a Smartphone*,","Dysarthria is a common speech disorder in Parkinson’s Disease (PD). The Dysarthria Analyzer software has emerged as a viable tool for automatic speech analysis in PD and quantification of dysarthria severity. However, most studies use the Dysarthria Analyzer with recordings obtained under tightly controlled conditions and high-quality microphones, and the utility of the Dysarthria Analyzer when used with recordings acquired under non-ideal conditions, such as in busy clinical settings, remains unexplored. This study investigates the Dysarthria Analyzer’s performance in a setting more akin to a clinical environment using a smartphone. We obtained data from three groups, including healthy controls (HC), PD patients with their deep brain stimulation on (ON-DBS), and PD patients with their DBS off (OFF-DBS). We found a significant decrease in pitch variability and an increase in speech rate for the OFF-DBS group compared to the HC. Furthermore, most of the estimated values for the speech markers fall within the reported values in the literature. Our findings demonstrate that the Dysarthria Analyzer effectively extracts relevant speech markers even when used with recordings obtained under non-ideal conditions, emphasizing its potential for widespread clinical adoption.Clinical Relevance— Our findings demonstrate the potential of using smartphone recordings obtained in clinical environments for automatic objective speech analysis. These findings are relevant for developing a clinical tool that can be widely accessible and easily implemented during routine clinical visits of PD to improve the assessment of dysarthria in PD.","keywords: {Speech analysis;Deep brain stimulation;Software;Acoustics;Recording;Engineering in medicine and biology;Diseases;Microphones},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10782673&isnumber=10781494,"G. T. Acevedo T. et al., ""Automated Acoustic Analysis in Parkinson’s Disease Using a Smartphone*,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10782673."
"An Optimal Speech Recognition Module for Patient's Voice Monitoring System in Smart Healthcare Applications,","During recent years, health care domain has rapidly developed in which patients and medical resources are directly connected with the smart way that enables Smart Health Care. The growth in design and development of a speech automated system will provide a life assistant service in smart health care environment. In automating the speech system, speech recognition is one of the basic steps to understand the human recognition and their behaviors. These speech recognition systems will be very much accessible for speakers who suffer from dysarthria, a neurological disability that damages the control of motor speech articulators. In this paper, the main objective is to develop an efficient speech recognition module based on the Voice Input Voice Output Communication Aid (VIVOCA) architecture that can device a support aid to the people with DYSARTHRIA. Totally there are seven features extracted from each noise eliminated real time bilingual isolated word speech signal data uttered by a speaker both in Tamil and English languages. Vector Quantization based Genetic Algorithm codebook is created for the recognition modeling. Optimization of Hidden Markov Model (HMM) is done based on Particle Swarm Optimization (PSO) method to improve the recognition accuracy compared to the conventional HMM and also experiment results of the proposed module shows 95% of accuracy. The proposed module will be very much useful for developing a speech recognition system that facilitates the patients and persons with special needs for communication. The proposed module is also evaluated for its complexity which will be therefore efficient for low consumption of energy.","keywords: {Hidden Markov models;Speech recognition;Feature extraction;Genetic algorithms;Vector quantization;Training;Particle swarm optimization;Dysarthria;Smart Healthcare Applications;Isolated speech recognition;Particle Swarm Optimization (PSO);Vector Quantization (VQ) and Hidden Markov Model (HMM)},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8488841&isnumber=8488772,"M. Krishnaveni, P. Subashini, J. Gracy and M. Manjutha, ""An Optimal Speech Recognition Module for Patient's Voice Monitoring System in Smart Healthcare Applications,"" 2018 Renewable Energies, Power Systems & Green Inclusive Economy (REPS-GIE), Casablanca, Morocco, 2018, pp. 1-6, doi: 10.1109/REPSGIE.2018.8488841."
"Optical force and distance sensing in intraoral devices for stroke rehabilitation: a distance calibration and force classification approach,","Stroke survivors often suffer from oro-facial impairments, affecting swallowing function and speech production. Measuring tongue pressure and position intraorally can help to improve therapy for both symptoms, but space inside the oral cavity is extremely limited and such devices can easily be prohibitively large and obstructive if too many sensors are needed. In this work, we present our efforts to sense the force of the tongue exerted against the hard palate and the tongue-palate distance, using only optical proximity sensors. To explore the feasibility and accuracy of this approach and to evaluate the selected sensor, we conducted a study with 10 subjects and measured the sensor's response to 10 discrete distances ranging from 0mm to 30mmbetween tongue and sensor, and to a continuously increasing tongue force against the sensor from 0.1N to 8N. For distance measurements, an existing in-situ calibration method was applied and verified that yielded errors of less than 2mm for the estimated distances in nearly every case. For force measurements, a Bayesian classification approach was adopted to map sensor data to two force regions (below and above a certain boundary value), where up to 84.1% (average: 71.7 %) of ADC values were classified correctly within-sample.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578052&isnumber=8577984,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578052&isnumber=8577984,"C. Wagner, S. Stone and P. Birkholz, ""Optical force and distance sensing in intraoral devices for stroke rehabilitation: a distance calibration and force classification approach,"" Speech Communication; 13th ITG-Symposium, Oldenburg, Germany, 2018, pp. 1-5."
"Abstract of Presentations on March 9th,",Presents abstracts for the articles comprising the conference proceedings.,URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754844&isnumber=9754592,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754844&isnumber=9754592,"""Abstract of Presentations on March 9th,"" 2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech), Osaka, Japan, 2022, pp. 89-94, doi: 10.1109/LifeTech53646.2022.9754844."
"Speech task based automatic classification of ALS and Parkinson’s Disease and their severity using log Mel spectrograms,","We consider the task of speech based classification of patients with amyotrophic lateral sclerosis (ALS), Parkinson's disease (PD) and healthy controls (HC). Recent work in convolutional neural networks (CNN) to solve image classification problems raises the possibility of utilizing spectral representation of speech for detection of neurological diseases. In this paper, a spectrogram based approach is used. Feeding overlapping windows to the CNN makes sure that the temporal aspects are considered by using short signal segments or wide analysis filters. A three class (ALS, PD or HC) dysarthria classification is performed. In addition, we perform two severity classification experiments for ALS (5 class) and PD (3 class) respectively. Experiments are conducted on both baseline MFCC data [1] and log Mel spectrograms. Classification results show that for several audio lengths, models trained on log Mel spectrograms consistently outperform those of MFCC's. The ability of the network to accurately classify different classes is evaluated via the area under receiver operating characteristic curve [2],[3]. The findings from this study could aid in better detection and monitoring of ALS and PD diseases.","keywords: {Mel frequency cepstral coefficient;Task analysis;Spectrogram;Neurons;Muscles;Parkinson's disease;spectrograms;CNN;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9179503&isnumber=9179490,"B. Suhas et al., ""Speech task based automatic classification of ALS and Parkinson’s Disease and their severity using log Mel spectrograms,"" 2020 International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2020, pp. 1-5, doi: 10.1109/SPCOM50965.2020.9179503."
"A Review and Classification of Amyotrophic Lateral Sclerosis with Speech as a Biomarker,","Amyotrophic Lateral Sclerosis (ALS) is a motor system neurodegenerative disease that affects speech impairment, spinal, respiratory and swallowing difficulties in patients. It has gradually increased in elderly people in recent years and is not easy to diagnose. The ALS bulbar form system is based on detecting dysarthria speech classification in discriminating healthy subjects from ALS patients. To construct the classification model by using various machine learning techniques, the studies used datasets related to speech impairment recordings of ALS patients and healthy subjects. Early diagnosis of ALS can somewhat improve the patient’s quality of life to an extent. Sustained vowel phonation is very useful for classifying ALS and healthy control (HC). In this study, jitter and shimmer were used to extract features. A support vector machine classifier was applied, and it classified ALS/HC with an accuracy of 98.5%. This study presents a detailed review of various machine learning techniques applied to the speech signal for the diagnosis of ALS and their impact on future research in this direction.","keywords: {Support vector machines;Focusing;Machine learning;Jitter;Speech enhancement;Feature extraction;Recording;Dysarthria;ALS;Speech Impairment;Machine learning;Speech disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10308048&isnumber=10306339,"S. M. Shabber, M. Bansal and K. Radha, ""A Review and Classification of Amyotrophic Lateral Sclerosis with Speech as a Biomarker,"" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-7, doi: 10.1109/ICCCNT56998.2023.10308048."
"Teager Energy Cepstral Coefficients For Classification of Dysarthric Speech Severity-Level,","Dysarthria is a neuro-motor speech impairment that renders speech unintelligibility, which is generally imperceptible to humans w.r.t severity-levels. Dysarthric speech classification acts as a diagnostic tool for evaluating the advancement in a patient's severity condition and also aids in automatic dysarthric speech recognition systems (an important assistive speech technology). This study investigates the significance of Teager Energy Cepstral Coefficients (TECC) in dysarthric speech classification using three deep learning architectures, namely, Convolutional Neural Network (CNN), Light-CNN (LCNN), and Residual Networks (ResNet). The performance of TECC is compared with state-of-the-art features, such as Short-Time Fourier Transform (STFT), Mel Frequency Cepstral Coefficients (MFCC), and Linear Frequency Cepstral Coefficients (LFCC). In addition, this study also investigate the effectiveness of cepstral features over the spectral features for this problem. The highest classification accuracy achieved using UA-Speech corpus is 97.18%, 94.63%, and 98.02% (i.e., absolute improvement of 1.98%, 1.41%, and 1.69%) with CNN, LCNN, and ResNet, respectively, as compared to the MFCC. Further, we evaluate feature discriminative capability using $F1$-score, Matthew's Correlation Coefficient (MCC), Jaccard index, and Hamming loss. Finally, analysis of latency period w.r.t. state-of-the-art feature sets indicates the potential of TECC for practical deployment of the severity-level classification system.","keywords: {Correlation coefficient;Fourier transforms;Deep architecture;Speech recognition;Information processing;Convolutional neural networks;Indexes;Dysarthria;UA-Speech Corpus;TEO Profiles;TECC. 1},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9980322&isnumber=9979808,"A. Kachhi, A. Therattil, A. T. Patil, H. B. Sailor and H. A. Patil, ""Teager Energy Cepstral Coefficients For Classification of Dysarthric Speech Severity-Level,"" 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 2022, pp. 1462-1468, doi: 10.23919/APSIPAASC55919.2022.9980322."
"Sequence-to-Sequence Models in Italian Atypical Speech Recognition,","In the domain of automatic speech recognition (ASR), we explore the usage of a state-of-the-art transformer-based sequence-to-sequence model to build a speaker-dependent isolated word recognizer for native Italian speakers with a speech disorder, such as dysarthria. In particular, this paper is concerned with a self-supervised learning approach, where the Wav2Vec2 has been fine-tuned on our private Italian corpus containing a total of 41 hours of speech contributions authored by 191 individuals with a disability and atypical speech. The discussed approach has been also evaluated thanks to collaboration of sixteen speakers with diverse degrees of speech disorders (mild, moderate, severe), and our analysis has shown a remarkable performance of our ASR system, with an overall word recognition accuracy of 97.4%.","keywords: {Computers;Dictionaries;Databases;Collaboration;Speech recognition;Self-supervised learning;Computer architecture;Transformers;Recording;Context modeling;Atypical speech recognition;self-supervised learning;dysarthria;transformers;ASR},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10733600&isnumber=10733557,"D. Mulfari, L. Carnevale and M. Villari, ""Sequence-to-Sequence Models in Italian Atypical Speech Recognition,"" 2024 IEEE Symposium on Computers and Communications (ISCC), Paris, France, 2024, pp. 1-6, doi: 10.1109/ISCC61673.2024.10733600."
"Monitoring Progress of Parkinson's Disease Based on Changes in Phonation: a Pilot Study,","Hypokinetic dysarthria (HD) is a frequent symptom of idiopathic Parkinson's disease (PD). Although it is hypothesized its progress is tightly linked with changes in other motor/non-motor features of PD, it has not been proved yet. The aim of this work is to employ acoustic analysis of sustained phonation in order to identify significant correlates between phonatory measures and motor/non-motor deficits in a two-year follow-up study. For this purpose, we repeatedly quantified a sustained vowel/a/ in 51 PD patients who were also assessed by 5 common clinical scales. In addition, a multivariate regression model was trained to predict the motor/non-motor deficits in the horizon of two years. Results suggest that mainly instability in vocal folds oscillation increases with the progress of PD and with overall cognitive decline. Based on the acoustic analysis, the change in clinical scores could be predicted with the error in the range of 11.83-19.60 %.","keywords: {Acoustics;Indexes;Correlation;Parkinson's disease;Multivariate regression;Monitoring;Standards;acoustic analysis;follow-up study;hypokinetic dysarthria;Parkinson's disease;phonation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8441307&isnumber=8441166,"Z. Galaz et al., ""Monitoring Progress of Parkinson's Disease Based on Changes in Phonation: a Pilot Study,"" 2018 41st International Conference on Telecommunications and Signal Processing (TSP), Athens, Greece, 2018, pp. 1-5, doi: 10.1109/TSP.2018.8441307."
"An automatic diagnosis and assessment of dysarthric speech using speech disorder specific prosodic features,","To diagnose and classify the dysarthric speech, speech language pathologist (SLP) conducts a listening test. On the basis of the scores given by listeners the dysarthria is diagnosed and assessed. The above mentioned method is costly, time consuming and not very accurate. Unlike the traditional method, this research proposes an automatic diagnosis and assessment of dysarthria. The aim of this paper is to diagnose and classify the severity of dysarthria. The speech disorder specific prosodic features are selected by using genetic algorithm. The diagnosis and assessment of dysarthric speech is done by support vector machines. During diagnosis the classification accuracy of 98% has been achieved. And 87% of the dysarthric speech utterances are correctly classified. The standard UASPEECH database has been used in this work.","keywords: {Speech;Feature extraction;Testing;Genetic algorithms;Support vector machines;Databases;Training;Dysarthric speech;diagnosis;assessment;speech disorder;prosodic features;support vector machines},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7760933&isnumber=7760810,"G. Vyas, M. K. Dutta, J. Prinosil and P. Harár, ""An automatic diagnosis and assessment of dysarthric speech using speech disorder specific prosodic features,"" 2016 39th International Conference on Telecommunications and Signal Processing (TSP), Vienna, Austria, 2016, pp. 515-518, doi: 10.1109/TSP.2016.7760933."
"Signal Analysis for Voice Evaluation in Parkinson’s Disease,","Parkinson's Disease (PD) is a neurodegenerative disorder that is frequently correlated with vowel articulation difficulties. The phonation problem arises in patients affected by PD is commonly known as Parkinsonian Dysarthria and identifiedby vocal signal analysis. The analysis supporte physicians and specialists in early detection and monitoring of dysarthria aiming, to increase patients life quality and to evaluate the efficacy of treatments. We investigate on vocal signal analysis correlation with speech patterns related to PD. Vowel parameters are considered as discriminant elements among PD patients and healthy subjects. Aim of this work is to define possible indicators for dysarthria in PD patients.","keywords: {Tongue;Signal analysis;Diseases;Speech;Electronic mail;Correlation;Jitter;Parkinson Disease;vocal signal analysis;vowel metric;acoustic analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8031207&isnumber=8031114,"D. Mirarchi, P. Vizza, G. Tradigo, N. Lombardo, G. Arabia and P. Veltri, ""Signal Analysis for Voice Evaluation in Parkinson’s Disease,"" 2017 IEEE International Conference on Healthcare Informatics (ICHI), Park City, UT, USA, 2017, pp. 530-535, doi: 10.1109/ICHI.2017.72."
"Two-stage and Self-supervised Voice Conversion for Zero-Shot Dysarthric Speech Reconstruction,","Dysarthria is a motor speech disorder commonly associated with conditions such as cerebral palsy, Parkinson’s disease, amyotrophic lateral sclerosis, and stroke. Individuals with dysarthria typically exhibit significant speech difficulties, including imprecise articulation, lack of fluency, slow speech rate, and decreased volume and clarity, which can hinder their ability to communicate effectively with others. We propose a two-stage Voice Conversion method to enhance the reconstruction of dysarthric speech. In the first stage, we develop a KNN-VC approach based on a same-gender-retrieval strategy to preliminarily repair the dysarthric speech. In this stage, we match the dysarthric speech only with normal speech of the same gender. In the second stage, we adapt so-vits-svc to restore the speaker’s timbre and improve the sound quality of the speech repaired in the first stage. Both objective and subjective evaluations were conducted on the dataset of the Low Resource Dysarthria Wake-Up Word Spotting Challenge (LRDWWS Challenge) shows that the proposed approach can achieve some improvements in terms of speaker similarity, speech intelligibility and naturalness for unknown speakers, and these evaluations also show our method has a good Zero-shot performance. Our audio samples can be accessed online 1.","keywords: {Cerebral palsy;Nearest neighbor methods;Maintenance engineering;Speech enhancement;Motors;Timbre;Diseases;dysarthric speech reconstruction;Any-to-any;Zero-shot;voice conversion},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10661160&isnumber=10660673,"D. Liu, Y. Lin, H. Bu and M. Li, ""Two-stage and Self-supervised Voice Conversion for Zero-Shot Dysarthric Speech Reconstruction,"" 2024 International Conference on Asian Language Processing (IALP), Hohhot, China, 2024, pp. 423-427, doi: 10.1109/IALP63756.2024.10661160."
"Computer aided methods for diagnosis and therapy of speech breathing disorders,","Computer-aided methods for the diagnosis and therapy of speech breathing disorders caused by brain damage (dysarthrias) are described. These methods make it possible to evaluate a speech disorder for diagnostic purposes using objective parameters. Special therapeutic tasks which proceed in accordance with the principle of biofeedback make it possible for the patient to carry out self-therapy through individualized exercise packages. Pathological speech patterns can be readily recognized, classified and quantified using the methods presented. It is concluded that these methods are clinically practical and show promise of becoming a useful clinical tool for the treatment of dysarthrias.<>","keywords: {Medical treatment;Belts;Speech processing;Military computing;Biological control systems;Immune system;Automatic control;Context;Speech analysis;Pathology},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=95920&isnumber=3080,"M. Finsterwald, M. Vogel and K. Trondle, ""Computer aided methods for diagnosis and therapy of speech breathing disorders,"" Images of the Twenty-First Century. Proceedings of the Annual International Engineering in Medicine and Biology Society,, Seattle, WA, USA, 1989, pp. 663-664 vol.2, doi: 10.1109/IEMBS.1989.95920."
"Automatic detection of voice onset time in dysarthric speech,","Although a number of speech disorders reflect varying involvement of brain areas, recently published automatic speech analyses have primarily been limited to hypokinetic dysarthria in Parkinson's disease (PD). Therefore, the aim of the present study was to provide an automatic algorithm suitable for the assessment of voice onset time (VOT) in various dysarthria types. Twenty-four PD participants with hypokinetic dysarthria and 40 Huntington's disease (HD) subjects with hyperkinetic dysarthria were included. These two types of dysarthria were selected in the design of a robust algorithm as they contain most of the dysarthric patterns found among all dysarthria subtypes. For a 10 ms threshold, the proposed algorithm reached approximately 90% accuracy in PD speakers and 80% accuracy in HD speakers. The accuracy of 80% obtained in HD was superior to the performance of 55% achieved by a previous algorithm designed particularly for hypokinetic dysarthria in PD.","keywords: {High definition video;Speech;Diseases;Algorithm design and analysis;Estimation;Robustness;Accuracy;Voice Onset Time;Dysarthria;Parkinson's disease;Huntington's disease;Speech disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178790&isnumber=7177909,"M. Novotný, J. Pospíšil, R. Čmejla and J. Rusz, ""Automatic detection of voice onset time in dysarthric speech,"" 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), South Brisbane, QLD, Australia, 2015, pp. 4340-4344, doi: 10.1109/ICASSP.2015.7178790."
"Hybrid CNN-GRU Model for Predicting Dysarthric Speech Using Deep Learning Approaches,","This research study presents the novel method for dysarthria speech detection and classification. In order to create systems that are capable of accurately categorizing speech patterns that are impacted by dysarthria, it is vital to have speech detection and classification capabilities for dysarthria. The condition known as dysarthria, which is characterized by difficulties in articulation, phonation, resonance, and prosody, is a disorder that leads to a reduction in the intelligibility of speech. This method of classification necessitates the development of machine learning or deep learning models, more specifically a combination of Convolutional Neural Networks (CNN) and Gated Recurrent Units (GRU), in order to accomplish the objective of automatically classifying dysarthric speech into categories such as severity or presence/absence of dysarthria. The primary purpose of dysarthria speech categorization is to bring about the development of a reliable instrument that can be used for identifying, categorizing, and managing dysarthric speech. As an added benefit, the classification of dysarthric speech allows for the adaptation of treatment methods and the acquisition of information about the underlying causes of the condition. The individuals who are affected by this speech difficulty are the target audience for this strategy, which ultimately aims to enhance the quality of life of those individual. The study addresses the challenge of classifying dysarthric speech caused by neurological impairments using a Hybrid CNN-GRU model, achieving accuracies of 70.3% (high-severity), 38% (severe-moderate), 34.5% (moderate), and 10.3% (mild dysarthria).","keywords: {Voice activity detection;Deep learning;Accuracy;Transfer learning;Speech recognition;Speech enhancement;Predictive models;Feature extraction;Robustness;Convolutional neural networks;Dysarthria Speech disordered;Convolutional Neural Networks;Gated Recurrent Units;Natural language processing},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10915057&isnumber=10914686,"S. K. Chelliah and A. N, ""Hybrid CNN-GRU Model for Predicting Dysarthric Speech Using Deep Learning Approaches,"" 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), Bengaluru, India, 2025, pp. 1880-1885, doi: 10.1109/IDCIOT64235.2025.10915057."
"Smart Voice Assistance for Speech disabled and Paralyzed People,","People who are paralyzed, confront numerous challenges in meeting their basic necessities on a daily basis. It is very difficult to understand the speech of people with dysarthria, amyotrophic lateral sclerosis (ALS) and similar conditions. Automatic speech command recognition system will enhance the lifestyle of people with voice disorder like dysarthria and paraplegics. The proposed work will convert the speech command of paralyzed people into text and send it to the care taker's mobile with the help of Twilio message services. Algorithms like Support Vector Machine (SVM) and Convolutional Neural network (CNN) model is used for speech command identification and speech to text conversion. CNN model yields an accuracy of 90.62%, whereas the SVM algorithm gives a very low accuracy. The developed TensorFlow model is deployed in the flask server.","keywords: {Support vector machines;Text categorization;Speech recognition;Speech enhancement;Feature extraction;Message service;Convolutional neural networks;text classification;raspberry pi;convolutional neural network;speech disability;MFCC},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9740922&isnumber=9740653,"L. T, I. R, A. A, A. K. S and S. S, ""Smart Voice Assistance for Speech disabled and Paralyzed People,"" 2022 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 2022, pp. 1-5, doi: 10.1109/ICCCI54379.2022.9740922."
"A Tool for Training Speech Imitation Accuracy,","Dysarthria is a neurological motor speech disorder that commonly results in reduced intelligibility. Communication partners can learn to better understand the speech of someone with dysarthria through perceptual training. Vocal imitation of the degraded speech during perceptual training has been shown to elevate this learning. A tool that provides the learner with real-time feedback regarding the accuracy of their imitation attempts during training may further enhance this learning. We describe a training tool that compares dysarthric speech productions with the imitation attempts of healthy subjects, using a two-level dynamic warp that accounts for both spectral and temporal degradation. Feature vectors derived from both the spectrogram and LPC are examined.","keywords: {Training;Tools;Distortion measurement;Heuristic algorithms;Distortion;Microsoft Windows;Feature extraction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8645286&isnumber=8645065,"A. -W. Al-Dulaimi, S. Budge, S. A. Borrie, T. K. Moon and J. H. Gunther, ""A Tool for Training Speech Imitation Accuracy,"" 2018 52nd Asilomar Conference on Signals, Systems, and Computers, Pacific Grove, CA, USA, 2018, pp. 1086-1090, doi: 10.1109/ACSSC.2018.8645286."
"Leveraging OpenAI Whisper Model to Improve Speech Recognition for Dysarthric Individuals,","Automatic Speech Recognition (ASR) systems are pivotal in facilitating human-technology interactions through voice commands. However, individuals with dysarthria face significant challenges in benefiting from these technologies due to their speech disorder. This paper proposes finetuning the Whisper model for Dysarthric Speech Recognition (DSR) by incorporating additional features extracted from Mel-frequency cepstral coefficients (MFCCs). By combining spectrograms and MFCCs within an attention mechanism, the model creates a richer feature representation, with spectrograms providing broader context and MFCCs highlighting crucial formant frequencies. The attention mechanism dynamically weighs the importance of each feature based on specific speech segments and dysarthric speech characteristics. Furthermore, a hierarchical attention approach is adopted, which encompasses a two-stage attention mechanism. This mechanism directs attention at both local and global levels, facilitating the capture of both fine-grained details and broader contextual information within the speech signal. This study involved the development and training of 45 speaker-adaptive dysarthric ASR systems. The proposed model achieves an average Word Recognition Accuracy (WRA) of 74.08%, showing a notable enhancement compared to the benchmark of 69.23%. The findings underscore the efficacy of the proposed approach in addressing dysarthria-related challenges in ASR systems.","keywords: {Training;Technological innovation;Attention mechanisms;Accuracy;Face recognition;Benchmark testing;Speech enhancement;DSR;MFCC;UASpeech;WRA;Whisper},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10673628&isnumber=10673418,"V. R, H. D and L. D. V. Anand, ""Leveraging OpenAI Whisper Model to Improve Speech Recognition for Dysarthric Individuals,"" 2024 Asia Pacific Conference on Innovation in Technology (APCIT), MYSORE, India, 2024, pp. 1-5, doi: 10.1109/APCIT62007.2024.10673628."
"Empirical Analysis of Machine Learning Models on Parkinson’s Speech Dataset,","Parkinson’s disease (PD) is a chronic and progressive neurodegenerative disorder that worsens over time. Diagnosing PD primarily relies on clinical assessments, which can be costly, time-consuming, and invasive. These evaluations may also be subjective and vulnerable to inaccuracy. Dysarthria, a common condition characterised by delayed and distorted speech, frequently coexists with PD. This opens up the possibility of using speech features for diagnostic reasons. This research paper explores different machine learning models trained on numerical data of changes in speech patterns due to Dysarthria. These models are based on classifiers such as Artificial Neural Networks (ANN), Multi-Layer Perceptron (MLP), Random Forests, and Decision Trees. Additionally, we compare the performance of a newly introduced HyperTab classifier with the existing models. Our findings demonstrate the significant potential of machine learning in diagnosing PD based on speech analysis. This progress holds the promise of creating a cost-effective tool to expedite disease detection. Furthermore, this research is of utmost importance in offering essential support to regions with limited access to specialized medical facilities.","keywords: {Analytical models;Biological system modeling;Artificial neural networks;Predictive models;Feature extraction;Numerical models;Recording;HyperTab;Classification;Evaluation Metrics;Deep Learning;Parkinson’s Disease},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10482963&isnumber=10482911,"B. Sachdeva, H. Rathee, P. Gambhir and P. Bansal, ""Empirical Analysis of Machine Learning Models on Parkinson’s Speech Dataset,"" 2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Delhi, India, 2023, pp. 1-5, doi: 10.1109/O-COCOSDA60357.2023.10482963."
"Investigation of Cross Modality Feature Fusion for Audio-Visual Dysarthric Speech Assessment,","Dysarthria, a speech disorder resulting from neurological conditions, presents significant obstacles to speech intelligibility and daily communication. Automatic dysarthria assessment has the capability to provide low-cost diagnosis and treatment assistant support for such diseases as Parkinson's disease, Alzheimer's disease, and stroke. This study investigates the efficacy of cross-modality feature fusion using audio-visual data for the automatic assessment of dysarthric speech. Leveraging advanced self-supervised learning models, AV-HuBERT and Wav2Vec 2.0, we develop a multimodal system to enhance dysarthria severity classification. Utilizing the Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) dataset, which includes synchronized audio and lip movement video recordings, our system achieves promising performance. Experimental results demonstrate that our back-end fusion and feature fusion approaches both outperform traditional single-modality methods, with the best back-end fusion system achieving a speaker-level F1 score of 0.841 while the best feature-level fusion system achieving a speaker-level F1 score of 0.772. This study marks the first application of pre-trained self-supervised learning models for multimodal dysarthria assessment, highlighting the potential for the assistance of diagnosis and treatment.","keywords: {Visualization;Parkinson's disease;Lips;Self-supervised learning;Feature extraction;Synchronization;Alzheimer's disease;Video recording;dysarthria speech;automatic assessment;modality fusion;AV-HuBERT;Wav2Vec 2.0},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800618&isnumber=10799969,"Y. Jiang et al., ""Investigation of Cross Modality Feature Fusion for Audio-Visual Dysarthric Speech Assessment,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 141-145, doi: 10.1109/ISCSLP63861.2024.10800618."
"Experimental Investigation on STFT Phase Representations for Deep Learning-Based Dysarthric Speech Detection,","Mainstream deep learning-based dysarthric speech detection approaches typically rely on processing the magnitude spectrum of the short-time Fourier transform of input signals, while ignoring the phase spectrum. Although considerable insight about the structure of a signal can be obtained from the magnitude spectrum, the phase spectrum also contains inherent structures which are not immediately apparent due to phase discontinuity. To reveal meaningful phase structures, alternative phase representations such as the modified group delay (MGD) and instantaneous frequency (IF) spectra have been investigated in several applications. The objective of this paper is to investigate the applicability of the unprocessed phase, MGD, and IF spectra for dysarthric speech detection. Experimental results show that dysarthric cues are present in all considered phase representations. Further, it is shown that using phase representations as complementary features to the magnitude spectrum is beneficial for deep learning-based dysarthric speech detection, with the combination of magnitude and IF spectra yielding a high performance. The presented results should raise awareness in the research community about the potential of the phase spectrum for dysarthric speech detection and motivate research into novel architectures which optimally exploit magnitude and phase information.","keywords: {Voice activity detection;Fourier transforms;Conferences;Signal processing;Feature extraction;Acoustics;Delays;phase;modified group delay;instantaneous frequency;CNN;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747205&isnumber=9746004,"P. Janbakhshi and I. Kodrasi, ""Experimental Investigation on STFT Phase Representations for Deep Learning-Based Dysarthric Speech Detection,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 6477-6481, doi: 10.1109/ICASSP43922.2022.9747205."
"Speech Recognition-Based Feature Extraction For Enhanced Automatic Severity Classification in Dysarthric Speech,","Due to the subjective nature of current clinical evaluation, the need for automatic severity evaluation in dysarthric speech has emerged. DNN models outperform ML models but lack user-friendly explainability. ML models offer explainable results at a feature level, but their performance is comparatively lower. Current ML models extract various features from raw waveforms to predict severity. However, existing methods do not encompass all dysarthric features used in clinical evaluation. To address this gap, we propose a feature extraction method that minimizes information loss. We introduce an ASR transcription as a novel feature extraction source. We finetune the ASR model for dysarthric speech, then use this model to transcribe dysarthric speech and extract word segment boundary information. It enables capturing finer pronunciation and broader prosodic features. These features demonstrated an improved severity prediction performance to existing features: balanced accuracy of 83.72%.","keywords: {Accuracy;Conferences;Speech recognition;Speech enhancement;Predictive models;Feature extraction;Data mining;Dysarthria severity classification;dysarthric speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832261&isnumber=10830793,"Y. Choi, J. Lee and M. -W. Koo, ""Speech Recognition-Based Feature Extraction For Enhanced Automatic Severity Classification in Dysarthric Speech,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 953-960, doi: 10.1109/SLT61566.2024.10832261."
"The Nemours database of dysarthric speech,","The Nemours database is a collection of 814 short nonsense sentences; 74 sentences spoken by each of 11 male speakers with varying degrees of dysarthria. Additionally, the database contains two connected-speech paragraphs produced by each of the 11 speakers. The database was designed to test the intelligibility of dysarthric speech before and after enhancement by various signal processing methods, and is available on CD-ROM. It can also be used to investigate general characteristics of dysarthric speech such as production error patterns. The entire database has been marked at the word level and sentences for 10 of the 11 talkers have been marked at the phoneme level as well. The paper describes the database structure and techniques adopted to improve the performance of a Discrete Hidden Markov Model (DHMM) labeler used to assign initial phoneme labels to the elements of the database. These techniques may be useful in the design of automatic recognition systems for persons with speech disorders, especially when limited amounts of training data are available.","keywords: {Databases;Signal design;Testing;Speech enhancement;Speech processing;Signal processing;CD-ROMs;Hidden Markov models;Automatic speech recognition;Training data},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=608020&isnumber=13325,"X. Menendez-Pidal, J. B. Polikoff, S. M. Peters, J. E. Leonzio and H. T. Bunnell, ""The Nemours database of dysarthric speech,"" Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96, Philadelphia, PA, USA, 1996, pp. 1962-1965 vol.3, doi: 10.1109/ICSLP.1996.608020."
"Corpus Design and Automatic Speech Recognition for Deaf and Hard-of-Hearing People,","This study describes automatic speech recognition (ASR) for the deaf and hard-of-hearing people. In the relevant literature, ASR for the deaf has been studied in a manner similar to the recognition of speech by people with dysarthria. However, prior studies have been conducted over a small number of deaf speakers. Therefore, to date, it remains unclear how the performance of ASR varies with different speakers. We conducted phoneme recognition experiments using speech from 12 deaf students to obtain analytical results from the perspective of ASR performance.","keywords: {Conferences;Consumer electronics;Automatic speech recognition;ASR;deaf speech;end-to-end;classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9621959&isnumber=9621353,"A. Kobayashi, K. Yasu, H. Nishizaki and N. Kitaoka, ""Corpus Design and Automatic Speech Recognition for Deaf and Hard-of-Hearing People,"" 2021 IEEE 10th Global Conference on Consumer Electronics (GCCE), Kyoto, Japan, 2021, pp. 17-18, doi: 10.1109/GCCE53005.2021.9621959."
"Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition,","Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)","keywords: {Medical treatment;Speech recognition;Predictive models;Stroke (medical condition);Speech enhancement;Signal processing;IP networks;Dysarthric Speech;Inappropriate Pause Detection;Pause Detection;Speech Recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10447681&isnumber=10445803,"J. Lee, Y. Choi, T. -J. Song and M. -W. Koo, ""Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition,"" ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 12486-12490, doi: 10.1109/ICASSP48485.2024.10447681."
"Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals,","Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based “AI Listener” that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.","keywords: {Measurement;Deep learning;Correlation coefficient;Neural engineering;Cost function;Brain-computer interfaces;Decoding;Brain Computer Interface (BCI);Automatic Speech Recognition (ASR);Speech Intelligibility;Speech Synthesis;Performance Metrics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10123751&isnumber=10123710,"S. Varshney, D. Farias, D. M. Brandman, S. D. Stavisky and L. M. Miller, ""Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals,"" 2023 11th International IEEE/EMBS Conference on Neural Engineering (NER), Baltimore, MD, USA, 2023, pp. 1-6, doi: 10.1109/NER52421.2023.10123751."
"Contemporary speech/speaker recognition with speech from impaired vocal apparatus,","Speech is the effective form of communication between human and its environment. Speech also has potential of being important mode of interaction with computer. This review paper deals with both speech and speaker recognition of persons with speech motor disorders. Normally speaker recognition consists of speaker verification and speaker identification. Speaker identification is the process of determining which registered speaker provides a given input sample. Speaker verification is the process of accepting or rejecting the identity claim of a speaker. On the other hand, the speech recognition system deals with the following challenges such as speech representation, feature extraction techniques, speech classifiers, databases and performance evaluation. Motor speech disorders are a class of speech disorder that disturbs the body's natural ability to speak. These disturbances vary in their etiology based on the integrity and integration of cognitive, neuromuscular, and musculoskeletal activities. There are various types of speech disorders like Apraxia, Cluttering (similar to stuttering), Dyspraxia, Dysarthria, Dysprosody and so on. The main objective of this review paper is to summarize and compare the well known methods used in various stages of speech and speaker recognition system.","keywords: {Speech;Hidden Markov models;Speech recognition;Databases;Speaker recognition;Feature extraction;Markov processes;Speech recognition;Speaker recognition;Speech motor disorders;Feature extraction;Speech classifiers},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7062754&isnumber=7062711,"S. Selva Nidhyananthan, R. Shantha Selvakumari and V. Shenbagalakshmi, ""Contemporary speech/speaker recognition with speech from impaired vocal apparatus,"" 2014 International Conference on Communication and Network Technologies, Sivakasi, India, 2014, pp. 198-202, doi: 10.1109/CNT.2014.7062754."
"Speech training system based on resonant frequencies of vocal tract,","Speech sounds are air pressure vibrations produced by air exhaled from the lungs and modulated and shaped by the vibrations of the glottal cords and the vocal tract as it is pushed out through the lips and nose. Speech signals, in addition to communicating the linguistic information, convey a multitude of other information including gender, age, accent, intent, emotion, humor and the state of health of the speaker. There are several neurological (e.g., aphasia, dysarthria, and apraxia) or anatomical (e.g., cleft lip and palate) factors that could affect the intelligibility and audibility of the human speech. In this paper we present our work in developing a training system based on the resonant frequencies (widely known as formants) of the vocal tract to help a human subject with speech impairment train himself or herself to improve the intelligibility and audibility of his or her speech.","keywords: {Speech;Cepstrum;Discrete Fourier transforms;Training;Speech processing;Resonant frequency;formants;impaired speech;speech training system;vocal tract resonse;cepstrum},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5745902&isnumber=5745722,"V. S. Selvam, V. Thulasibai and R. Rohini, ""Speech training system based on resonant frequencies of vocal tract,"" 13th International Conference on Advanced Communication Technology (ICACT2011), Gangwon, Korea (South), 2011, pp. 674-679."
"Analysis of adverse effects of stimulation during DBS surgery by patient-specific FEM simulations,","Deep brain stimulation (DBS) represents today a well-established treatment for movement disorders. Nevertheless the exact mechanism of action of DBS remains incompletely known. During surgery, numerous stimulation tests are frequently performed in order to evaluate therapeutic and adverse effects before choosing the optimal implantation site for the DBS lead. Anatomical structures responsible for the induced adverse effects have been investigated previously, but only based on stimulation data obtained with the implanted DBS lead. The present study introduces a methodology to identify these anatomical structures during intraoperative stimulation tests based on patient-specific electric field simulations and visualization on the patient specific anatomy. The application to 4 patients undergoing DBS surgery and presenting dysarthria, paresthesia or pyramidal effects shows the different anatomical structures, which might be responsible for the adverse effects. Several of the identified structures have been previously described in the literature. To draw any statistically significant conclusions, the methodology has to be applied to further patients. Together with the visualization of the therapeutic effects, this new approach could assist the neurosurgeons in the future in choosing the optimal implant position.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512796&isnumber=8512178,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512796&isnumber=8512178,"A. A. Shah et al., ""Analysis of adverse effects of stimulation during DBS surgery by patient-specific FEM simulations,"" 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, USA, 2018, pp. 2222-2225, doi: 10.1109/EMBC.2018.8512796."
"Targeting Friedreich Ataxia: A Sustainable Path to Safer and Smarter Therapeutics Through Integrated Docking and Toxicology,","Friedreich's ataxia (FA) is an autosomal recessive disorder affecting the nervous and cardiovascular systems, characterized by progressive ataxia, dysarthria, and muscle weakness. It results from a GAA trinucleotide repeat expansion in the FXN gene, leading to epigenetic silencing via heterochromatin formation, which reduces frataxin production-a mitochondrial protein essential for iron-sulphur cluster biogenesis and cellular energy production. Frataxin deficiency causes oxidative stress and mitochondrial dysfunction. Current treatments offer limited effectiveness, primarily addressing symptoms. Our study aims to develop novel therapeutic candidates by leveraging computational protein-ligand docking through the Galaxy EU platform, targeting frataxin deficiencies. High-throughput docking and toxicology studies have identified several promising compounds with potential therapeutic efficacy, offering hope for more effective and sustainable treatments for FA.","keywords: {Proteins;Cardiovascular system;Toxicology;Gallium arsenide;Production;Muscles;Epigenetics;Compounds;Information technology;Stress},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10816941&isnumber=10816707,"H. B. Attel, R. P, A. K, B. S, Shivandappa and S. Manokaran, ""Targeting Friedreich Ataxia: A Sustainable Path to Safer and Smarter Therapeutics Through Integrated Docking and Toxicology,"" 2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS), Bengaluru, India, 2024, pp. 1-4, doi: 10.1109/CSITSS64042.2024.10816941."
"Check Your Audio Data: Nkululeko for Bias Detection,","We present a new release of the software tool Nkululeko. New additions enable users to automatically perform sanity checks, data cleaning, and bias detection in the data based on machine learning predictions. Two open-source databases from the medical domain are investigated: the Androids de-pression corpus and the UASpeech dysarthria corpus. Results show that both databases have some bias, but not in a severe manner.","keywords: {Databases;Machine learning;Cleaning;Software tools;open-source tool;machine learning;bias detection;speaker characteristics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800580&isnumber=10799972,"F. Burkhardt, B. T. Atmaja, A. Derington, F. Eyben and B. Schuller, ""Check Your Audio Data: Nkululeko for Bias Detection,"" 2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Hsinchu City, Taiwan, 2024, pp. 1-6, doi: 10.1109/O-COCOSDA64382.2024.10800580."
"Extraction of patients subpopulations with psychiatric symptoms using a transformer architecture,","In this paper, we demonstrate a novel pipeline for identifying and extracting patient subpopulations from unstructured physician’s notes. We validate the method by extracting patients with psychiatric issues from a general patient population. This method first uses a clinical metathesaurus to select terms of interest from reports, then vectorizes the terms using a transformer model. These vectors’ dimensions are reduced using Uniform Manifold Approximation and Projection (UMAP), and the results grouped by optimal cluster selection methods. We demonstrate this technique on a freely-available collection of deidentified patient notes (MIMIC IV), extracting and clustering “mental or behavioral dysfunctions”. Our results show that it is possible to select user-defined groups of patients from unstructured text with minimal model oversight to group patients with similar profiles. In our study cohort, the models automatically segmented the patients into two groups: patients with more physical symptoms (alcohol/drug abuse, dysarthria, tongue-biting, eating disorders) and patients with mental/emotional symptoms. By detecting the underlying similarities in patient profiles, we believe this method can be utilized for symptom prediction tasks, as well as curating treatment plans based on their cluster profiles. Such a system can assist in clinical decision-making without the need for individually-created NLP models.","keywords: {Training;Manifolds;Dimensionality reduction;Pipelines;MIMICs;Eating disorders;Transformers;Data mining;Reliability;Engineering in medicine and biology;NLP;transformers;EHR extraction;clustering;dimensionality reduction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10781648&isnumber=10781494,"B. Holmes, M. Raymer and T. Banerjee, ""Extraction of patients subpopulations with psychiatric symptoms using a transformer architecture,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10781648."
"Signal Recognition for Parkinson's Disease Diagnosis Based on Multi-Source Deep Domain Generalization,","Parkinson's disease (PD) is an incurable neurodegenerative disorder. Dysarthria, stemming from PD, in speech offers an accessible and non-invasive diagnostic indicator. However, PD speech data, with its limited sample size and high aliasing, presents challenges for machine learning. Thought multi-source deep transfer learning methods show promise in PD speech recognition, they typically rely on labelled target domain data. To overcome this limitation, our study proposes an unsupervised transfer learning approach, namely multi-source deep domain generalization (MDDG). MDDG comprises four key modules: feature extraction, inter-domain and classes adversarial learning, and classifier training solely on multisource datasets. Leveraging adversarial networks, MDDG extracts invariant features from source domains, minimizing both inter-domain distribution discrepancies and intra-class difference while maximizing the distance between different classes from multi-source domains. Experimental results demonstrate the superiority of the MDDG over recent methodologies, where MDDG achieves remarkable metrics: highest accuracy (75.80%), precision (66.48%), and specificity (87.61%), all with the lowest standard error, underscoring the effectiveness and stability of MDDG offering valuable support to medical professionals in efficient PD diagnosis and monitoring.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10830707&isnumber=10830612,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10830707&isnumber=10830612,"L. Li, Y. Rao, H. Li and Y. Liu, ""Signal Recognition for Parkinson's Disease Diagnosis Based on Multi-Source Deep Domain Generalization,"" BIBE 2024; The 7th International Conference on Biological Information and Biomedical Engineering, Hohhot, China, 2024, pp. 21-26."
"Notice of Violation of IEEE Publication Principles: A voice-input voice-output communication assists in favor of people with harsh verbal communication destruction,","An actual VOICE INPUT VOICE OUTPUT Correspondence Help identifies the actual disordered talk from the person as well as develops communications that are changed into artificial talk. Tests demonstrated this technique works within producing great acknowledgement overall performance (mean precision 98 percentage) upon extremely disordered talk, even if acknowledgement perplexity is actually elevated. The actual VOICE INPUT VOICE OUTPUT Communication Assists (VIVOCA) had been examined inside an area test through people with reasonable in order to serious dysarthria as well as verified that they'll utilize the gadget to create intelligible talk result through disordered talk enter. The actual test outlined a few problems that restrict the actual overall performance as well as user friendliness from the gadget whenever used within actual utilization circumstances, along with imply acknowledgement precision associated with 85 percentages within these types of conditions. Once after receiving the clear speech in English as input, the same can be translated to any other language as an output, with the same meaning as it is in input. These types of restrictions are going to be tackled within long term function.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7034078&isnumber=7033740,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7034078&isnumber=7033740,"M. Babu and V. D. Kumar, ""Notice of Violation of IEEE Publication Principles: A voice-input voice-output communication assists in favor of people with harsh verbal communication destruction,"" International Conference on Information Communication and Embedded Systems (ICICES2014), Chennai, India, 2014, pp. 1-6, doi: 10.1109/ICICES.2014.7034078."
"A Wearable System for Monitoring Neurological Disorder Events with Multi-Class Classification Model in Daily Life,","Dysphagia and dysarthria are the prominent sequelae of neurological disorders. Treatment and rehabilitation of these impairments necessitate continuously monitoring symptoms related to swallowing and speaking. However, current medical technologies require large and diverse equipment to record these symptoms, which are predominantly limited to clinical environments. In this study, we propose an innovative wearable system for distinguishing neurological disorder events using a mechano-acoustic (MA) sensor and multi-class ensemble classification model. The MA sensor exhibits a high sensitivity to neck vibration without any interference from ambient sounds. A multi-class classification model was also developed to discern the symptoms from the recorded signals accurately. The proposed classification model is an ensemble neural network trained on waveforms and mel spectrograms. As a result, we achieve a high classification accuracy of 91.94%, surpassing the performance of previous single neural networks.","keywords: {Neurological diseases;Vibrations;Accuracy;Time series analysis;Vibration measurement;Neck;Biomedical monitoring;Monitoring;Biological neural networks;Spectrogram;Neurological disorder symptoms;wearable monitoring system;mechano-acoustic sensor;multi-class classification model},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10782047&isnumber=10781494,"Y. Song, I. Yun, S. Giovanoli, C. A. Easthope and Y. Chung, ""A Wearable System for Monitoring Neurological Disorder Events with Multi-Class Classification Model in Daily Life,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10782047."
"Tongue-Trackpad: Tongue Rehabilitation Through Quantitative Personalized Visual-Feedback Intervention,","Effective rehabilitation of tongue movement relies on delivering personalized interventions and quantitatively assessing the intervention's impact. Current approaches primarily focus on enhancing tongue movement control and strength through oromotor exercises and audio feedback. Here, we introduce the Tongue- Trackpad a novel solution for tongue movement rehabilitation and progress tracking. The Tongue- Trackpad is a wireless intra-oral device that provides real-time visual feedback of tongue movement while quantifying the movement, thus enabling personalized interventions. In this preliminary feasibility study, a stroke survivor diag-nosed with dysarthria participated in eighteen sessions of per-sonalized visual-feedback pursuit intervention using the Tongue-Trackpad. The intervention was customized to the participant's tongue movement deficit areas identified during the initial evaluation. Despite the participant's severe speech disorder, characterized by a Diadochokinetic rate of zero, the results indicated a modest positive change in tongue movement both within a single session and over the intervention period. The results showed an average increase of 3.5 $\pm 4.7{\%}$ in coverage area, a reduction of 1.5 $\pm 1.9{\%}$ in the deficit area, and a 1.7 $\pm$ 3.1 % reduction in the excess area within a single session. Similar modest trends were observed throughout the entire intervention period. Although further studies with more participants are needed for robust conclusions, this preliminary feasibility study suggests provided preliminary that providing personalized visual-feedback intervention using the Tongue- Trackpad holds promise for tongue movement rehabilitation.","keywords: {Wireless communication;Visualization;Body sensor networks;Tongue;Tracking;Market research;Real-time systems;Intra-Oral Technology;Tongue Rehabilitation;Personalized Intervention;Tongue Movement;Visual-Feedback},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10780592&isnumber=10780450,"A. Scarpellini, A. R. Carroll, E. M. Babbitt, J. Patton and H. Esmailbeigi, ""Tongue-Trackpad: Tongue Rehabilitation Through Quantitative Personalized Visual-Feedback Intervention,"" 2024 IEEE 20th International Conference on Body Sensor Networks (BSN), Chicago, IL, USA, 2024, pp. 1-4, doi: 10.1109/BSN63547.2024.10780592."
"Classifying Speech Disorders Using Voice Signals and Machine Learning,","This study explores the application of machine learning (ML) and advanced voice signal analysis to classify speech disorders, including vocal tremors, dysarthria, and stuttering. These disorders pose significant challenges to communication and quality of life, requiring precise and reliable diagnostic tools. To address the inherent challenges of limited data availability in this domain, the study employs synthetic voice data generated using mathematical models to augment real-world recordings. This hybrid approach ensures a more comprehensive dataset, enabling robust training and evaluation of machine learning models.Key machine learning algorithms such as Support Vector Machines (SVMs), Random Forest, and Gradient Boosting are utilized to extract and analyze a wide range of acoustic features from speech samples. These methods are selected for their ability to handle complex, nonlinear patterns and to identify subtle distinctions between normal and disordered speech. By leveraging these techniques, the study investigates their performance in accurately classifying speech disorders, emphasizing their potential in improving diagnostic outcomes.The findings highlight the transformative potential of machine learning in the field of speech pathology. ML models demonstrated notable improvements in the precision, efficiency, and consistency of diagnosing speech disorders compared to traditional methods. This breakthrough not only enhances diagnostic reliability but also equips clinicians with objective tools that reduce subjectivity in assessments. Moreover, the integration of these technologies promotes earlier detection of disorders, enabling timely and tailored therapeutic interventions.By facilitating more accurate and accessible diagnostic practices, this work paves the way for significant advancements in patient care. The use of ML-powered tools can bridge the gap between clinical expertise and technological innovation, empowering healthcare professionals to offer personalized treatment plans. Ultimately, this approach has the potential to improve patient outcomes, reduce the stigma associated with speech disorders, and ensure more equitable access to high-quality care for individuals worldwide.","keywords: {Training;Support vector machines;Pathology;Accuracy;Machine learning algorithms;Boosting;Real-time systems;Mathematical models;Random forests;Synthetic data;machine learning;speech disorders;acoustic features;classification;synthetic data;diagnosis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10940483&isnumber=10940074,"D. Al Masri, A. Yousef, L. Turkistani, T. Tadmori, E. Barkat and N. Kabbaj, ""Classifying Speech Disorders Using Voice Signals and Machine Learning,"" 2025 22nd International Learning and Technology Conference (L&T), jeddah, Saudi Arabia, 2025, pp. 349-353, doi: 10.1109/LT64002.2025.10940483."
"Speech Formation Objectivisation Methods Development and Analysis for Correct Phonation Diagnostics,","In connection with various infringements speech production people have requirement for development of methods of an objective estimation of the vocal apparatus functional condition. Methods should be directed on revealing of the attributes forming articulation-acoustic characteristics of correct harmonization of themes. The given problem is especially important for children for whom increase speech pathologies (legasthenia, dysgraphia, dysarthria, phonasthenia, hoarseness, etc.) is observed. As results of the carried out research, it is possible to come to a conclusion that for the analysis of voice pathologies it is expedient to use the offered method.","keywords: {Speech analysis;Frequency;Pathology;Educational institutions;State estimation;Appraisal;Parameter estimation;Cities and towns;Microphones;Passband},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4292395&isnumber=4292363,"O. G. Fetisova, D. V. Lamtyugin, V. K. Makukha, V. T. Maslov and E. M. Voronin, ""Speech Formation Objectivisation Methods Development and Analysis for Correct Phonation Diagnostics,"" 2006 8th International Conference on Actual Problems of Electronic Instrument Engineering, Novosibirsk, Russia, 2006, pp. 123-123, doi: 10.1109/APEIE.2006.4292395."
"Synthesizing Dysarthric Speech Using Multi-Speaker Tts For Dysarthric Speech Recognition,","Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems may help dysarthric talkers communicate more effectively. To have robust dysarthria-specific ASR, sufficient training speech is required, which is not readily available. Recent advances in Text-To-Speech (TTS) synthesis multi-speaker end-to-end systems suggest the possibility of using synthesis for data augmentation. In this paper, we aim to improve multi-speaker end-to-end TTS systems to synthesize dysarthric speech for improved training of a dysarthria-specific DNN-HMM ASR. In the synthesized speech, we add dysarthria severity level and pause insertion mechanisms to other control parameters such as pitch, energy, and duration. Results show that a DNN-HMM model trained on additional synthetic dysarthric speech achieves WER improvement of 12.2% compared to the baseline, the addition of the severity level and pause insertion controls decrease WER by 6.5%, showing the effectiveness of adding these parameters. Audio samples are available at https://mohammadelc.github.io/SpeechGroupUKY/","keywords: {Training;Databases;Conferences;Web pages;Training data;Speech recognition;Production;Dysarthria;speech recognition;Speech-To-Text;Synthesized speech;Data augmentation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746585&isnumber=9746004,"M. Soleymanpour, M. T. Johnson, R. Soleymanpour and J. Berry, ""Synthesizing Dysarthric Speech Using Multi-Speaker Tts For Dysarthric Speech Recognition,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 7382-7386, doi: 10.1109/ICASSP43922.2022.9746585."
"The Change of Vocal Tract Length in People with Parkinson’s Disease,","Hypokinetic dysarthria is one of the early symptoms of Parkinson’s disease (PD) and has been proposed for early detection and also for monitoring of the progression of the disease. PD reduces the control of vocal tract muscles such as the tongue and lips and, therefore the length of the active vocal tract is altered. However, the change in the vocal tract length due to the disease has not been investigated. The aim of this study was to determine the difference in the apparent vocal tract length (AVTL) between people with PD and age-matched control healthy people. The phoneme, /a/ from the UCI Parkinson’s Disease Classification Dataset and the Italian Parkinson’s Voice and Speech Dataset were used and AVTL was calculated based on the first four formants of the sustained phoneme (F1-F4). The results show a correlation between Parkinson’s disease and an increase in vocal tract length. The most sensitive feature was the AVTL calculated using the first formants of sustained phonemes (F1). The other significant finding reported in this article is that the difference is significant and only appeared in the male participants. However, the size of the database is not sufficiently large to identify the possible confounding factors such as the severity and duration of the disease, medication, age, and comorbidity factors.Clinical relevance—The outcomes of this research have the potential to improve the identification of early Parkinsonian dysarthria and monitor PD progression.","keywords: {Tongue;Correlation;Databases;Lips;Muscles;Larynx;Biomedical monitoring},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10340263&isnumber=10339939,"N. D. Pah, M. A. Motin, G. C. Oliveira and D. K. Kumar, ""The Change of Vocal Tract Length in People with Parkinson’s Disease,"" 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Sydney, Australia, 2023, pp. 1-4, doi: 10.1109/EMBC40787.2023.10340263."
"Weak-Supervised Dysarthria-Invariant Features for Spoken Language Understanding Using an Fhvae and Adversarial Training,","The scarcity of training data and the large speaker variation in dysarthric speech lead to poor accuracy and poor speaker generalization of spoken language understanding systems for dysarthric speech. Through work on the speech features, we focus on improving the model generalization ability with limited dysarthric data. Factorized Hierarchical Variational Auto-Encoders (FHVAE) trained unsupervisedly have shown their advantage in disentangling content and speaker representations. Earlier work showed that the dysarthria shows in both feature vectors. Here, we add adversarial training to bridge the gap between the control and dysarthric speech data domains. We extract dysarthric and speaker invariant features using weak supervision. The extracted features are evaluated on a Spoken Language Understanding task and yield a higher accuracy on unseen speakers with more severe dysarthria compared to features from the basic FHVAE model or plain filterbanks.","keywords: {Training;Conferences;Training data;Filter banks;Speech recognition;Feature extraction;Generators;Dysarthric speech;FHVAE;adversarial training;weak supervision;end-to-end spoken language understanding},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10023085&isnumber=10022330,"J. Qi and H. Van hamme, ""Weak-Supervised Dysarthria-Invariant Features for Spoken Language Understanding Using an Fhvae and Adversarial Training,"" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 375-381, doi: 10.1109/SLT54892.2023.10023085."
"Dysarthric Speech Detection Using Hybrid Models,","Dysarthria is a speech disorder caused by weak or poorly coordinated speech-related muscles. Dysarthria can be caused by various factors such as stroke, multiple sclerosis, or cerebral palsy, as well as brain injury or certain medications. The ultimate goal of our paper is to make a suitable and accurate tool for detecting dysarthric speech that can be used in clinical settings for early diagnosis, treatment planning for dysarthric individuals. In this work, the machine learning models namely CNN (Convolutional Neural Network), and hybrid models, such as CNN, combined with LSTM (Long Short-Term Memory), and CNN combined with GRU (Gated Recurrent Unit) are trained on TORGO dataset, to classify the dysarthric speech from non-dysarthric speech. The model's performance is evaluated on the testing data and all of these models produce more than 95% accuracy rate. From this study, we understood that this methodology can be used in developing Automatic Speech Recognition(ASR) for people with dysarthria, which would become an essential technology for many applications.","keywords: {Voice activity detection;Multiple sclerosis;Computational modeling;Speech recognition;Muscles;Data models;Planning;Dysarthria;Speech;CNN;LSTM;GRU;ASR},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10308386&isnumber=10306339,"S. H. Fazil and S. D, ""Dysarthric Speech Detection Using Hybrid Models,"" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-5, doi: 10.1109/ICCCNT56998.2023.10308386."
"Towards Improving the Performance of Dysarthric Speech Severity Assessment System,","Dysarthria is a kind of speech disorder caused by difficulties in muscular control of the speech mechanism, owing to the impairment of the central or peripheral nervous systems. Dysarthria is a speaking impairment due to the weak muscles of the patient due to brain injury. Face, throat muscles, lips are important to generate intelligible speech in humans. The severity of the disorder decides the understandability of the dysarthric speech, as the speech can be slow and unclear. This affects dysarthric people's life. This creates a communication gap between the common people and dysarthric patients. This work proposes a prototype that will improve the intelligibility of dysarthric speech. The main objective of the prototype is to enhance the rehabilitation of dysarthric people's livelihood. From the severity assessment, the clinician and caretakers can provide an improved speech treatment for dysarthric patients. Audio features like MFCC (Mel Frequency Spectral Coefficients), ZCR (Zero Crossing Rate), Spectral centroid, spectral rolloff, Mel spectrogram are used for feature extraction process. The KNN (K-nearest neighbor) and SVM (Support Vector Machine) classifiers are used to categorize the severity level of dysarthria based upon the features obtained.","keywords: {Support vector machines;Peripheral nervous system;Lips;Prototypes;Muscles;Feature extraction;Mel frequency cepstral coefficient;Dysarthria;MFCC;ZCR;Spectral centroid;feature extraction;SVM;KNN;speech therapy;speech severity assessment},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9740812&isnumber=9740653,"A. B. B, S. A. Kumar, K. T, S. Sasikala and K. P. C. V, ""Towards Improving the Performance of Dysarthric Speech Severity Assessment System,"" 2022 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 2022, pp. 1-6, doi: 10.1109/ICCCI54379.2022.9740812."
"Effects of acoustic features modifications on the perception of dysarthric speech — Preliminary study (Pitch, intensity and duration modifications),","Marking stress is important in conveying meaning and drawing listener's attention to specific parts of a message. Extensive research has shown that healthy speakers mark stress using three main acoustic cues; pitch, intensity, and duration. The relationship between acoustic and perception cues is vital in the development of a computer-based tool that aids the therapists in providing effective treatment to people with Dysarthria. It is, therefore, important to investigate the acoustic cues deficiency in dysarthric speech and the potential compensatory techniques needed for effective treatment. In this paper, the relationship between acoustic and perceptive cues in dysarthric speech are investigated. This is achieved by modifying stress marked sentences from 10 speakers with Ataxic dysarthria. Each speaker produced 30 sentences using the 10 SubjectVerb-Object-Adjective (SVOA) structured sentences across three stress conditions. These stress conditions are stress on the initial (S), medial (O) and final (A) target words respectively. To effectively measure the deficiencies in Dysarthria speech, the acoustic features (pitch, intensity, and duration) are modified incrementally. The paper presents the techniques involved in the modification of these acoustic features. The effects of these modifications are analysed based on steps of 25% increments in pitch, intensity and duration. For robustness and validation, 50 untrained listeners participated in the listening experiment. The results and the relationship between acoustic modifications (what is measured) and perception (what is heard) in Dysarthric speech are discussed.","keywords: {Stress marking;perception;dysarthria;acoustics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8361566&isnumber=8329307,"T. B. Ijitona, J. J. Soraghan, A. Lowit, G. Di-Caterina and H. Yue, ""Effects of acoustic features modifications on the perception of dysarthric speech — Preliminary study (Pitch, intensity and duration modifications),"" IET 3rd International Conference on Intelligent Signal Processing (ISP 2017), London, 2017, pp. 1-6, doi: 10.1049/cp.2017.0363."
"FBSE-FTFCWT-Based Novel Automated Framework for Dysarthric Speech Detection,","Neurological injuries or neurodegenerative diseases can lead to dysarthria, a condition that impairs speech intelligibility. Accurate detection of dysarthria and its severity from speech signals are crucial for advancing smart healthcare solutions. This study presents an automated system for dysarthria detection and severity classification, using a Fourier-Bessel series expansion-based flexible time-frequency coverage wavelet transform (FBSE-FTFCWT) and an autoencoder. Initially, FBSE-FTFCWT decomposes the speech signal into 16 sub-band signals, which are used as an input in the form of tensor for autoencoders to generate latent representation. This latent representation is subsequently used for dysarthric speech and its severity level detection. The proposed framework outperformed the current state-of-the-art in classifying dysarthric and normal speech on the UA-speech dataset, achieving 3.28% higher accuracy. Additionally, for the dysarthric severity detection task using speech signals from the same dataset, it showed 3.1% improvement in accuracy over existing methods.","keywords: {Wavelet transforms;Voice activity detection;Time-frequency analysis;Accuracy;Tensors;Autoencoders;Medical services;Signal processing;Feature extraction;Injuries;Autoencoder;Dysarthria;FBSE-FTFCWT;Severity-level detection;UA-speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10889688&isnumber=10887541,"A. Vijay, R. B. Pachori, B. Appina and N. Tiwari, ""FBSE-FTFCWT-Based Novel Automated Framework for Dysarthric Speech Detection,"" ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025, pp. 1-5, doi: 10.1109/ICASSP49660.2025.10889688."
"Study of the Automatic Detection of Parkison’s Disease Based on Speaker Recognition Technologies and Allophonic Distillation,","The use of new tools to detect Parkinson's Disease (PD) from speech articulatory movements can have a considerable impact in the diagnosis of patients. In this study, a novel approach involving speaker recognition techniques with allophonic distillation is proposed and tested separately in four parkinsonian speech databases (205 patients and 186 controls in total). This new scheme provides values between 72% and 94% of accuracy in the automatic detection of PD, depending on the database, and improvements up to 9% respect to baseline techniques. Results not only point towards the importance of the segmentation of the speech for the differentiation of parkinsonian and control speakers but confirm previous findings about the relevance of plosives and fricatives in the detection of parkinsonian dysarthria.","keywords: {Databases;Liquids;Diseases;Task analysis;Acoustics;Speech recognition;Speaker recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512562&isnumber=8512178,"L. Moro-Velazquez et al., ""Study of the Automatic Detection of Parkison’s Disease Based on Speaker Recognition Technologies and Allophonic Distillation,"" 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, USA, 2018, pp. 1404-1407, doi: 10.1109/EMBC.2018.8512562."
"Monitoring the Effect of Levodopa Using Sustained Phonemes in Parkinson’s Disease Patients,","Parkinson's disease (PD) is a neurological disease identified by multiple symptoms, and levodopa is one of the most effective medications for treating the disease. To determine the dosage of levodopa, it is necessary to meet on a regular basis and observe motor function. The early detection and progression of the disease have been proposed using hypokinetic dysarthria. However, previous studies have not examined the effects of levodopa on speech rigorously and have provided inconsistent results. In this study, three sustained phonemes of PD patients were investigated for the effect of medication. A set of features characterizing vocal fold dynamics as well as the vocal tract coordinators were extracted from the sustained phonemes /of 28 PD patients during levodopa medication off and on states. All the features were statistically investigated and classified using a linear discriminant analysis (LDA) classifier. LDA classifier identified medication on from medication off based on the combined features from phoneme /a/, /o/ and /m/ with the accuracy=82.75% and F1-score=82.18%. Voice recording of PD patients during sustained phonemes /a/, /o/ and /m/ has the potential for identifying whether the patients are in On state or Off state of medication.Clinical Relevance— The outcomes of this study have the potential to monitor the effect and progress of levodopa on PD patients.","keywords: {Neurological diseases;Parkinson's disease;Cepstral analysis;Feature extraction;Biology;Recording;Linear discriminant analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10340507&isnumber=10339939,"M. A. Motin, N. D. Pah and D. K. Kumar, ""Monitoring the Effect of Levodopa Using Sustained Phonemes in Parkinson’s Disease Patients,"" 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Sydney, Australia, 2023, pp. 1-4, doi: 10.1109/EMBC40787.2023.10340507."
"Adapting acoustic and lexical models to dysarthric speech,","Dysarthria is a motor speech disorder resulting from neurological damage to the part of the brain that controls the physical production of speech. It is, in part, characterized by pronunciation errors that include deletions, substitutions, insertions, and distortions of phonemes. These errors follow consistent intra-speaker patterns that we exploit through acoustic and lexical model adaptation to improve automatic speech recognition (ASR) on dysarthric speech. We show that acoustic model adaptation yields an average relative word error rate (WER) reduction of 36.99% and that pronunciation lexicon adaptation (PLA) further reduces the relative WER by an average of 8.29% on a large vocabulary task of over 1500 words for six speakers with severe to moderate dysarthria. PLA also shows an average relative WER reduction of 7.11% on speaker-dependent models evaluated using 5-fold cross-validation.","keywords: {Speech;Speech recognition;Adaptation models;Data models;Acoustics;Hidden Markov models;Databases;dysarthria;dysarthric speech;pronunciation lexicon adaptation;speech recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947460&isnumber=5946226,"K. T. Mengistu and F. Rudzicz, ""Adapting acoustic and lexical models to dysarthric speech,"" 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Prague, Czech Republic, 2011, pp. 4924-4927, doi: 10.1109/ICASSP.2011.5947460."
"Improving the intelligibility of dysarthric speech towards enhancing the effectiveness of speech therapy,","Dysarthria is a neuro-motor disorder in which the muscles used for speech production and articulation are severely affected. Dysarthric patients are characterized by slow or slurred speech that is difficult to understand. This work aims at enhancing the intelligibility of dysarthric speech towards developing an effective speech therapy tool. In this therapy tool, enhanced speech is used for providing auditory feedback with a delay to instill confidence in the patients, so that they can improve their speech intelligibility gradually through relearning. Feature level transformation techniques based on linear predictive coding (LPC) coefficient mapping and frequency warping of LPC poles are experimented in this work. Speech utterances from Nemours dataset with mild and moderate dysarthria are used to study the effectiveness of the proposed algorithms. The quality of the transformed speech is evaluated using subjective and objective measures. A significant improvement in the intelligibility of speech was observed. Our method henceforth could be used to enhance the effectiveness of speech therapy, by encouraging the dysarthric patients talk more, thus helping in their fast rehabilitation.","keywords: {Speech;Speech enhancement;Databases;Production;Medical treatment;Linear predictive coding;Error analysis;Dysarthria;intelligibility;speech enhancement;speech therapy;delayed auditory feedback;Linear prediction coefficients;dynamic time warping;frequency warping},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7732175&isnumber=7732013,"S. A. Kumar and C. S. Kumar, ""Improving the intelligibility of dysarthric speech towards enhancing the effectiveness of speech therapy,"" 2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI), Jaipur, India, 2016, pp. 1000-1005, doi: 10.1109/ICACCI.2016.7732175."
"A Novel Approach for Intelligibility Assessment in Dysarthric Subjects,","Dysarthria is a motor speech impairment caused by muscle weakness. Individuals, with this condition, are unable to control rapid movement of the velum leading to reduction in intelligibility, audibility, naturalness and efficiency of vocal communication. Systems that can assess intelligibility of dysarthric speech can help clinicians diagnose the impact of therapy and medication. In the paper, we propose a usable novel method to assess intelligibility of dysarthric speakers. The approach is based on the observation that the performance of a speech recognition engine deteriorates with increase in severity of the disorder. The mismatch between the original word and the recognized string is exploited to compute the dysarthria intelligibility score. Experiments on UA speech corpus show that the computed intelligibility score exhibits a significant correlation with perceptually assessed intelligibility scores. We further show that a small set of words spoken by the dysarthric subject is sufficient to assess the speech intelligibility reliably.","keywords: {Medical treatment;Speech recognition;Signal processing;Muscles;Reliability;Speech processing;Medical diagnostic imaging;Dysarthria;intelligibility assessment;diagnosis;deepspeech;ASR},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9053339&isnumber=9052899,"A. Tripathi, S. Bhosale and S. K. Kopparapu, ""A Novel Approach for Intelligibility Assessment in Dysarthric Subjects,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 6779-6783, doi: 10.1109/ICASSP40776.2020.9053339."
"Intelligibility modification of dysarthric speech using HMM-based adaptive synthesis system,","Dysarthria is a manifestation of an inability to control and coordinate on one or more articulatory subsystems, which results in poorly articulated, slurred, and unintelligible speech. In order to enable a dysarthric speaker to communicate more efficiently with others, a text-to-speech synthesis system that generates speech in his voice, but without the errors he makes would be desirable. In this regard, the current work proposes a system, where the dysarthric speech is first recognized by an HMM-based speech recognition system. A sentence-level network is used to ensure 100% recognition accuracy. The recognized text is then synthesized by a speech synthesis system adapted to the dysarthric speaker's voice. This system replaces the sound units wrongly uttered by the dysarthric speaker, thereby improving intelligibility. The rate of synthesized speech is quite low for speakers with moderate and severe dysarthria. Therefore, the speech rate is modified using time-domain pitch synchronous overlap add (TD-PSOLA) technique. Degradation mean opinion score (DMOS) is used to prove that wrongly uttered sound units are replaced by correct sound units and that the synthetic speech is made more intelligible with the speaker's identity.","keywords: {Speech;Hidden Markov models;Speech recognition;Databases;Adaptation models;Speech synthesis;Acoustics;Dysarthria;perceptual analysis;hidden Markov model (HMM);speech recognition and synthesis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7235130&isnumber=7235123,"M. Dhanalakshmi and P. Vijayalakshmi, ""Intelligibility modification of dysarthric speech using HMM-based adaptive synthesis system,"" 2015 2nd International Conference on Biomedical Engineering (ICoBE), Penang, Malaysia, 2015, pp. 1-5, doi: 10.1109/ICoBE.2015.7235130."
"Enhancement of dysarthric speech for developing an effective speech therapy tool,","Dysarthria is a neuromuscular disorder that results from weakened movement of muscles used in speech production. This results in poor articulation causing the dysarthric speech to be slurred and difficult to understand. The natural auditory feedback makes the patients understand that their speech is of low quality, and this lowers their self confidence and they become more and more introverted causing the disorder to aggravate. In this work, we enhance the dysarthric speech and provide the enhanced speech to the patient through auditory feedback. This helps the patients to feel comfortable with their speech and gradually develop confidence to speak more and hence achieve a speedy rehabilitation. The utterances are analyzed using linear predictive coding (LPC). The LPC features in the acoustic space of dysarthric speaker are mapped to the feature space of the normative population using constrained maximum likelihood linear regression (CMLLR) before they are used for re-synthesising the enhanced speech. We then evaluated the quality of the enhanced speech using subjective and objective measures, DMOS and PESQ, and obtained an improvement of 63% and 43.4% for DMOS and PESQ measures respectively. Clinical trials are being pursued at Amrita Institute of Medical Sciences, Kochi on patients with dysarthria to evaluate the effectiveness of the proposed approach for faster rehabilitation of the patients. The results of these clinical trials will be reported in due course of time.","keywords: {Speech;Speech enhancement;Tools;Medical treatment;Maximum likelihood linear regression;Conferences;Production;Dysarthria;feature space mapping;constrained maximum likelihood linear regression;instantaneous auditory feedback;gaussian mixture mapping},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8300222&isnumber=8299705,"S. Sivaram, C. S. Kumar and A. A. Kumar, ""Enhancement of dysarthric speech for developing an effective speech therapy tool,"" 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET), Chennai, India, 2017, pp. 2548-2551, doi: 10.1109/WiSPNET.2017.8300222."
"Cochlear Filter-Based Cepstral Features for Dysarthric Severity-Level Classification,","Severity-level classification of dysarthria helps in diagnosing a patient and choosing an appropriate course of treatment. This would also aid in redirecting the speech to an appropriate dysarthric Automatic Speech Recognition (ASR), as traditional ASR does not perform well on dysarthric speech. In the recent past, several approaches have been used to study the severity-level classification of dysarthria using state-of-the-art features, such as Short-Time Fourier Transform (STFT) and Mel Frequency Cepstral Coefficients (MFCC). This study investigates novel auditory transform-based Cochlear Filter Cepstral Coefficients (CFCC) features for dysarthric severity-level classification. Three DNN-based classifiers, namely, Convolutional Neural Network (CNN), Light-CNN (LCNN), and Residual Neural Network (ResNet) were employed on UA-Speech Corpus and TORGO corpus. Our proposed CFCC feature set yields an improved classification accuracy of 97.46% (98.99%), 94.92% (94.97%), and 96.66% (98.93%) on UA (Torgo)-corpus using CNN, LCNN and ResNet classifiers respectively. Furthermore, performance metrics, such as the Jaccard index, Matthew's Correlation Coefficient (MCC), $F$1-score, and Hamming loss are used to examine feature discrimination power of CFCC. Finally, latency period of CFCC was also analysed for practical deployment of system.","keywords: {Measurement;Correlation coefficient;Fourier transforms;Europe;Signal processing;Convolutional neural networks;Indexes;Dysarthria;UA-Speech Corpus;TORGO Corpus;CFCC;LFCC;MFCC;CNN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10289888&isnumber=10289713,"S. Rathod, P. Gupta, A. Kachhi and H. A. Patil, ""Cochlear Filter-Based Cepstral Features for Dysarthric Severity-Level Classification,"" 2023 31st European Signal Processing Conference (EUSIPCO), Helsinki, Finland, 2023, pp. 1095-1099, doi: 10.23919/EUSIPCO58844.2023.10289888."
"Dysarthric Speech Recognition using Depthwise Separable Convolutions: Preliminary Study,","As a neurological disability that affects muscles involved in articulation, dysarthria is a speech impairment that leads to reduced speech intelligibility. In severe cases, these individuals could also be handicapped and unable to interact with digital devices. For such individuals, Automatic Speech Recognition (ASR) technologies could be life changing by enabling them to communicate with others as well as computing devices via voice commands. Nonetheless, ASR systems designed to recognize healthy speech have shown very poor performance to transcribe dysarthric speech, signaling the need to design ASR specifically tailored for dysarthria. Dysarthric Speech Recognition (DRS) research has progressed gradually because of the challenges the research community faces such as the scarcity of dysarthric speech that does not allow the researchers to design deeper acoustic models needed to better learn dysarthric speech variations. In this paper we report on our preliminary findings to improve our previous DSR called Speech Vision and study the effects of Separable Convolutional neurons to improve its acoustic model. Speech Vision is a novel Dysarthric Speech Recognition system that learns to recognize the shape of the words uttered by dysarthric speakers instead of recognizing phone sequences and then mapping them to words. Experiments conducted on the utterances provided by all UA-Speech dysarthric speakers indicate the proposed Depthwise separable architecture provided better word recognition accuracies compared to the original Speech Vision’s architecture across all dysarthric speech intelligibility classes.","keywords: {Performance evaluation;Convolution;Shape;Training data;Personal digital devices;Speech recognition;Computer architecture;dysarthria;dysarthric speech recognition;depthwise separable convolution;speech vision},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10314894&isnumber=10314866,"S. R. Shahamiri, K. Mandal and S. Sarkar, ""Dysarthric Speech Recognition using Depthwise Separable Convolutions: Preliminary Study,"" 2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD), Bucharest, Romania, 2023, pp. 78-82, doi: 10.1109/SpeD59241.2023.10314894."
"Feature selection in Parkinson's disease: A rough sets approach,","Parkinson's disease is a neurodegenerative disorder with a long time course and a significant prevalence, which increases significantly with age. Although the etiology is currently unknown, the disease presents with neurodegeneration of regions of the basal ganglia. the onset occurs later in life, and the disease progresses slowly. The disease is diagnosed clinically, requiring the identification of several factors such as distal resting tremor, rigidity, and bradykinesia. The common thread throughout the range of symptoms is motor dysfunction, and recent reports have focused on dysphonia, the impairment in voice production as a diagnostic measure. In this paper, a number of features associated with speech have been collected through clinical studies from both healthy and people with Parkinson's (PWP) and analysed in order to determine if one or more of them can be used to diagnose PWP. The feature set is analysed using the rough sets paradigm, which maps feature vectors associated with objects onto decision classes. The results from applying rough sets is a set of rules that map features via rules into a decision support system - performing classification of objects. the results FOM this study indicate that a subset of typical voice derived features is adequate to differentiate healthy from PWP with 100% accuracy. These result are important in that they imply that a diagnosis can be automated and performed remotely. This work will be extended to determine if this approach can be utilised with the same effectiveness for the diagnosis of parkinsonism disorders - a collection-diseases with Parkinson's like symptoms.","keywords: {Parkinson's disease;Rough sets;Mathematics;Computer science},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5352688&isnumber=5352681,"K. Revett, F. Gorunescu and A. -B. M. Salem, ""Feature selection in Parkinson's disease: A rough sets approach,"" 2009 International Multiconference on Computer Science and Information Technology, Mragowo, Poland, 2009, pp. 425-428, doi: 10.1109/IMCSIT.2009.5352688."
"Universal Paralinguistic Speech Representations Using self-Supervised Conformers,","Many speech applications require understanding aspects beyond the words being spoken, such as recognizing emotion, detecting whether the speaker is wearing a mask, or distinguishing real from synthetic speech. In this work, we introduce a new state-of-the-art paralinguistic representation derived from large-scale, fully self-supervised training of a 600M+ parameter Conformer-based architecture. We benchmark on a diverse set of speech tasks and demonstrate that simple linear classifiers trained on top of our time-averaged representation outperform nearly all previous results, in some cases by large margins. Our analyses of context-window size demonstrate that, surprisingly, 2 second context-windows achieve 96% the performance of the Conformers that use the full long-term context on 7 out of 9 tasks. Furthermore, while the best per-task representations are extracted internally in the network, stable performance across several layers allows a single universal representation to reach near optimal performance on all tasks.","keywords: {Training;Emotion recognition;Conferences;Speech recognition;Signal processing;Benchmark testing;Acoustics;speech;representation learning;self-supervised learning;paralinguistics;transformer},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747197&isnumber=9746004,"J. Shor, A. Jansen, W. Han, D. Park and Y. Zhang, ""Universal Paralinguistic Speech Representations Using self-Supervised Conformers,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 3169-3173, doi: 10.1109/ICASSP43922.2022.9747197."
"Feature extraction using pre-trained convolutive bottleneck nets for dysarthric speech recognition,","In this paper, we investigate the recognition of speech uttered by a person with an articulation disorder resulting from athetoid cerebral palsy based on a robust feature extraction method using pre-trained convolutive bottleneck networks (CBN). Generally speaking, the amount of speech data obtained from a person with an articulation disorder is limited because their burden is large due to strain on the speech muscles. Therefore, a trained CBN tends toward overfitting for a small corpus of training data. In our previous work, the experimental results showed speech recognition using features extracted from CBNs outperformed conventional features. However, the recognition accuracy strongly depends on the initial values of the convolution kernels. To prevent overfitting in the networks, we introduce in this paper a pre-training technique using a convolutional restricted Boltzmann machine (CRBM). Through word-recognition experiments, we confirmed its superiority in comparison to convolutional networks without pre-training.","keywords: {Feature extraction;Convolution;Speech;Speech recognition;Europe;Kernel;Articulation disorders;feature extraction;convolutional neural networks;bottleneck feature;convolutional restricted Boltzmann machine},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7362616&isnumber=7362087,"Y. Takashima, T. Nakashika, T. Takiguchi and Y. Ariki, ""Feature extraction using pre-trained convolutive bottleneck nets for dysarthric speech recognition,"" 2015 23rd European Signal Processing Conference (EUSIPCO), Nice, France, 2015, pp. 1411-1415, doi: 10.1109/EUSIPCO.2015.7362616."
"Modeling pathological speech perception from data with similarity labels,","The current state of the art in judging pathological speech intelligibility is subjective assessment performed by trained speech pathologists (SLP). These tests, however, are inconsistent, costly and, oftentimes suffer from poor intra- and inter-judge reliability. As such, consistent, reliable, and perceptually-relevant objective evaluations of pathological speech are critical. Here, we propose a data-driven approach to this problem. We propose new cost functions for examining data from a series of experiments, whereby we ask certified SLPs to rate pathological speech along the perceptual dimensions that contribute to decreased intelligibility. We consider qualitative feedback from SLPs in the form of comparisons similar to statements “Is Speaker A's rhythm more similar to Speaker B or Speaker C?” Data of this form is common in behavioral research, but is different from the traditional data structures expected in supervised (data matrix + class labels) or unsupervised (data matrix) machine learning. The proposed method identifies relevant acoustic features that correlate with the ordinal data collected during the experiment. Using these features, we show that we are able to develop objective measures of the speech signal degradation that correlate well with SLP responses.","keywords: {Speech;Pathology;Vectors;Cost function;Prediction algorithms;Feature extraction;Acoustics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6853730&isnumber=6853544,"V. Berisha, J. Liss, S. Sandoval, R. Utianski and A. Spanias, ""Modeling pathological speech perception from data with similarity labels,"" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014, pp. 915-919, doi: 10.1109/ICASSP.2014.6853730."
"Towards a clinical tool for automatic intelligibility assessment,","An important, yet under-explored, problem in speech processing is the automatic assessment of intelligibility for pathological speech. In practice, intelligibility assessment is often done through subjective tests administered by speech pathologists; however research has shown that these tests are inconsistent, costly, and exhibit poor reliability. Although some automatic methods for intelligibility assessment for telecommunications exist, research specific to pathological speech has been limited. Here, we propose an algorithm that captures important multi-scale perceptual cues shown to correlate well with intelligibility. Nonlinear classifiers are trained at each time scale and a final intelligibility decision is made using ensemble learning methods from machine learning. Preliminary results indicate a marked improvement in intelligibility assessment over published baseline results.","keywords: {Speech;Feature extraction;Pathology;Support vector machine classification;Distortion measurement;Speech processing;intelligibility assessment;speech pathology;machine learning;multi-scale analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6638172&isnumber=6637585,"V. Berisha, R. Utianski and J. Liss, ""Towards a clinical tool for automatic intelligibility assessment,"" 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, 2013, pp. 2825-2828, doi: 10.1109/ICASSP.2013.6638172."
"Semantic Analysis of NIH Stroke Scale using Machine Learning Techniques,","In particular, stroke is a major disease leading to death in adults and elderly people, as well as disability. Rapid detection of stroke is very difficult because the cause and cause of the onset are different for each individual. In this paper, we design and implement a system for semantic analysis of early detection of stroke and recurrence of stroke in Koreans over 65 years old, based on the National Institutes of Health (NIH) Stroke Scale. Using C4.5 of the decision tree series represented by the analytics algorithm of machine learning technique, we conduct a semantic interpretation that analyzes and extracts the semantic rules of the execution mechanism that are additionally provided by C4.5. The C4.5 algorithm is used to construct a classification and prediction model using the information gain of the NIH stroke scale features, and to obtain additional NIH Stroke Scale feature reduction effects.","keywords: {Stroke (medical condition);Machine learning;Predictive models;Diseases;Medical diagnostic imaging;Semantics;Senior citizens;National Institutes of Health (NIH) Stroke Scale;Machine Learning;Medical Big Data Analysis;Stroke Disease Prediction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8668961&isnumber=8668955,"J. Yu et al., ""Semantic Analysis of NIH Stroke Scale using Machine Learning Techniques,"" 2019 International Conference on Platform Technology and Service (PlatCon), Jeju, Korea (South), 2019, pp. 1-5, doi: 10.1109/PlatCon.2019.8668961."
"Online speaking rate estimation using recurrent neural networks,","A reliable online speaking rate estimation tool is useful in many domains, including speech recognition, speech therapy intervention, speaker identification, etc. This paper proposes an online speaking rate estimation model based on recurrent neural networks (RNNs). Speaking rate is a long-term feature of speech, which depends on how many syllables were spoken over an extended time window (seconds). We posit that since RNNs can capture long-term dependencies through the memory of previous hidden states, they are a good match for the speaking rate estimation task. Here we train a long short-term memory (LSTM) RNN on a set of speech features that are known to correlate with speech rhythm. An evaluation on spontaneous speech shows that the method yields a higher correlation between the estimated rate and the ground-truth rate when compared to the state-of-the-art alternatives. The evaluation on longitudinal pathological speech shows that the proposed method can capture long-term and short-term changes in speaking rate.","keywords: {Speech;Estimation;Training;Feature extraction;Recurrent neural networks;Speech recognition;Correlation;recurrent neural networks;speaking rate estimation;clinical tool},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7472678&isnumber=7471614,"Y. Jiao, M. Tu, V. Berisha and J. Liss, ""Online speaking rate estimation using recurrent neural networks,"" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5245-5249, doi: 10.1109/ICASSP.2016.7472678."
"Breath to speech communication with fall detection for elder/patient with take care analytics,","People suffering from Developmental-Disabilities are almost entirely paralyzed disabling them to communicate in any way except using an Augmentative and Alternative Communication device. Survey analysis tells that 1.4% of globe's population suffers from speech disorder which is more than the Karnataka's population. Looking into the elderly group it was analyzed that the fall events cannot be predicted and might be an unsafe event. Estimates tell that 33.33% of 65 and above aged people fall every year. It can be seen that out of these falls 55% occur at home and 23% occur near the home. Hence, a dependable fall detection system has to be developed, and commercially be used all over the globe among the elderly. Depending on fast detection and delivering signals, the cost of the system can be reduced which is interconnected to the reaction and saving time. An enhanced breathe to speech communication and fall detection system for elderly people and also monitoring through a take care analytics is suggested that are based on intelligent sensors that are put by the person using that device.","keywords: {Speech;Senior citizens;Market research;Acceleration;Conferences;Communications technology;breath to speech communication;fail detection;patient monitoring;styling;insert},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7807877&isnumber=7807761,"S. S. Kumar, B. K. Aishwarya, K. N. Bhanutheja and M. Chaitra, ""Breath to speech communication with fall detection for elder/patient with take care analytics,"" 2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT), Bangalore, India, 2016, pp. 527-531, doi: 10.1109/RTEICT.2016.7807877."
"Raw Source and Filter Modelling for Dysarthric Speech Recognition,","Acoustic modelling for automatic dysarthric speech recognition (ADSR) is a challenging task. Data deficiency is a major problem and substantial differences between the typical and dysarthric speech complicates transfer learning. In this paper, we build acoustic models using the raw magnitude spectra of the source and filter components. The proposed multi-stream model consists of convolutional and recurrent layers. It allows for fusing the vocal tract and excitation components at different levels of abstraction and after per-stream pre-processing. We show that such a multi-stream processing leverages these two information streams and helps s model towards normalising the speaker attributes and speaking style. This potentially leads to better handling of the dysarthric speech with a large inter-speaker and intra-speaker variability. We compare the proposed system with various features, study the training dynamics, explore usefulness of the data augmentation and provide interpretation for the learned convolutional filters. On the widely used TORGO dysarthric speech corpus, the proposed approach results in up to 1.7% absolute WER reduction for dysarthric speech compared with the MFCC base-line. Our best model reaches up to 40.6% and 11.8% WER for dysarthric and typical speech, respectively.","keywords: {Training;Representation learning;Convolution;Perturbation methods;Transfer learning;Speech recognition;Information filters;Dysarthric speech recognition;source-filter separation and fusion;multi-stream acoustic modelling},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746553&isnumber=9746004,"Z. Yue, E. Loweimi and Z. Cvetkovic, ""Raw Source and Filter Modelling for Dysarthric Speech Recognition,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 7377-7381, doi: 10.1109/ICASSP43922.2022.9746553."
"Glottal signal parameters as features set for neurological voice disorders diagnosis using K-Nearest Neighbors (KNN),","Disorders affecting nervous system can affect the voice in different ways. Different neurological disorders may lead to speech problems; this may modify the articulatory characteristics related to vocal folds function, which provide important information for detecting certain neurological diseases. In order to improve the diagnosis of Neurological Voice Disorders (NVD) an objective technique based on articulatory evaluations of vocal folds vibration, based on an estimation of a Glottic Signal (GS) extracted from Speech signal. In this work, we propose a method based on parameters extracted from GS obtained by an inverse filtering algorithm for automatic classification and diagnosis of NVD using K-Nearest Neighbors (KNN). Our work is developed around Saarbrucken Voice Database it is an open German database containing deferent samples, words, sentences of normal and pathological voices. We have selected three groups of subjects: persons with normal voices, which considered as reference, persons having suffered Parkinson disease (PD) and persons with spasmodic dysphonia.","keywords: {Pathology;Diseases;Feature extraction;Databases;Filtering;Mel frequency cepstral coefficient;Harmonic analysis;Neurological Voice Disorders;Glottal signal parameters;articultory;KNN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8374384&isnumber=8374364,"M. Dahmani and M. Guerti, ""Glottal signal parameters as features set for neurological voice disorders diagnosis using K-Nearest Neighbors (KNN),"" 2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP), Algiers, Algeria, 2018, pp. 1-5, doi: 10.1109/ICNLSP.2018.8374384."
"A Kepstrum based approach for enhancement of dysarthric speech,","A novel speech processing algorithm based on Kepstrum analysis procedure is proposed in this paper, which provides very good speech enhancement for Dysarthric speech. Kepstrum approach has so far been used in communication applications like two microphone noise cancellation. The other applications are derivation of Kalman filter and wiener filter equations. So an attempt to use kepstrum approach to enhance the dysarthric speech is made in this paper. The algorithm is tested on various monosyllabic and bisyllabic (Consonant-Vowel pattern and Consonant-Vowel-Consonant-Vowel pattern) dysarthric speech samples of cerebral palsy patients between the age group of 40–60 years and it was found that there was considerable formant shift and modification in the energy of the output signal. Also the results obtained by kepstrum approach is compared with the results obtained by Linear Prediction Coefficients (LPC) method and it is found that kepstrum approach gives better results.","keywords: {Speech;Speech enhancement;Filter bank;Wiener filter;Estimation;Equations;Dysarthric speech;Kepstrum analysis;Formants;Linear Prediction Coefficients},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5646752&isnumber=5646157,"V. Lalitha, P. Prema and L. Mathew, ""A Kepstrum based approach for enhancement of dysarthric speech,"" 2010 3rd International Congress on Image and Signal Processing, Yantai, China, 2010, pp. 3474-3478, doi: 10.1109/CISP.2010.5646752."
"A 40nm CMOS SoC for Real-Time Dysarthric Voice Conversion of Stroke Patients,","This paper presents the first dysarthric voice conversion SoC, which can translate stroke patients' voice into more intelligible and clearer speech in real time. The SoC is composed of a RISC-V MPU and a compact DNN engine with a single 16-bit multiply-accumulator, which improves 12x performance and > 100x energy efficiency, and has been implemented in 40nm CMOS. The silicon area is 0.68×0.79mm2, and the measured power is 18.4mW for converting 3-sec dysarthric voice within 0.5 sec (at 200MHz and 0.8V) and 4.8mW for conversion < 1 sec (at 100MHz and 0.6V).","keywords: {Power measurement;Design automation;Asia;Area measurement;Stroke (medical condition);Real-time systems;Silicon},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9712584&isnumber=9712479,"T. -J. Lin et al., ""A 40nm CMOS SoC for Real-Time Dysarthric Voice Conversion of Stroke Patients,"" 2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC), Taipei, Taiwan, 2022, pp. 7-8, doi: 10.1109/ASP-DAC52403.2022.9712584."
"Speech recognition of deaf and hard of hearing people using hybrid neural network,","This paper describes isolated word recognition of deaf students by unsupervised and supervised neural network. Compared to normal speech, there is high variability in deaf speech and by hearing once we couldn't understand it. By the use of proposed method deaf people can make use of all voice operated devices. In this paper we use combination of SOFM and BPN neural network for recognition. Initially the input is sampled, filtered, windowed and Perceptual Linear Predictive Coefficients are determined for each frame. These coefficients are applied as input to the SOFM neural network. The output of this network is given to BPN neural network comprising of 3 layers for learning. The network has been trained with five words uttered by five different deaf persons in the age group of 5-10 years. Another set of same five words uttered by same five deaf persons were used for test purposes. The recognition results for the word one, three, four is 50 to 60% and for five is 50%...But the recognition results for word two are only10% since the variability is high for two. The results can be improved by varying the parameters of the hybrid neural network.","keywords: {Accuracy;Back propagation neural network (BPN);Self organized feature map neural network (SOFM);Perceptual linear prediction coefficients (PLP)},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5558589&isnumber=5558502,"C. Jeyalakshmi, V. Krishnamurthi and A. Revathi, ""Speech recognition of deaf and hard of hearing people using hybrid neural network,"" 2010 2nd International Conference on Mechanical and Electronics Engineering, Kyoto, Japan, 2010, pp. V1-83-V1-87, doi: 10.1109/ICMEE.2010.5558589."
"ASR for electro-laryngeal speech,","The electro-larynx device (EL) offers the possibility to re-obtain speech when the larynx is removed after a total laryngectomy. Speech produced with an EL suffers from inadequate speech sound quality, therefore there is a strong need to enhance EL speech. When disordered speech is applied to Automatic Speech Recognition (ASR) systems, the performance will significantly decrease. ASR systems are increasingly part of daily life and therefore, the word accuracy rate of disordered speech should be reasonably high in order to be able to make ASR technologies accessible for patients suffering from speech disorders. Moreover, ASR is a method to get an objective rating for the intelligibility of disordered speech. In this paper we apply disordered speech, namely speech produced by an EL, on an ASR system which was designed for normal, healthy speech and evaluate its performance with different types of adaptation. Furthermore, we show that two approaches to reduce the directly radiated EL (DREL) noise from the device itself are able to increase the word accuracy rate compared to the unprocessed EL speech.","keywords: {Speech;Training;Speech enhancement;Speech recognition;Databases;Accuracy;Materials;Automatic Speech Recognition (ASR);electro-larynx (EL);speech enhancement;MLLR adaptation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6707735&isnumber=6707689,"A. K. Fuchs, J. A. Morales-Cordovilla and M. Hagmüller, ""ASR for electro-laryngeal speech,"" 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, Czech Republic, 2013, pp. 234-238, doi: 10.1109/ASRU.2013.6707735."
"Navigo--Accessibility Solutions for Cerebral Palsy Affected,","This paper presents the architecture of an integrated teachable interface, designed and implemented to provide accessibility solutions to the cerebral palsy patient in the virtual and urban space. It enables multimodal interaction between the palsy user and computers, handhelds and remote controlled appliances for enhanced usability. The idea is to redesign the system by changing the way we interpret the input, to suit the user's needs. The USP of the solution is that it is extremely dynamic, flexible and customizable to the entire range and levels of a complex condition as cerebral palsy. It works on the basis of allowing an individual to define simple variable inputs for himself and the system enabling him using the same by various specially designed tools, interfaces and feedback mechanisms.","keywords: {Birth disorders;Home appliances;Keyboards;Computational intelligence;Computer architecture;Handheld computers;Usability;Feedback;Mice;Speech recognition;Human Computer Interaction;Cerebral Palsy;Accessibility Solutions},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4052772&isnumber=4052643,"H. Pokhariya, P. Kulkarni, V. Kantroo and T. Jindal, ""Navigo--Accessibility Solutions for Cerebral Palsy Affected,"" 2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06), Sydney, NSW, Australia, 2006, pp. 143-143, doi: 10.1109/CIMCA.2006.155."
"A study of pronunciation verification in a speech therapy application,",Techniques are presented for detecting phoneme level mispronunciations in utterances obtained from a population of impaired children speakers. The intended application of these approaches is to use the resulting confidence measures to provide feedback to patients concerning the quality of pronunciations in utterances arising within interactive speech therapy sessions. The pronunciation verification scenario involves presenting utterances of known words to a phonetic decoder and generating confusion networks from the resulting phone lattices. Confidence measures are derived from the posterior probabilities obtained from the confusion networks. Phoneme level mispronunciation detection performance was significantly improved with respect to a baseline system by optimizing acoustic models and pronunciation models in the phonetic decoder and applying a nonlinear mapping to the confusion network posteriors.,"keywords: {Speech;Medical treatment;Acoustic measurements;Application software;Decoding;Natural languages;Lattices;Neuromuscular;Loudspeakers;Communications technology;confidence measure;speech therapy},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4960657&isnumber=4959496,"Shou-Chun Yin, R. Rose, O. Saz and E. Lleida, ""A study of pronunciation verification in a speech therapy application,"" 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, Taipei, Taiwan, 2009, pp. 4609-4612, doi: 10.1109/ICASSP.2009.4960657."
"Building a Truly Inclusive Protocol for Students with Disabilities from an Experience in STEM areas,","Three of the most reported strategies for students with disabilities (SWD) along their stay in higher education institutions include adapting changes in management and facilities through protocols that focused on Admission, Retention and Graduation. These strategies are important, although they do not guarantee full inclusivity during the teaching, learning and evaluation process. In this work, through a qualitatively methodology, we describe the perceptions of inclusivity of five lecturers, eight undergraduates and one SWD in science courses for Computer Systems Engineering. Furthermore, we report one successful experience to design an inclusive evaluation in a Mathematics course. The evidence found in this work suggests that the center of every protocol for SWD should be focused on Attention, as a new and longer stage. This stage includes e.g. teacher training, inclusive curriculum (for teaching and evaluation), student service and university extension programs, and inclusivity-focused research. Our purpose is to address this experience to the engineering community and promote the establishment of policies towards a more inclusive higher education system.","keywords: {Training;Protocols;Conferences;Education;Buildings;Documentation;Systems engineering and theory;inclusive education;physically impaired student;inclusive protocols;educational innovation;higher education},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9454024&isnumber=9453843,"M. Peña-Becerril, C. Camacho-Zuñiga, C. Martínez-Peña and J. C. González-Balderas, ""Building a Truly Inclusive Protocol for Students with Disabilities from an Experience in STEM areas,"" 2021 IEEE Global Engineering Education Conference (EDUCON), Vienna, Austria, 2021, pp. 189-193, doi: 10.1109/EDUCON46332.2021.9454024."
"Fuzzy inference system & fuzzy cognitive maps based classification,",Fuzzy classification is very necessary because it has the ability to use interpretable rules. It has got control over the limitations of crisp rule based classifiers. This paper mainly deals with classification on the basis of soft computing techniques fuzzy cognitive maps and fuzzy inference system on the lenses dataset. The results obtained with FIS shows 100% accuracy. Sometimes the data available for classification contain missing or ambiguous data so Neutrosophic logic is used for classification to deal with indeterminacy.,"keywords: {Lenses;Information services;Electronic publishing;Internet;Fuzzy cognitive maps;Computers;Classification;Fuzzy cognitive maps;Fuzzy inference system},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7164720&isnumber=7164643,"K. Bhutani, Gaurav and M. Kumar, ""Fuzzy inference system & fuzzy cognitive maps based classification,"" 2015 International Conference on Advances in Computer Engineering and Applications, Ghaziabad, India, 2015, pp. 305-309, doi: 10.1109/ICACEA.2015.7164720."
"Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion,","Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.","keywords: {Training;Error analysis;Measurement uncertainty;Asia;Information processing},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10317592&isnumber=10317095,"W. -C. Huang and T. Toda, ""Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion,"" 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Taipei, Taiwan, 2023, pp. 1161-1166, doi: 10.1109/APSIPAASC58517.2023.10317592."
"Using Acoustic Deep Neural Network Embeddings to Detect Multiple Sclerosis From Speech,","Multiple sclerosis (MS) is a chronic inflammatory disease of the central nervous system. It affects cognitive and motor functions, and the limitation of executive functions can also manifest itself in speech production. Due to this, automatic speech analysis might serve as an effective technique for assessing MS, or for monitoring the status of the patient. However, choosing the features to be extracted from the recordings is not straightforward. In the past few years, general feature extractors such as i-vectors, d-vectors and x-vectors have found their way into automatic speech analysis. In this study we show that there is no need to employ a special neural network architecture such as x-vectors to calculate effective features, but (even more) indicative features can be derived on the basis of a standard Deep Neural Network acoustic model. From our results, these features could effectively be used to distinguish MS subjects from healthy controls, as we measured AUC scores up to 0.935. We found that classification performance depended only slightly on the choice of the hid-den layer used to extract our features, but the speech task per-formed by the subject turned out to be an important factor.","keywords: {Deep learning;Speech analysis;Multiple sclerosis;Signal processing;Feature extraction;Acoustics;Speech processing;Multiple Sclerosis;medical speech processing;Deep Neural Networks;embeddings;x-vectors},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746856&isnumber=9746004,"G. Gosztolya, L. Tóth, V. Svindt, J. Bóna and I. Hoffmann, ""Using Acoustic Deep Neural Network Embeddings to Detect Multiple Sclerosis From Speech,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 6927-6931, doi: 10.1109/ICASSP43922.2022.9746856."
"Speech Therapy Applications based on Speech Technologies,","The proposed paper focuses on the area of ICTbased supportive tools for speech therapy in children with speech and hearing disorders in Slovak language. The main idea was to design a concept of the supportive tool, where speech technologies can be used in a new, modern way. The web application was designed to help a child to train a correct pronunciation of particular sounds. To evaluate the similarity of the spoken word, Dynamic Time Warping algorithm was implemented, which measure distance between spoken word and the pattern in the database. Obtained distance help a caregivers or therapist to evaluate the need of continuation in speech therapy with a particular sound, or it is possible to move to another one. We also proved the concept of using automatic speech recognition in the speech therapy to create modern game-like speech therapy tools. The proposed paper brings the first observations from preliminary tests and discuss advantages and drawbacks of designed tools.","keywords: {Databases;Heuristic algorithms;Medical treatment;Auditory system;Time measurement;Automatic speech recognition;speech therapy;speech technologies;web application;telemedicine},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10109062&isnumber=10109002,"S. Ondáš, A. Čižmár, L. Šimčiková and J. Kováč, ""Speech Therapy Applications based on Speech Technologies,"" 2023 33rd International Conference Radioelektronika (RADIOELEKTRONIKA), Pardubice, Czech Republic, 2023, pp. 1-5, doi: 10.1109/RADIOELEKTRONIKA57919.2023.10109062."
"Automatic Screening Of Children With Speech Sound Disorders Using Paralinguistic Features,","Subjective screening of children with speech disorders is costly, time consuming and infeasible due to the limited availability of Speech and Language Pathologists (SLPs). Therefore, there is an increasing interest in automatic speech analysis of children with speech disorders as it can offer a practical alternative to human assessment. Paralinguistic features are a set of low-level descriptors commonly used in speech emotion recognition. However, they have not yet been examined with childhood speech sound disorders such as, apraxia-of-speech and phonological and articulation disorders. In this paper, we investigated the effectiveness of paralinguistic features in discriminating between typically developing children and those who suffer from different types of speech sound disorders. Two types of standard paralinguistic features were explored, the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) and its extended version, (eGeMAPS) feature sets. We applied feature selection to find the most discriminant set of features and employed binary classification using a support vector machine (SVM) to discriminate between the two groups. The method was tested on a recently-released public speech corpus collected from typically developing children and children with various types of speech sound disorders. The system achieved segment-level and subject-level unweighted average recall (UAR) of around 78% and 87% respectively.","keywords: {speech sound disorders;speech therapy;paralinguistic features},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8918725&isnumber=8918685,"M. Shahin, B. Ahmed, D. V. Smith, A. Duenser and J. Epps, ""Automatic Screening Of Children With Speech Sound Disorders Using Paralinguistic Features,"" 2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP), Pittsburgh, PA, USA, 2019, pp. 1-5, doi: 10.1109/MLSP.2019.8918725."
"Dysarthric vocal interfaces with minimal training data,","Over the past decade, several speech-based electronic assistive technologies (EATs) have been developed that target users with dysarthric speech. These EATs include vocal command & control systems, but also voice-input voice-output communication aids (VIVOCAs). In these systems, the vocal interfaces are based on automatic speech recognition systems (ASR), but this approach requires much training data and detailed annotation. In this work we evaluate an alternative approach, which works by mining utterance-based representations of speech for recurrent acoustic patterns, with the goal of achieving usable recognition accuracies with less speaker-specific training data. Comparisons with a conventional ASR system on dysarthric speech databases show that the proposed approach offers a substantial reduction in the amount of training data needed to achieve the same recognition accuracies.","keywords: {Hidden Markov models;Abstracts;Computers;Filter banks;Films;Accuracy;vocal user interface;dysarthric speech;non-negative matrix factorisation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7078582&isnumber=7078533,"J. F. Gemmeke, S. Sehgal, S. Cunningham and H. Van hamme, ""Dysarthric vocal interfaces with minimal training data,"" 2014 IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoe, NV, USA, 2014, pp. 248-253, doi: 10.1109/SLT.2014.7078582."
"Interaction of speech disorders with speech coders: effects on speech intelligibility,","Modern speech coding schemes have been developed to address the demand for economical spoken language telecommunication of acceptable quality. A variety of speech coding algorithms have been described, which compress speech to facilitate efficient transmission of spoken language over communication networks ((J.R. Deller Jr., 1993; P.E. Papamichalis, 1987). Most such speech coding algorithms are lossy in the sense that the processed speech is not identical to the original speech. As a result, some distortion is invariably introduced with any lossy speech coding strategy. For this reason, candidate coders undergo detailed evaluation to ensure that the associated speech output is of acceptable quality (S.R. Quackenbush et al., 1988). Three different coding algorithms were investigated relative to unprocessed speech: the Codebook Excited Linear Prediction (CELP), the Global System for Mobile Communications (GSM) algorithm which is a standardized speech coding algorithm in Europe, and the Linear Predictive Coding (LPC) algorithm. The specific coding schemes evaluated were MatLab implementations of NSA FS-1015 LPC-l0e; NSA FS-1016 CELP-v3.2; and ETSI GSM (A. Spanias, 1995). One of the goals of this study was to quantify the coding distortion using objective measures and to correlate these measures with speech intelligibility and subjective quality data, in the hope of identifying one or more measures that can predict the subjective results.","keywords: {Speech coding;GSM;Distortion measurement;Natural languages;Speech processing;Linear predictive coding;Economic forecasting;Communication networks;Speech analysis;Europe},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=607467&isnumber=13324,"D. G. Jamieson, L. Deng, M. Price, V. Parsa and J. Till, ""Interaction of speech disorders with speech coders: effects on speech intelligibility,"" Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96, Philadelphia, PA, USA, 1996, pp. 737-740 vol.2, doi: 10.1109/ICSLP.1996.607467."
"Models for objective evaluation of dysarthric speech from data annotated by multiple listeners,","In subjective evaluation of dysarthric speech, the inter-rater agreement between clinicians can be low. Disagreement among clinicians results from differences in their perceptual assessment abilities, familiarization with a client, clinical experiences, etc. Recently, there has been interest in developing signal processing and machine learning models for objective evaluation of subjective speech quality. In this paper, we propose a new method to address this problem by collecting subjective ratings from multiple evaluators and modeling the reliability of each annotator within a machine learning framework. In contrast to previous work, our model explicitly models the dependence of the speaker on an evaluators reliability. We evaluate the model on a series of experiments on a dysarthric speech database and show that our method outperforms other similar approaches.","keywords: {Speech;Noise measurement;Reliability;Feature extraction;Mathematical model;Training;Correlation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7869163&isnumber=7868973,"M. Tu, Y. Jiao, V. Berisha and J. M. Liss, ""Models for objective evaluation of dysarthric speech from data annotated by multiple listeners,"" 2016 50th Asilomar Conference on Signals, Systems and Computers, Pacific Grove, CA, USA, 2016, pp. 827-830, doi: 10.1109/ACSSC.2016.7869163."
"Evaluation of the word fluency in parkinson's disease patients treated with deep brain stimulation-a pilot study-,","Parkinson's disease is a chronic, progressive and common disease of neurological disorders. Motor complication in parkinson's disease (PD) is resting tremor, slow movement, rigidity and postural instability. The motor symptoms of PD respond well to bilateral deep brain stimulation (DBS). Recent study, there are also reports of worsened verbal fluency, executive dysfunction, and processing speed with DBS. Whether subthalamic nuclei (STN) stimulation worsens there are under debate. The aim of this study was to explore the effects of STN stimulation verbal fluency as assessed with clinical neuropsychological tests. Eight patients treated with deep brain stimulation were enrolled, and some of the patients continued anti-PD medications. Assessments were done both with the STN stimulation turned OFF and ON. In both test conditions, the following were assessed: speech , word fluency A, and B. The score of the word fluency test of all patients have undergone DBS surgery significantly worsened as compared with before surgery. Five patients speech ware worsened, but three patients were improved when the STN stimulation was turned OFF. On the other hand, five patients were reduced the word fluency's total score when the STN stimulation was turned OFF. In this sample, STN stimulation significantly worsened the result of the word fluency test. When the STN stimulation was turned OFF, it was reduced. These finding suggests that STN-DBS might be worse speech conditions and verbal fluency.","keywords: {Satellite broadcasting;Speech;Parkinson's disease;Deep brain stimulation;Word fluency;Verbal fluency;Cognitive function},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6275625&isnumber=6275588,"Y. Watanabe, T. Sada, R. Takashima, M. Takano, H. Tateno and K. Hirata, ""Evaluation of the word fluency in parkinson's disease patients treated with deep brain stimulation-a pilot study-,"" 2012 ICME International Conference on Complex Medical Engineering (CME), Kobe, Japan, 2012, pp. 5-8, doi: 10.1109/ICCME.2012.6275625."
"Compression of acoustic inventories using asynchronous interpolation,","A compression method is proposed that takes advantage of a powerful property of acoustic unit inventories: In the appropriate acoustic space, units that share a (context-dependent or -independent) phoneme label must be close to a vector phoneme template associated with the phoneme. The method approximates units by interpolation between templates. The interpolation operation involves two asynchronous weight functions operating on the template. One is associated with spectral peak locations, the second with spectral balance. This enables approximating transitions such as [i:]/spl rarr/[v], in which formant movement precedes frication onset. The algorithm guarantees smooth concatenation points.","keywords: {Interpolation;Speech synthesis;Speech coding;Acoustic devices;Artificial intelligence;Acoustic applications;Loudspeakers;Compression algorithms;Natural languages;Acoustical engineering},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1224378&isnumber=27469,"A. B. Kain and J. P. H. van Santen, ""Compression of acoustic inventories using asynchronous interpolation,"" Proceedings of 2002 IEEE Workshop on Speech Synthesis, 2002., Santa Monica, CA, USA, 2002, pp. 83-86, doi: 10.1109/WSS.2002.1224378."
"Development of Speech Therapy Mobile Application for Speech Disorder Post-Stroke Patients,","In Malaysia, stroke is the third cause of death and disability. Stroke cause significant injury to the brain that may result in long-term problems such as communication, concentration, memory, and executive functions. About one-third of post-stroke patients have speech and communication problems that require to undergo series of one-to-one speech therapy sessions. However, there are only 300 speech therapists in Malaysia which limit the recovery and may not reach to the needed patient. More importantly, frequent therapy conducted could fasten the recovery of the patients' speech. Therefore, this research develops a mobile application to be used as an alternative for speech therapy session. Although there are mobile applications for speech therapy, none of them are in Bahasa Melayu. The mobile application implements an automatic speech recognition technology that accepts the vowel speech sound from a post-stroke patient in Bahasa Melayu. The mobile application will process the sound, evaluate, and provide feedback score for the vowel sound in an accuracy percentage. It is expected that the ASR speech therapy mobile application could help speech disorder post-stroke patients to practice their speech ability at their own time without attending speech therapy sessions.","keywords: {Training;Conferences;Medical treatment;Systems engineering and theory;Mobile applications;Engines;Monitoring;mobile app;interface;automatic speech recognition;stroke},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9612432&isnumber=9612427,"H. Basiron, M. A. Azmi, M. J. Abd Latif, A. I. Kamaruddin, A. I. M. Zaidi and W. M. F. W. Badrulzaman, ""Development of Speech Therapy Mobile Application for Speech Disorder Post-Stroke Patients,"" 2021 IEEE 11th International Conference on System Engineering and Technology (ICSET), Shah Alam, Malaysia, 2021, pp. 130-133, doi: 10.1109/ICSET53708.2021.9612432."
"Exploratory analysis of speech features related to depression in adults with Aphasia,","Aphasia is an acquired communication disorder resulting from brain damage and impairs an individual's ability to use, produce, and comprehend language. Loss of communication skills can be stressful and may result in depression, yet most depression diagnostic tools are designed for adults without aphasia. This paper discusses preliminary results from a research effort to examine acoustic profiles of adults with aphasia who have been assessed as having possible depression versus those who assessment suggests they are not depressed based on tools completed by their caretakers. This study analyzes prosodic and spectral features in 14 participants (7 assessed as having possible depression and 7 whose assessment does not suggest depression). The results showed using Cepstral Peak Prominence provided the best overall performance in separating depressed and non-depressed speech among adults with aphasia.","keywords: {Speech;Cepstral analysis;Feature extraction;Jitter;Stress;Acoustic measurements;aphasia;depression;speech analysis;prosodic features},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7472792&isnumber=7471614,"S. Gillespie, E. Moore, J. Laures-Gore and M. Farina, ""Exploratory analysis of speech features related to depression in adults with Aphasia,"" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5815-5819, doi: 10.1109/ICASSP.2016.7472792."
"Speech enhancement for pathological voice using time-frequency trajectory excitation modeling,","This paper proposes a speech enhancement algorithm for pathological voices using a time-frequency trajectory excitation (TFTE) modeling. The TFTE model has a capability of delicately controlling the periodic and non-periodic excitation components by taking a single pitch based decomposition process. By investigating the difference of frequency characteristics between pathological and normal voices, this paper proposes an enhancement algorithm which can efficiently reduce the breathiness of the pathological voice while maintaining the identity of the speaker. Subjective test results are presented to verify the effectiveness of the proposed algorithm.","keywords: {Pathology;Speech;Indexes;Speech enhancement;Time-frequency analysis;Noise measurement;Speech coding},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6694125&isnumber=6694103,"E. Song, J. Ryu and H. -G. Kang, ""Speech enhancement for pathological voice using time-frequency trajectory excitation modeling,"" 2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, Kaohsiung, Taiwan, 2013, pp. 1-4, doi: 10.1109/APSIPA.2013.6694125."
"Comparative Analysis of Machine Learning and Ensemble Learning Classifiers for Parkinson’s Disease Detection,","A neurological illness called Parkinson's disease (PD) commonly appears between the ages of 55 and 65. Moreover, a patient's entire quality of life is significantly impacted by the progressive development of motor as well as non-motor symptoms due to this disease. There is no known cure for PD, although a number of therapies have been created to assist control its symptoms. Therefore, the management of PD is a field that is expanding, and there is a need to develop a comprehensive framework for the timely detection and classification of PD. In this paper, we developed six machine learning-based and five ensemble learning-based classification models to forecast PD. The six base classifiers which are used in the current study are Support Vector Machine (SVM), Decision Trees (DTs), Random Forest (RF), K-Nearest Neighbor (KNN), Logistic Regression (LR), Naive Bayes (NB); and five ensemble classifiers named XGBoost, Gradient Boost, Bagging, CatBoost and Light Gradient Boosted Machine (LGBM) respectively, are then carefully compared. To improve the performance of the classifiers and to reduce the problem of overfitting, a feature selection method named Principal Component Analysis (PCA) and various preprocessing techniques are applied. Further, this study uses the voice samples dataset from the UCI repository having 188 PD and 64 normal patients. Overall, our findings revealed that, when compared to the other five base classifiers, the RF model offered the best classification performance with an accuracy of 82.37%, and the ensemble classifier named LGBM shows best results when compared with base as well as ensemble classifiers having an accuracy of 85.90%.","keywords: {Support vector machines;Parkinson's disease;Predictive models;Feature extraction;Ensemble learning;Older adults;Random forests;Parkinson’s Disease;Ensemble Classifiers;xgboost;Gradient Boost;Bagging;catboost;LGBM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10007376&isnumber=10006842,"P. Goyal, R. Rani and K. Singh, ""Comparative Analysis of Machine Learning and Ensemble Learning Classifiers for Parkinson’s Disease Detection,"" 2022 3rd International Conference on Computing, Analytics and Networks (ICAN), Rajpura, Punjab, India, 2022, pp. 1-6, doi: 10.1109/ICAN56228.2022.10007376."
Title,Abstract,Keywords,Links,Citation
"Quantitative Assessment of Syllabic Timing Deficits in Ataxic Dysarthria,","Parametric analysis of Cerebellar Dysarthria (CD) may be valuable and more informative compared to its clinical assessment. A quantifiable estimation of the timing deficits in repeated syllabic utterance is described in the current study. Thirty-five individuals were diagnosed with cerebellar ataxia to varying degrees and twenty-six age-matched healthy controls were recruited. To automatically detect the local maxima of each syllable in the recorded speech files, a topographic prominence incorporated concept is designed. Subsequently, four acoustic features and eight corresponding parametric measurements are extracted to identify articulatory deficits in ataxic dysarthria. A comparative study on the behaviour of these measures for dysarthric and non-dysarthric subjects is presented in this paper. The results are further explored using a dimensionreduction tool (Principal Component Analysis) to emphasize variation and bring out the strongest discriminating patterns in our feature dataset.","keywords: {Feature extraction;Principal component analysis;Acoustic measurements;Timing;Eigenvalues and eigenfunctions;Rhythm;dysarthria;speech disorder;repeated syllable;cerebellar ataxia;topographic prominence},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512311&isnumber=8512178,"B. Kashyap, P. N. Pathirana, M. Horne, L. Power and D. Szmulewicz, ""Quantitative Assessment of Syllabic Timing Deficits in Ataxic Dysarthria,"" 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, USA, 2018, pp. 425-428, doi: 10.1109/EMBC.2018.8512311."
"Non-invasive stroke diagnosis using speech data from dysarthria patients,","Acute Ischemic Stroke (AIS) is a major cause of disability and can lead to death in severe cases. A common symptom of AIS, dysarthria, significantly impacts the quality of life of patients. In this study, we developed a deep learning model using dysarthria data for cost-effective and non-invasive brain stroke diagnosis. We utilized models such as ResNet50, InceptionV4, ResNeXt50, SEResNeXt18, and AttResNet50 to effectively extract and classify speech features indicative of stroke symptoms. These models demonstrated high performance, with Sensitivity, Specificity, Precision, Accuracy, and F1-score values reaching 96.77%, 96.08%, 92.82%, 95.52%, and 93.82%, respectively. Our approach offers a non-invasive, cost-effective alternative for early stroke detection, with potential for further accuracy improvements through additional research. This method promises rapid, economical early diagnosis, which could positively impact long-term treatment and healthcare options.","keywords: {Deep learning;Accuracy;Sensitivity;Medical services;Learning (artificial intelligence);Stroke (medical condition);Brain modeling;Feature extraction;Data models;Residual neural networks;Artificial intelligence;Stroke;Dysarthria;Diagnosis;Deep learning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10781716&isnumber=10781494,"S. B. Mun, Y. J. Kim and K. G. Kim, ""Non-invasive stroke diagnosis using speech data from dysarthria patients,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10781716."
"Fine-Tuning Pre-Trained Audio Models for Dysarthria Severity Classification: A Second Place Solution in the Multimodal Dysarthria Severity Classification Challenge,","Dysarthria, a neurological disorder affecting speech, poses significant challenges for automatic diagnosis and severity classification due to its complexity and the scarcity of relevant datasets. This study addresses these challenges by leveraging the dataset provided by the ISCSLP 2024 Multimodal Dysarthria Severity Classification Challenge. Our approach primarily involves two key steps: splitting the training set into training and validation subsets, and fine-tuning pre-trained au-dio models to adapt them to the task. To ensure robust evaluation and prevent label leakage, we employed the StratifiedGroupKFold function from sklearn for dataset splitting, ensuring that training and validation sets do not share participants. This method allowed us to maintain consistent statistical distributions across folds. For model finetuning, we selected two pre-trained audio models, w2v-bert-2.0 and whisper-Iarge-v3, and fine-tuned them on different folds of the dataset. Our results indicate that the w2v-bert-2.0 model fine-tuned on fold-2 achieved the highest performance, with a validation Fl-score of 0.699 and an online score of 7.7452. The experimental results, including confusion matrices and embedding distributions, highlight the model's ability to distinguish between different severity levels of dysarthria. Our approach achieved a commendable second place in the competition’ demonstrating the effectiveness of fine-tuning pre-trained audio models for this task. Future work will explore the integration of multimodal data and multi-task learning strategies to further enhance model performance.","keywords: {Training;Neurological diseases;Adaptation models;Magnetic resonance imaging;Statistical distributions;Metadata;Multitasking;Complexity theory;Optimization;Overfitting;Dysarthria Severity Classification;Dataset Splitting;Pre-trained Audio Models;Fine-Tuning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800228&isnumber=10799969,"W. Dai, M. Li, Y. He and Y. Zhu, ""Fine-Tuning Pre-Trained Audio Models for Dysarthria Severity Classification: A Second Place Solution in the Multimodal Dysarthria Severity Classification Challenge,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 151-153, doi: 10.1109/ISCSLP63861.2024.10800228."
"Automated Detection and Severity Assessment of Dysarthria using Raw Speech,","Dysarthria is a medical condition that impairs an individual’s ability to speak clearly due to muscle weakness or paralysis. To diagnose and monitor dysarthria severity, this article proposes the use of a deep learning model that utilizes raw speech waveforms. This approach eliminates the need for feature engineering and enhances the model’s ability to handle noise and speech variability. The proposed system was compared to a standard convolutional neural network (CNN) model and was found to perform better in both dysarthria severity classification and dysarthria/healthy control classification tasks. The results indicate that the SincNet model achieved an accuracy of 95.7% and 99.6% in these tasks, respectively. The proposed system can aid clinicians in diagnosing and monitoring dysarthria severity and could have broader applications in speech-related disorders. The study emphasizes the importance of using raw waveform-based models for speech analysis and demonstrates the effectiveness of SincNet in automatic dysarthria/healthy control classification and dysarthria severity assessment.","keywords: {Deep learning;Speech analysis;Neural networks;Speech enhancement;Muscles;Paralysis;Convolutional neural networks;Dysarthria severity level assessment;Speech-related disorders;Deep learning;SincNet;Raw waveforms},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10307923&isnumber=10306339,"K. Radha and M. Bansal, ""Automated Detection and Severity Assessment of Dysarthria using Raw Speech,"" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-7, doi: 10.1109/ICCCNT56998.2023.10307923."
"Investigation on articulatory and acoustic characteristics of dysarthria,","Direct measurement of articulation contributes to figure out the mechanism of dysarthric speech production accurately, comparing to analyzing acoustic signal alone. This study makes a statistical comparison between dysarthric and normal speech, and investigates and analyzes dysarthric characteristics based on articulatory and acoustic data of continuous English utterance. The distribution of the articulatory point is calculated for each vowel, where the vowels show a relatively smaller region for dysarthria than for normal speech. This implies that the speakers with dysarthria may have some difficulties to fully use the articulatory space as the speakers without dysarthria. The rhythm and sound source are analyzed using autocorrelation function, power spectrum and linear prediction coding. The results reveal that the speakers without dysarthria can keep steady rhythm and energy in uttering consonant-vowel repetition sequences but it is hard for the speakers with dysarthria to maintain the stability. Meanwhile, the dysarthric sound source shows unstable period of the fundamental frequency, which reflects that the speakers with dysarthria could not control the glottis movement well.","keywords: {Speech;Speech recognition;Rhythm;Correlation;Standards;Tongue;dysarthria;speech analysis;articulator movement},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6936631&isnumber=6936571,"C. Zhang, J. Dang, J. Zhang and J. Wei, ""Investigation on articulatory and acoustic characteristics of dysarthria,"" The 9th International Symposium on Chinese Spoken Language Processing, Singapore, 2014, pp. 326-330, doi: 10.1109/ISCSLP.2014.6936631."
"Evolving Diagnostic Techniques for Speech Disorders: Investigating Dysarthria Classification Through DenseNet201 CNN Framework,","The primary subject of this research work pertains to dysarthria, a prevalent speech impairment observed in individuals diagnosed with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Prompt detection of dysarthria is essential for effective therapies and improved patient results, as it significantly impacts communication proficiency. The research employed a DenseNet201 Convolutional Neural Network (CNN) to categorize speech using the TORGO database, which has 2000 samples of individuals with and without dysarthria, representing various genders. The dataset comprises individuals of both genders, encompassing both dysarthric and non-dysarthric individuals. Each dataset consists of 500 samples, which were collected during distinct sessions. The DenseNet201 convolutional neural network (CNN) technique has a notable accuracy rate of 96% in distinguishing between cases of dysarthria and non-dysarthria. This outcome underscores the potential of deep learning technology in aiding the timely identification of dysarthria and facilitating fast interventions for affected individuals. This work provides valuable insights on the progress of diagnostic capabilities and offers a potential avenue for enhancing the well-being of those experiencing speech impairments.","keywords: {Deep learning;Databases;Computational modeling;Medical treatment;Speech enhancement;Robustness;Telecommunication computing;Artificial Intelligence;Deep Learning;DenseNet201 Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10550236&isnumber=10550194,"K. Mittal, K. S. Gill, S. Malhotra and S. Devliyal, ""Evolving Diagnostic Techniques for Speech Disorders: Investigating Dysarthria Classification Through DenseNet201 CNN Framework,"" 2024 International Conference on Communication, Computing and Internet of Things (IC3IoT), Chennai, India, 2024, pp. 1-6, doi: 10.1109/IC3IoT60841.2024.10550236."
"Analysis of Features for Dysarthria Severity Classification from Speech,","Dysarthria is a disorder that affects the ability of an individual to speak clearly. Diagnosis of the severity of dysarthria can aid in providing appropriate therapy to the individual. Therefore, the current work focuses on identifying features that can be used to estimate the severity of dysarthria. In this regard, two feature sets are considered, namely DisVoice and OpenSMILE, and the genetic algorithm is used to identify the most suitable list of features from both feature sets using 3 speech corpora, namely SSN-TDSC, Torgo, and UA Speech corpora. In order to test the effectiveness of these features, classifiers are trained on each feature set as a whole and on the features identified by the genetic algorithm, and their performance compared. It is observed that articulatory and prosodic features are most important in the diagnosis of severity of dysarthria. A maximum accuracy of 85% is obtained with the shortlisted DisVoice features and 97% with the OpenSMILE features.","keywords: {Accuracy;Medical treatment;Classification algorithms;Genetic algorithms;IEEE Regions;dysarthria;feature selection;genetic algorithm;severity classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10902941&isnumber=10902666,"P. H. Keerthi, P. Vijayalakshmi, A. R. Gladston and T. Nagarajan, ""Analysis of Features for Dysarthria Severity Classification from Speech,"" TENCON 2024 - 2024 IEEE Region 10 Conference (TENCON), Singapore, Singapore, 2024, pp. 335-338, doi: 10.1109/TENCON61640.2024.10902941."
"Multi-Modal Dysarthria Severity Assessment Using Dual-Branch Feature Decoupling Network and Mixed Expert Framework,","Dysarthria, a motor speech disorder resulting from neurological damage, significantly impairs an individual's ability to articulate words. Assessing the severity of dysarthria is crucial for effective therapeutic interventions and rehabilitation strategies. However, current methods are time-consuming and subjective, leading to potential inconsistencies. This study proposed a dual-branch feature decoupling network and mixed expert framework for the automatic diagnosis and assessment of dysarthria. The proposed hybrid network architecture integrates a ResNet-based branch for audio data with an attention module and a 3D ResNet branch for video data, and was evaluated on the Multimodal Dysarthria Severity Assessment Challenge (MDSA) dataset, achieving significant performance improvements and ranking first in the challenge with a best score of 8.7404. The study's findings highlight the efficacy of the proposed framework in capturing essential features from multimodal data, contributing to more accurate and reliable assessments of dysarthria.","keywords: {Measurement;Visualization;Three-dimensional displays;Accuracy;Neural networks;Network architecture;Motors;Acoustics;Complexity theory;Reliability;Multimodal Fusion;Dysarthria Severity Assessment;Convolutional Neural Network;Feature Decoupling Network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800159&isnumber=10799969,"S. Liang and Y. Gu, ""Multi-Modal Dysarthria Severity Assessment Using Dual-Branch Feature Decoupling Network and Mixed Expert Framework,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 126-130, doi: 10.1109/ISCSLP63861.2024.10800159."
"Spectral Analysis of Vowels and Fricatives at Varied Levels of Dysarthria Severity for Amyotrophic Lateral Sclerosis,","Dysarthria due to Amyotrophic Lateral Sclerosis (ALS) affects the acoustic characteristics of different speech sounds. The effects intensify with increasing severity leading to the collapse of the acoustic space of the affected individuals. With an aim to characterize such changes in the acoustic space, this paper studies the variations in band-specific and full-band spectral properties of 4 sustained vowels (/a/, /i/, /o/, /u/) and 3 sustained fricatives (/s/, /sh/, /f/) at different dysarthria severity levels. Effect of dysarthria on spectral features of these phonemes are not well explored. Statistical comparison of these features among different severities for the phonemes considered and among different vowels/fricatives for every severity level using speech data from 119 ALS and 40 healthy subjects indicate the followings. Though all band-specific and full-band features of the three fricatives and most of those features for the four vowels become statistically similar at high severity levels, certain features remain distinguishable. Spectral differences in 0-2 kHz band between /a/ and the other vowels and in the 2-6 kHz band between /a/ and /o/, /u/ persist through all severity levels. Moreover, properties of /f/ remain mostly unchanged with increasing dysarthria severity levels.","keywords: {Signal processing;Acoustics;Speech processing;Spectral analysis;Diseases;Amyotrophic Lateral Sclerosis;dysarthria;severity;vowels;fricatives},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10448175&isnumber=10445803,"C. V. Thirumala Kumar et al., ""Spectral Analysis of Vowels and Fricatives at Varied Levels of Dysarthria Severity for Amyotrophic Lateral Sclerosis,"" ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 12767-12771, doi: 10.1109/ICASSP48485.2024.10448175."
"Advancing Speech Disorder Diagnostics: A Comprehensive Study on Dysarthria Classification with CNN,","This study article focuses on dysarthria, a speech problem that is often seen in patients with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early identification of dysarthria is crucial for successful treatments and better patient outcomes, since it has a substantial influence on communication ability. Using the TORGO database, which consists of 2000 samples of persons with and without dysarthria, spanning different genders, the research use a Convolutional Neural Network (CNN) to classify speech. The dataset includes both dysarthric and non-dysarthric individuals of both genders, with 500 samples each, captured during separate sessions. The CNN-based technique demonstrates an impressive 96% accuracy in differentiating between dysarthric and non-dysarthric instances, highlighting the promise of deep learning technology in assisting with the early detection of dysarthria and enabling prompt therapies for afflicted people. This study makes significant contributions to the advancement of diagnostic capacities and presents a potential opportunity to improve the quality of life for those with speech difficulties.","keywords: {Deep learning;Cerebral palsy;Technological innovation;Accuracy;Databases;Medical treatment;Market research;Artificial Intelligence;Deep Learning;Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10673651&isnumber=10673418,"K. Mittal, K. Singh Gill, P. Aggarwal, R. Singh Rawat and G. Sunil, ""Advancing Speech Disorder Diagnostics: A Comprehensive Study on Dysarthria Classification with CNN,"" 2024 Asia Pacific Conference on Innovation in Technology (APCIT), MYSORE, India, 2024, pp. 1-5, doi: 10.1109/APCIT62007.2024.10673651."
"Progressing Speech Disorder Identification: A Thorough Investigation into Dysarthria Categorization Utilizing ResNet50 CNN,","The speech issue of dysarthria, which is frequently observed in individuals with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS), is the subject of this research article. Timely detection of dysarthria is essential for effective interventions and improved patient results, as it significantly impacts communication proficiency. The study employs the TORGO database, which has 2000 samples of individuals with and without dysarthria, encompassing various genders. The researchers utilize a ResNet50 Convolutional Neural Network (CNN) to categorize speech. The dataset comprises individuals of both genders, encompassing both dysarthric and non-dysarthric individuals. Each group consists of 500 samples, which were recorded over distinct sessions. The CNN-based technique achieves a remarkable 96% accuracy in distinguishing between dysarthric and non-dysarthric instances. This underscores the potential of deep learning technology in aiding the early identification of dysarthria and facilitating timely treatments for affected individuals. This study provides valuable contributions to the enhancement of diagnostic capabilities and offers a potential avenue for enhancing the quality of life for individuals experiencing speech impairments.","keywords: {Deep learning;Cerebral palsy;Accuracy;Databases;Medical treatment;Speech enhancement;Robustness;Artificial Intelligence;Deep Learning;ResNet50 Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10581063&isnumber=10580998,"M. Singla, K. S. Gill, D. Upadhyay and S. Devliyal, ""Progressing Speech Disorder Identification: A Thorough Investigation into Dysarthria Categorization Utilizing ResNet50 CNN,"" 2024 International Conference on Intelligent Systems for Cybersecurity (ISCS), Gurugram, India, 2024, pp. 1-5, doi: 10.1109/ISCS61804.2024.10581063."
"Enhancing the Diagnosis of Speech Disorders: An In-Depth Investigation into Dysarthria Classification Using the ResNet18 Model,","This research article concentrates on dysarthria, a speech impediment commonly observed in individuals with conditions such as cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early detection of dysarthria is essential for effective interventions and improved patient outcomes, as it significantly impacts communication abilities. Employing the TORGO database, comprising 2000 samples from individuals with and without dysarthria, encompassing diverse genders, the study utilizes a Convolutional Neural Network (ResNet18 Model) for speech classification. The dataset encompasses both dysarthric and non-dysarthric subjects of both genders, with 500 samples each, recorded in separate sessions. The ResNet18 Model-based approach demonstrates an impressive 96% accuracy in distinguishing between dysarthric and non-dysarthric instances, underscoring the potential of deep learning technology in aiding early dysarthria detection and facilitating timely interventions. This investigation significantly contributes to advancing diagnostic capabilities and presents a promising avenue to enhance the quality of life for individuals grappling with speech difficulties.","keywords: {Deep learning;Cerebral palsy;Accuracy;Databases;Speech enhancement;Market research;Robustness;Artificial Intelligence;Deep Learning;Convolutional Neural Network (ResNet18 Model);Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10625627&isnumber=10624740,"K. Mittal, K. S. Gill, K. Rajput and V. Singh, ""Enhancing the Diagnosis of Speech Disorders: An In-Depth Investigation into Dysarthria Classification Using the ResNet18 Model,"" 2024 IEEE International Conference on Information Technology, Electronics and Intelligent Communication Systems (ICITEICS), Bangalore, India, 2024, pp. 1-5, doi: 10.1109/ICITEICS61368.2024.10625627."
"Statistical Analysis of Speech Disorder Specific Features to Characterise Dysarthria Severity Level,","Poor coordination of the speech production subsystems due to any neurological injury or a neuro-degenerative disease leads to dysarthria, a neuro-motor speech disorder. Dysarthric speech impairments can be mapped to the deficits caused in phonation, articulation, prosody, and glottal functioning. With the aim of reducing the subjectivity in clinical evaluations, many automated systems are proposed in the literature to assess the dysarthria severity level using these features. This work aims to analyse the suitability of these features in determining the severity level. A detailed investigation is done to rank these features for their efficacy in modelling the pathological aspects of dysarthric speech, using the technique of paraconsistent feature engineering. The study used two dysarthric speech databases, UA-Speech and TORGO. It puts light into the fact that both the prosody and articulation features are best useful for dysarthria severity estimation, which was supported by the classification accuracies obtained on using different machine learning classifiers.","keywords: {Pathology;Statistical analysis;Databases;Estimation;Production;Machine learning;Signal processing;dysarthria severity estimation;paraconsistent feature engineering;statistical analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10095366&isnumber=10094560,"A. A. Joshy, P. N. Parameswaran, S. R. Nair and R. Rajan, ""Statistical Analysis of Speech Disorder Specific Features to Characterise Dysarthria Severity Level,"" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095366."
"Identification of Cerebellar Dysarthria with SISO Characterisation,","Quantitative identification of dysarthria plays a major role in the classification of its severity. This paper quantitatively analyses several components of cerebellar dysarthria. The methodology described in this study will be extended to other types of dysarthria via systematic analysis. The speech production model is characterized as a second-order singleinput and single-output (SISO), linear, time-invariant (LTI) system in our study. A comparative study on the behavior of the damping ratio and resonant frequency for dysarthric and non-dysarthric subjects is presented. The results are further analyzed using the Principal component analysis (PCA) technique to emphasize the variation and uncover strong patterns in the selected features. The effects of some other related factors like decay time and Q-factor are also highlighted.","keywords: {Speech;Damping;Feature extraction;Resonant frequency;Production;Diseases;Transfer functions;dysarthria;speechdisorder;damping;resonantfrequency;secondordermodel;decayrate;vocaltract},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8251336&isnumber=8251248,"B. Kashyap, D. Szmulewicz, P. N. Pathirana, M. Horne and L. Power, ""Identification of Cerebellar Dysarthria with SISO Characterisation,"" 2017 IEEE 17th International Conference on Bioinformatics and Bioengineering (BIBE), Washington, DC, USA, 2017, pp. 479-485, doi: 10.1109/BIBE.2017.000-8."
"CNN-Driven Innovations in Dysarthria Classification and Speech Disorder Management,","This research article addresses dysarthria, a speech disorder frequently observed in individuals with cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS). Early detection of dysarthria is essential for effective treatment and improved patient outcomes, as it significantly impacts communication abilities. Utilizing the TORGO database, which comprises 2000 speech samples from both dysarthric and non-dysarthric individuals across various genders, this study employs a Convolutional Neural Network (CNN) for classification. The dataset includes 500 samples each from both groups, collected in different sessions. The CNN approach achieves an impressive 96% accuracy in distinguishing dysarthric from non-dysarthric speech, showcasing the potential of deep learning technology in facilitating early dysarthria diagnosis and enabling timely interventions. This study makes substantial contributions to enhancing diagnostic capabilities and offers a promising avenue for improving the quality of life for individuals with speech disorders.","keywords: {Deep learning;Computers;Cerebral palsy;Technological innovation;Electric potential;Accuracy;Databases;Speech enhancement;Convolutional neural networks;Medical diagnosis;Artificial intelligence;deep learning;convolutional neural network (CNN) model;model training;dysarthria classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10739183&isnumber=10738866,"A. Kheterpal, K. S. Gill, D. Upadhyay and S. Devliyal, ""CNN-Driven Innovations in Dysarthria Classification and Speech Disorder Management,"" 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT), Greater Noida, India, 2024, pp. 1-5, doi: 10.1109/ICEECT61758.2024.10739183."
"Constant Q Transform for Audio-Visual Dysarthria Severity Assessment,","This paper describes an automatic dysarthria severity assessment system submitted for the MDSA2024 challenge. The system utilizes audio and visual features to classify the severity of dysarthria. We investigate the effectiveness of different acoustic feature combinations, such as Mel spectrogram, filterbank and constant Q transform. Different encoders for audio data processing are compared in the experiments. The results demonstrate that the Conformer encoder achieves superior performance compared to the Densenet encoder on pure audio features. We further explore model-level fusion by combining audio and visual embeddings, leading to improvements in precision’ recall, and accuracy metrics. The best-achieved score on the test set is 7.5579, with an individual F1-score of 0.7028. The findings suggest that the proposed system with Conformer encoder and model-level fusion holds promise for automatic dysarthria assessment.","keywords: {Measurement;Visualization;Accuracy;Filter banks;Transforms;Data processing;Acoustics;Data models;Spectrogram;dysarthria severity assessment;acoustic feature combination;multimodalities model-level fusion},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800051&isnumber=10799969,"G. Sun and L. Wang, ""Constant Q Transform for Audio-Visual Dysarthria Severity Assessment,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 146-150, doi: 10.1109/ISCSLP63861.2024.10800051."
"Dysarthria Detection with Deep Representation Learning for Patients with Parkinson’s Disease,","Dysarthria is a very common motor speech symptom in Parkinson’s disease impairing normal communications of patients. Detection of dysarthria could assist clinical diagnosis and intervention of Parkinson’s disease, provide monitoring approach for treatment-related side effects, and lead to effective speech therapy to prevent further communication and social deficits. Applying machine learning techniques to speech analysis for patients offers more resource-efficient, accessible and objective tools for screening and assessment of dysarthria. In this study, we constructed a multi-task speech dataset recorded from 600 participants with high data diversity to facilitate the development of detection models. We established a remote data acquisition and end-to-end prediction pipeline with deep representation learning, compared the performance with different feature-based learning and classification methods, and achieved a superior accuracy of over 90%. Different affecting factors were analyzed for model performance. Our proposed framework demonstrates the potential of developing and deploying an automated self-monitoring approach of dysarthria for patients with Parkinson’s disease, which could benefit a large-scale population and their disease managements.","keywords: {Representation learning;Speech analysis;Pipelines;Data acquisition;Medical treatment;Multitasking;Motors;Data models;Monitoring;Diseases;Parkinson’s disease;dysarthria;deep representation learning;speech;detection},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10781584&isnumber=10781494,"C. Zhang, C. Gong and Y. Sui, ""Dysarthria Detection with Deep Representation Learning for Patients with Parkinson’s Disease,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-5, doi: 10.1109/EMBC53108.2024.10781584."
"Automatic assessment of dysarthria severity level using audio descriptors,","Dysarthria is a motor speech impairment, often characterized by speech that is generally indiscernible by human listeners. Assessment of the severity level of dysarthria provides an understanding of the patient's progression in the underlying cause and is essential for planning therapy, as well as improving automatic dysarthric speech recognition. In this paper, we propose a non-linguistic manner of automatic assessment of severity levels using audio descriptors or a set of features traditionally used to define timbre of musical instruments and have been modified to suit this purpose. Multitapered spectral estimation based features were computed and used for classification, in addition to the audio descriptors for timbre. An Artificial Neural Network (ANN) was trained to classify speech into various severity levels within Universal Access dysarthric speech corpus and the TORGO database. An average classification accuracy of 96.44% and 98.7% was obtained for UA speech corpus and TORGO database respectively.","keywords: {Speech;Databases;Harmonic analysis;Estimation;Timbre;Speech recognition;Dysarthria;Severity level;Automatic assessment;Audio descriptors;Multi-taper},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7953122&isnumber=7951776,"C. Bhat, B. Vachhani and S. K. Kopparapu, ""Automatic assessment of dysarthria severity level using audio descriptors,"" 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), New Orleans, LA, USA, 2017, pp. 5070-5074, doi: 10.1109/ICASSP.2017.7953122."
"Automated Dysarthria Severity Classification Using Deep Learning Frameworks,","Dysarthria is a neuro-motor speech disorder that renders speech unintelligible, in proportional to its severity. Assessing the severity level of dysarthria, apart from being a diagnostic step to evaluate the patient's improvement, is also capable of aiding automatic dysarthric speech recognition systems. In this paper, a detailed study on dysarthia severity classification using various deep learning architectural choices, namely deep neural network (DNN), convolutional neural network (CNN) and long short-term memory network (LSTM) is carried out. Mel frequency cepstral coefficients (MFCCs) and its derivatives are used as features. Performance of these models are compared with a baseline support vector machine (SVM) classifier using the UA-Speech corpus and the TORGO database. The highest classification accuracy of 96.18% and 93.24% are reported for TORGO and UA-Speech respectively. Detailed analysis on performance of these models shows that a proper choice of a deep learning architecture can ensure better performance than the conventionally used SVM classifier.","keywords: {Deep learning;Support vector machines;Neural networks;Speech recognition;Signal processing;Reliability;Mel frequency cepstral coefficient;dysarthria;intelligibility;automatic assessment;deep learning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9287741&isnumber=9287310,"A. A. Joshy and R. Rajan, ""Automated Dysarthria Severity Classification Using Deep Learning Frameworks,"" 2020 28th European Signal Processing Conference (EUSIPCO), Amsterdam, Netherlands, 2021, pp. 116-120, doi: 10.23919/Eusipco47968.2020.9287741."
"Deep Learning Based Speech Recognition for Hyperkinetic Dysarthria Disorder,","Speech recognition is a technology that aims to transform human speech into text and has applications in a variety of fields, including information technology, healthcare, automobiles, and others. This research explores the advantages of employing deep learning methods to provide individualized assistance technology solutions for people with dysarthria. We proposed a Convolutional Temporal Bidirectional Network (CTBNet) model for translating Russian Hyperkinetic Dysarthria Disorder (RHDD) speech into text. CTBNet encodes input audio features using 1D convolution layers, and BidirectionalGRU (Bi-GRU) layers. The encoder effectively captures both short-term and long-term spatial temporal information. More importantly, CTBNet includes with an attention-CTC decoder, that is responsible for producing output texts. We utilized a dataset of 2797 recordings of RHDD speech, with a cumulative recorded duration of 2 hours and 33 minutes, for training purposes. The CTBNet model has achieved a 15% Character Error Rate (CER) and a 59% Word Error Rate (WER) on the dataset proposed. Our experimental findings show superior performance compared to state-of-the-art models, namely, 3xCNN, and 3xLSTM, when evaluated on the same dataset.","keywords: {Training;Deep learning;Accuracy;Error analysis;Biological system modeling;Speech recognition;Transforms;dysarthria;recognition;encoder-decoder;deep learning;attention mechanism},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10584052&isnumber=10583970,"A. M. Hashan, C. R. Dmitrievich, M. A. Valerievich, D. D. Vasilyevich, K. N. Alexandrovich and B. B. Andreevich, ""Deep Learning Based Speech Recognition for Hyperkinetic Dysarthria Disorder,"" 2024 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT), Yekaterinburg, Russian Federation, 2024, pp. 012-015, doi: 10.1109/USBEREIT61901.2024.10584052."
"Developing a Hyperkinetic Dysarthria Speech Classification System using Residual Learning,","Hyperkinetic dysarthria abnormalities are exceedingly widespread across the worldwide populace. One of the most effective methodologies for disordered audio speech identification is classical deep learning technique. However, the efficiency of classification is impacted by the limitations imposed by the quality of the training dataset, such as expense and limited resources, data imbalance, and data annotation difficulties. Accordingly, we suggest implementing a residual learning framework (ResN et architecture) with various conditions and hyperparameters (batch size and learning rate) that improve the training process of deeper networks compared to earlier approaches. Pre-emphasis, windowing, and lifting techniques are applied to improve the quality of Mel-Frequency Cepstral Coefficients. In addition, we have introduced the hyperkinetic dysarthria speech dataset in the Russian language, consisting of 4 hours and 4 minutes of recorded speech. The empirical findings demonstrate that ResNet40 has an overall validation accuracy of 86.7%, surpassing other research models. It is anticipated that it will be recognized as an alternative diagnostic instrument for physicians in the future as research continues to progress.","keywords: {Training;Measurement;Deep learning;Accuracy;Cepstral analysis;Annotations;Instruments;Medical services;Real-time systems;Informatics;dysarthria;classification;feature extraction;deep learning;residual network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10805075&isnumber=10804862,"M. H. Antor, N. A. Khlebnikov and B. A. Bredikhin, ""Developing a Hyperkinetic Dysarthria Speech Classification System using Residual Learning,"" 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE), Novosibirsk, Russian Federation, 2024, pp. 1020-1023, doi: 10.1109/PIERE62470.2024.10805075."
"Multilingual Speaker-Invariant Dysarthria Severity Assessment Using Adversarial Domain Adaptation and Self-Supervised Learning,","Traditional assessments for dysarthria are subjective and time-consuming, highlighting the need for automated, objective approaches that can be scaled for remote and resource-constrained environments. This paper introduces an adversarial domain adaptation framework tailored for dysarthria severity assessment, addressing the challenges of high intra-class variability and limited availability of dysarthric speech data. By framing speaker variability as a domain adaptation problem, we utilise an adversarially trained feature extractor to derive speaker-invariant yet discriminatively powerful representations utilising speech features learned through self-supervised learning. Experiments on previously unseen and diverse speakers reveal that the proposed approach yields a 7.86% average improvement across multilingual datasets compared to traditional severity-discriminative training, outperforming competitive baselines by 12.33% on average. Additionally, the method inherently supports privacy-preserving applications by minimising reliance on speaker-specific information. The results demonstrate strong alignment with clinical assessments, reinforcing our model’s clinical relevance and effectiveness.","keywords: {Training;Deep learning;Adaptation models;Self-supervised learning;Signal processing;Feature extraction;Robustness;Multilingual;Speech processing;Overfitting;Dysarthria severity level classification;Adversarial domain adaptation;Self-supervised learning;Wav2vec},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10889800&isnumber=10887541,"L. Stumpf, B. Kadirvelu and A. A. Faisal, ""Multilingual Speaker-Invariant Dysarthria Severity Assessment Using Adversarial Domain Adaptation and Self-Supervised Learning,"" ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025, pp. 1-5, doi: 10.1109/ICASSP49660.2025.10889800."
"AI-Enabled Speech Monitoring for Dysarthria Detection in Consumer Electronics,","Speech is essential in human communication, and recent advancements in artificial intelligence (AI) have enabled new applications for voice data, particularly in health monitoring. Building on this progress, a home-based health detection system that utilizes voice signals provides an accessible method for users to independently identify and respond to conditions such as dysarthria, without the need for hospital or clinic visits. In this study, we developed and evaluated convolutional neural network (CNN) models to classify audio data from both healthy individuals and individuals with dysarthria. The dataset comprised command-based speech samples, which were segmented into sentence-specific input. Preprocessing steps included segmentation, downsampling, and feature extraction tailored for CNN input. The models exhibited high potential in distinguishing between normal and dysarthric states, with the most effective model achieving an f1-score of 96.18±4.7% and an accuracy of 96.01±5.02%. Furthermore, sentence structure and composition significantly influenced model performance, with specific sentences demonstrating higher classification accuracy. These findings suggest that AI-driven speech analysis may support detecting and managing specific health conditions in non-clinical environments. However, further work is needed to address data imbalance and sentence variability limitations. Future research could expand on real-world applications, particularly in settings with limited traditional health monitoring.","keywords: {Accuracy;Speech analysis;Hospitals;Data models;Convolutional neural networks;Reliability;Object recognition;Artificial intelligence;Monitoring;Consumer electronics;Audio classification;convolutional neural network;deep learning;dysarthria detection;healthcare},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10929844&isnumber=10929768,"S. Lee et al., ""AI-Enabled Speech Monitoring for Dysarthria Detection in Consumer Electronics,"" 2025 IEEE International Conference on Consumer Electronics (ICCE), Las Vegas, NV, USA, 2025, pp. 1-4, doi: 10.1109/ICCE63647.2025.10929844."
"Dysarthria Severity Classification Using Phase Based Features of LP Residual,","Classifying the severity of speech impairment due to dysarthria is crucial for optimizing care and enhancing communication abilities for affected individuals. This study explores the use of the Modified Group Delay Function (MGDF) of LP residual signal in classifying dysarthria severity-levels. Evaluations were conducted using standard UA-Speech and TORGO datasets. A stratified Convolutional Neural Network (CNN) with 5-fold cross-validation validated the results. Baseline features included Linear Frequency Cepstral Coefficients (LFCC), Mel Frequency Cepstral Coefficients (MFCC), and Whisper module. Key performance evaluation metrics were accuracy, precision, recall, and F1-score. Finally, the latency period was analyzed for practical deployment of the system, system’s ability to accurately recognize and process speech from any speaker, without needing to be specifically trained or adapted to individual voice characteristics.","keywords: {Performance evaluation;Asia;Speech recognition;Information processing;Speech enhancement;Delays;Convolutional neural networks;Character recognition;Mel frequency cepstral coefficient;Standards;Dysarthria;LP residual;Modified Group Delay Function},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10848959&isnumber=10848533,"R. S. Mannepalli, A. Pusuluri and H. A.Patil, ""Dysarthria Severity Classification Using Phase Based Features of LP Residual,"" 2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Macau, Macao, 2024, pp. 1-5, doi: 10.1109/APSIPAASC63619.2025.10848959."
"Automatic Early Detection of Dysarthria using Deep Neural Network,","Dysarthria is dis-coordination among articulatory muscles responsible for articulation in the production of speech. A person with dysarthric disorder cannot control their tongue, hence, are prone to slurred speech or swallowing words which lead to improper pronunciation and less intelligibility of speech. This work focuses on the automatic early detection of Dysarthria. To elucidate the main idea, it must be understood that the whole detection procedure revolves around the speech intelligibility of a person. It becomes an arduous job to distinguish between a healthy person's speech and a person who is slowly evolving with the disorder. The prime motive of the work is to provide a proper classification of the disorder in its earliest stages. The common words and phrases recorded in the Universal Access database are used for this work. Mel Frequency Cepstral Coefficients (MFCC) are extracted from both controlled and high intelligible speech data, and fed to a Deep Neural Network (DNN) classifier. MFCC fed to the DNN classifier has provided the best classification accuracy for the above experimental setup.","keywords: {Training;Recurrent neural networks;Tongue;Databases;Artificial neural networks;Production;Muscles;Dysarthria;speech intelligibility;Mel Frequency Cepstral Co-efficient;classifier;Deep Neural network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10200180&isnumber=10199221,"A. K, L. T N, S. U. Bhat, S. R and C. H M, ""Automatic Early Detection of Dysarthria using Deep Neural Network,"" 2023 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES), Tumakuru, India, 2023, pp. 1-4, doi: 10.1109/ICSSES58299.2023.10200180."
"Learning to Detect Dysarthria from Raw Speech,","Speech classifiers of paralinguistic traits traditionally learn from diverse hand-crafted low-level features, by selecting the relevant information for the task at hand. We explore an alternative to this selection, by learning jointly the classifier, and the feature extraction. Recent work on speech recognition has shown improved performance over speech features by learning from the waveform. We extend this approach to paralinguistic classification and propose a neural network that can learn a filterbank, a normalization factor and a compression power from the raw speech, jointly with the rest of the architecture. We apply this model to dysarthria detection from sentence-level audio recordings. Starting from a strong attention-based baseline on which mel-filterbanks outperform standard low-level descriptors, we show that learning the filters or the normalization and compression improves over fixed features by 10% absolute accuracy. We also observe a gain over OpenSmile features by learning jointly the feature extraction, the normalization, and the compression factor with the architecture. This constitutes a first attempt at learning jointly all these operations from raw audio for a speech classification task.","keywords: {Task analysis;Neural networks;Time-domain analysis;Speech recognition;Databases;Standards;Computational modeling;dysarthria;paralinguistic;classification;waveform;lstm},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8682324&isnumber=8682151,"J. Millet and N. Zeghidour, ""Learning to Detect Dysarthria from Raw Speech,"" ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 5831-5835, doi: 10.1109/ICASSP.2019.8682324."
"Modeling fundamental frequency dynamics in hypokinetic dysarthria,","Hypokinetic dysarthria (Hd), which often accompanies Parkinson's Disease (PD), is characterized by hypernasality and by compromised phonation, prosody, and articulation. This paper proposes automated methods for detection of Hd. Whereas most such studies focus on measures of phonation, this paper focuses on prosody, specifically on fundamental frequency (F0) dynamics. Prosody in Hd is clinically described as involving monopitch, which has been confirmed in numerous studies reporting reduced within-utterance pitch variability. We show that a new measure of F0 dynamics, based on a superpositional pitch model that decomposes the F0 contour into a declining phrase curve and (generally, single-peaked) accent curves, performs more accurate Hd vs. Control classification than simpler versions of the model or than conventional variability statistics.","keywords: {Speech;Feature extraction;Mathematical model;Accuracy;Foot;Equations;Support vector machines;Hypokinetic dysarthria;Parkinson's Disease;Pitch decomposition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7078586&isnumber=7078533,"M. S. E. Langarani and J. van Santen, ""Modeling fundamental frequency dynamics in hypokinetic dysarthria,"" 2014 IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoe, NV, USA, 2014, pp. 272-276, doi: 10.1109/SLT.2014.7078586."
"Automatic detection of speech disorder in dysarthria using extended speech feature extraction and neural networks classification,","This paper presents an automatic detection of Dysarthria, a motor speech disorder, using extended speech features called Centroid Formants. Centroid Formants are the weighted averages of the formants extracted from a speech signal. This involves extraction of the first four formants of a speech signal and averaging their weighted values. The weights are determined by the peak energies of the bands of frequency resonance, formants. The resulting weighted averages are called the Centroid Formants. In our proposed methodology, these centroid formants are used to automatically detect Dysarthric speech using neural network classification technique. The experimental results recorded after testing this algorithm are presented. The experimental data consists of 200 speech samples from 10 Dysarthric speakers and 200 speech samples from 10 age-matched healthy speakers. The experimental results show a high performance using neural networks classification. A possible future research related to this work is the use of these extended features in speaker identification and recognition of disordered speech.","keywords: {Dysarthria;speech disorder;Centroid Formants;Neural Networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8361563&isnumber=8329307,"T. B. Ijitona, J. J. Soraghan, A. Lowit, G. Di-Caterina and H. Yue, ""Automatic detection of speech disorder in dysarthria using extended speech feature extraction and neural networks classification,"" IET 3rd International Conference on Intelligent Signal Processing (ISP 2017), London, 2017, pp. 1-6, doi: 10.1049/cp.2017.0360."
"Analysis of Time Domain Features of Dysarthria Speech,","In general, Speech can be described as the ability to express thoughts and feelings by articulating sounds in order to communicate with the person who understand the speech and accordingly interpret their action. The types of communication disorders that affects a person’s ability to produce speech sounds due to the lack of control over motor functions and articulators are speech disorders. Many people of varying age groups suffer from speech disorders and require early treatment in order to correct them. This paper analyses the time domain features such as jitter and shimmer that are extracted from the speech samples of dysarthria and healthy people. This is done by looking for dissimilarities between the different speech features preset in the normal healthy person and the disordered one by employing steps such as pre-processing followed by feature extraction.","keywords: {Jitter;Feature extraction;Time-domain analysis;Pitch;jitter;shimmer;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9076507&isnumber=9076357,"A. Singh, A. Kittur, K. Sonawane, A. Singh and S. Upadhya, ""Analysis of Time Domain Features of Dysarthria Speech,"" 2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC), Erode, India, 2020, pp. 122-125, doi: 10.1109/ICCMC48092.2020.ICCMC-00025."
"Emotional Communication Assist Interface App for People with Dysarthria,","Text to speech (TTS) helps people who have speech disorders communicate with other people. The current TTS can express emotion, but this requires extra time and effort. We developed a TTS application by which users can render messages with emotion with ease by changing the pitch angle of a smartphone. We designed two types of TTS: free input and phrase selection. Also, we calculated the TTS editing parameters for each degree of emotion on the basis of the Online Game Voice Chat Corpus (OGVC).","keywords: {Communication aids;Conferences;Natural languages;Games;Consumer electronics;TTS;dysarthria;emotional message;smartphone;pitch angle},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9291830&isnumber=9291703,"T. Kawase and M. Iwaki, ""Emotional Communication Assist Interface App for People with Dysarthria,"" 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), Kobe, Japan, 2020, pp. 568-571, doi: 10.1109/GCCE50665.2020.9291830."
"Fuzzy Expectation Maximization Phoneme Prediction in Diffusion Model-based Dysarthria Voice Conversion,","In this paper proposed an effective fuzzy expectation-maximization phoneme prediction method in diffusion model-based dysarthria voice conversion (FEMPPDM-DVC) which is accessible to (i) training without parallel data (ii) converting a longer duration of the audio data (iii) preserves speaker identity. By integrating Fuzzy Expectation-Maximization (FEM) clustering and Diffusion model-based dysarthria voice conversion approach, the proposed method is able gradually generate normal utterances. Feature extraction is performed using Mel-frequency cepstral coefficients (MFCCs), a robust method in acoustic feature extraction in Text-to-Speech systems. Ensures the effectiveness and accuracy, through repeated parameter adjustment using the FEM clustering algorithm. The conversion network is a diffusion model-based structure, which can convert normal utterances to dysarthria utterances in the forward diffusion process. After forward diffusion process, the dysarthria utterance will go through a reverse diffusion process to convert dysarthria voice to normal utterance. Once converted, a GAN-based vocoder is applied to convert mel-frequency spectrogram to waveform. Objective evaluation is conducted on the Saarbrücken Voice Database (SVD) dataset show that FEMDDPM-DVC is able to improve the intelligibility and naturalness of dysarthria utterances in 15 kinds of dysarthria voice. The proposed method is compared with five other dysarthria voice conversion methods, show that the proposed method has the most performance among other method.","keywords: {Training;Adaptation models;Vocoders;Diffusion processes;Predictive models;Feature extraction;Transformers;voice conversion;diffusion probabilistic model;dysarthria voice;fuzzy expectation maximization;phoneme prediction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10661368&isnumber=10661367,"G. Lin, W. Hsu, G. Liu and S. Chen, ""Fuzzy Expectation Maximization Phoneme Prediction in Diffusion Model-based Dysarthria Voice Conversion,"" 2024 International Conference on Fuzzy Theory and Its Applications (iFUZZY), Kagawa, Japan, 2024, pp. 1-4, doi: 10.1109/iFUZZY63051.2024.10661368."
"Revolutionary Dysarthria Analysis Through Convolutional Neural Networks,","Dysarthria occurs when there are impairments in the muscles necessary for speech, including those in the lips, tongue, voice box, and breathing muscles. This condition can result in unclear speech, even in individuals who are skilled communicators. Dysarthria can range from mild speech difficulties, where speech is hard to understand, to severe cases, where speech is completely unclear. A method for distinguishing between different types of speech involves using recordings and advanced computational techniques such as convolutional neural networks (CNNs). The strengths of CNNs are leveraged to construct a highly effective model for detecting dysarthria in speech samples. The dataset utilized includes recordings from individuals with and without dysarthria. The results indicate that the CNN model performs exceptionally well, achieving precision, recall, and F1-scores of 0.99 for both categories. With an accuracy of 0.99, the model demonstrates a high level of proficiency in differentiating between speech affected by dysarthria and normal speech. These findings highlight the effectiveness and utility of this CNN-based approach in diagnosing and assessing dysarthria. It offers valuable potential for assisting healthcare professionals and enhancing personalized speech therapy interventions.","keywords: {Training;Accuracy;Tongue;Computational modeling;Medical treatment;Speech recognition;Speech enhancement;Muscles;Recording;Convolutional neural networks;Dysarthria;Speech Disorder;Convolutional Neural Networks (CNNs);Audio Classification;Speech Recognition;Machine Learning;Neural Networks;Speech Therapy;Automated Diagnosis;Clinical Applications},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10912315&isnumber=10912086,"A. Verma, K. S. Gill, N. Thapliyal and R. S. Rawat, ""Revolutionary Dysarthria Analysis Through Convolutional Neural Networks,"" 2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT), Faridabad, India, 2024, pp. 832-837, doi: 10.1109/ICAICCIT64383.2024.10912315."
"A Study of Speech Recognition Techniques for Dysarthria Speeches Based on Digit Recognition,","With the introduction of speech recognition technology into people's lives, the application of which is no longer limited to ordinary people, but begins to target special populations, such as patients with dysarthria. It has become a major research challenge to build a high-performance speech recognition system for patients with dysarthria. To address this problem, in this paper the speech data of four patients with dysarthria and four healthy speakers on digital commands are firstly collected, secondly several speech recognition experiments are separately conducted by using Aishell-2 and Wenetspeech, two pre-trained models based on the WeNet open-source architecture, as well as the four major commercial recognition API systems, at the end a method to optimize the dysarthria data by using the so-vits-svc tool is proposed. The experiments show that the current state-of-the-art speech recognition model has a high recognition rate for speech data from the general population, but there is still room for improvement in the recognition performance of speech data from individuals with dysarthria. Later the optimization method for the dysarthria data proposed in this paper effectively improves the quality of speech data. For the two above open-source pre-trained models, the recognition rates have increased by 60% and 67.64 % compared to the original data, respectively.","keywords: {Human computer interaction;Analytical models;Sequences;Error analysis;Optimization methods;Speech recognition;Data models;dysarthria;automatic speech recognition;speech conversion;open source models;commercial models;pre-trained models;digital data recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10675073&isnumber=10674745,"J. Yang, H. Guo, X. Xu and H. Bu, ""A Study of Speech Recognition Techniques for Dysarthria Speeches Based on Digit Recognition,"" 2024 5th International Conference on Electronic Communication and Artificial Intelligence (ICECAI), Shenzhen, China, 2024, pp. 382-386, doi: 10.1109/ICECAI62591.2024.10675073."
"Fractal features for automatic detection of dysarthria,","Amytrophic lateral sclerosis (ALS) is an incurable neurodegenerative disease. Difficulty articulating speech, dysarthria, is a common early symptom of ALS. Detecting dysarthria currently requires manual analysis of several different speech tasks by pathology experts. This is time consuming and can lead to misdiagnosis. Many existing automatic classification approaches require manually preprocessing recordings, separating individual spoken utterances from a repetitive task. In this paper, we propose a fully automated approach which does not rely on manual preprocessing. The proposed method uses novel features based on fractal analysis. Acoustic and associated articulatory recordings of a standard speech diagnostic task, the diadochokinetic test (DDK), are used for classification. This study's experiments show that this approach attains 90.2% accuracy with 94.2% sensitivity and 85.1% specificity.","keywords: {Fractals;Speech;Jitter;Feature extraction;Time series analysis;Acoustics;Lips},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7897299&isnumber=7897179,"T. Spangler, N. V. Vinodchandran, A. Samal and J. R. Green, ""Fractal features for automatic detection of dysarthria,"" 2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), Orlando, FL, USA, 2017, pp. 437-440, doi: 10.1109/BHI.2017.7897299."
"Assessing Dysarthria severity using global statistics and boosting,","A new method for automatic assessment of Dysarthria severity is described. It uses the forward selection method (FSM) on global statistics of low-complexity features to find effective feature sets. FSM is embedded in a boosting algorithm that combines multiple weak classifiers to achieve a single strong classifier. Unlike standard boosting, this uses nonlinear class boundaries and unique feature sets per iteration. Results on a 39 speaker dysarthria database are described.","keywords: {Boosting;Speech;Classification algorithms;Training;Training data;Speech processing;Frequency measurement},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6190184&isnumber=6189941,"A. DeMino, R. Kubichek and K. Caves, ""Assessing Dysarthria severity using global statistics and boosting,"" 2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR), Pacific Grove, CA, USA, 2011, pp. 1103-1106, doi: 10.1109/ACSSC.2011.6190184."
"Dysarthria diagnosis via respiration and phonation,",This report discusses the implementation of a computerized application - the Computerised Frenchay Dysarthria Assessment Procedure (CFDA) - which uses digital signal processing (DSP) techniques to objectively evaluate digitised speech recordings in order to detect any symptoms of dysarthria (a type of motor speech disorder). This investigation focuses specifically on two CFDA diagnostic sub-applications which assess a patient's ability to maintain a prolonged exhalation (the “Respiration at Rest” task) as well as execute a steady state phonation (i.e. the “Sustained Phonation” task). It is demonstrated that combining the functionality of these two sub-applications substantially increases their diagnostic effectiveness in the context of identifying a particular variety of dysarthria known as spastic dysarthria. It is further demonstrated that certain patterns of energy fluctuations are closely correlated with manifestations of spastic dysarthria. The CFDA is intended for use in real-world conditions to assist speech and language therapists when conducting clinical evaluations.,"keywords: {Speech;Steady-state;Digital signal processing;Fluctuations;Speech processing;Context;Production;DSP;diagnosis;dysarthria;respiration;phonation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7344537&isnumber=7344420,"J. Carmichael, ""Dysarthria diagnosis via respiration and phonation,"" 2015 International Conference and Workshop on Computing and Communication (IEMCON), Vancouver, BC, Canada, 2015, pp. 1-5, doi: 10.1109/IEMCON.2015.7344537."
"Deep Learning Based Dysarthria Detection: A Comprehensive Approach,","Making a model that can analyze numerous input features and predict whether a person will develop dysarthria is the first step in utilizing machine learning to forecast diseases like dysarthria. A motor speech disorder called dysarthria can be caused by a variety of underlying conditions, such as degenerative disorders, traumatic brain injuries, or neurological disorders. The authors used the 2000 audio signals from males and females with and without dysarthria from the TORGO data set. To extract the important features from the audio signals, the authors used the MFCC approach. To determine if dysarthria is present or absent, a machine learning model or algorithm will be fed with these MFCC coefficients.","keywords: {Training;Machine learning algorithms;Predictive models;Feature extraction;Prediction algorithms;Motors;Robustness;Mel frequency cepstral coefficient;Testing;Overfitting;Dysarthria;motor speech impairment;degenerative diseases;brain traumas;neurological disorders;TORGO;MFCC},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10892620&isnumber=10892612,"S. Patil, S. Borude, C. Budhwani, A. Bhandare and P. Bhujbal, ""Deep Learning Based Dysarthria Detection: A Comprehensive Approach,"" 2024 Fourth International Conference on Multimedia Processing, Communication & Information Technology (MPCIT), Shivamogga, India, 2024, pp. 262-266, doi: 10.1109/MPCIT62449.2024.10892620."
"Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition,","In this paper, we propose a deep learning-based algorithm to improve the performance of automatic speech recognition (ASR) systems for aphasia, apraxia, and dysarthria speech by utilizing electroencephalography (EEG) features recorded synchronously with aphasia, apraxia, and dysarthria speech. We demonstrate a significant decoding performance improvement by more than 50% during test time for isolated speech recognition task and we also provide preliminary results indicating performance improvement for the more challenging continuous speech recognition task by utilizing EEG features. The results presented in this paper show the first step towards demonstrating the possibility of utilizing non-invasive neural signals to design a real-time robust speech prosthetic for stroke survivors recovering from aphasia, apraxia, and dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will be released to the public to help further advance this interesting and crucial research.","keywords: {Stroke (medical condition);Electroencephalography;Real-time systems;Biology;Decoding;Task analysis;Prosthetics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9629802&isnumber=9629471,"G. Krishna et al., ""Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition,"" 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Mexico, 2021, pp. 6008-6014, doi: 10.1109/EMBC46164.2021.9629802."
"Hmm-Based and Svm-Based Recognition of the Speech of Talkers With Spastic Dysarthria,","This paper studies the speech of three talkers with spastic dysarthria caused by cerebral palsy. All three subjects share the symptom of low intelligibility, but causes differ. First, all subjects tend to reduce or delete word-initial consonants; one subject deletes all consonants. Second, one subject exhibits a painstaking stutter. Two algorithms were used to develop automatic isolated digit recognition systems for these subjects. HMM-based recognition was successful for two subjects, but failed for the subject who deletes all consonants. Conversely, digit recognition experiments assuming a fixed word length (using SVMs) were successful for two subjects, but failed for the subject with the stutter.","keywords: {Hidden Markov models;Speech recognition;Automatic speech recognition;Birth disorders;Natural languages;Microphone arrays;Speech analysis;Muscles;Brain injuries;Automatic control},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1660840&isnumber=34759,"M. Hasegawa-Johnson, J. Gunderson, A. Perlman and T. Huang, ""Hmm-Based and Svm-Based Recognition of the Speech of Talkers With Spastic Dysarthria,"" 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, Toulouse, France, 2006, pp. III-III, doi: 10.1109/ICASSP.2006.1660840."
"Design of a dysarthria classifier using global statistics of speech features,","Dysarthria is a neurological disorder in which the speech production system is impaired. There are five main types of dysarthrias depending on the location of the lesion in the nervous system. There is evidence suggesting a relationship between the location of the lesion and the resulting speech characteristics. This paper describes a non-intrusive classifier to identify the dysarthria type in a person using global statistics, e.g., mean, variance, etc., of speech features. A tree-based classifier was developed using multiple low-level maximum likelihood classifiers as inputs. An error of 10.5% was achieved in the classification of three types of dysarthrias.","keywords: {Statistics;Speech analysis;Classification tree analysis;Lesions;Decision trees;Neural networks;Cepstral analysis;Hidden Markov models;Speech coding;Speech processing;Speech disorders;dysarthria diagnosis;objective speech quality analysis;global speech statistics;decision trees},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5495563&isnumber=5494886,"M. V. Mujumdar and R. F. Kubichek, ""Design of a dysarthria classifier using global statistics of speech features,"" 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, Dallas, TX, USA, 2010, pp. 582-585, doi: 10.1109/ICASSP.2010.5495563."
"Dysarthria Speech Disorder Classification Using Traditional and Deep Learning Models,","Dysarthria is a motor speech disorder that results in speech difficulties due to the weakness of associated muscles. This unclear speech makes it difficult for dysarthric patients to present himself understood. This neurological limitation is usually occurs due to damages to the brain or central nervous system. Speech therapy can be effectively employed to enhance the range and consistency of voice production and improve intelligibility and communicative effectiveness. Assessing the degree of severity of dysarthria provides vital information on the patient's progress which inturn assists pathologists in arriving at a treatment plan that includes developing automated voice recognition system suitable for dysarthria patients. This work performs an exhaustive study on dysarthria severity level classification using deep neural network (DNN) and convolution neural network (CNN) architectures. Mel Frequency Cepstral Coefficients (MFCCs) and their derivatives constitute feature vectors for classification. Using the UA-Speech database, the performance metrics of DNN/CNN based learning models have been compared to baseline classifiers like support vector machine (SVM) and Random Forest (RF). The highest classification accuracy of 97.6\% is reported for DNN under UA speech database. A detailed examination of the performance from the models discussed above reveal that appropriate choice of deep learning architecture ensures better results than traditional classifiers like SVM and Random Forest.","keywords: {Support vector machines;Radio frequency;Deep learning;Databases;Computational modeling;Speech enhancement;Indexes;CNN;deep learning;DNN;Dysarthria;motor speech disorder;Random Forest;SVM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10157285&isnumber=10156874,"M. Suresh, R. Rajan and J. Thomas, ""Dysarthria Speech Disorder Classification Using Traditional and Deep Learning Models,"" 2023 Second International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT), Trichirappalli, India, 2023, pp. 01-06, doi: 10.1109/ICEEICT56924.2023.10157285."
"Harnessing Deep Learning Techniques for Dysarthria Detection,","Dysarthria, a motor speech disorder resulting from neurological impairments. This study explores various approaches for the automated detection of dysarthria, integrating both traditional and emerging technologies. Speech assessments by acoustic analysis, machine learning models, and deep learning techniques are considered. Machine learning models are trained on datasets containing normal and dysarthric speech for males and females, while deep learning methods, including convolutional and recurrent neural networks, are employed to automatically learn features from speech data. Our goal is to develop a reliable and accessible dysarthria detection system thatcomplements the expertise of healthcare professionals. Validation using diverse datasets is emphasized, acknowledging the importance of early detection and intervention in improving outcomes for individuals with dysarthria.","keywords: {Deep learning;Analytical models;Recurrent neural networks;Collaboration;Medical services;Motors;Feature extraction;Acoustic analysis;Convolutional neural net-works;Deep learning techniques;Dysarthria;Machine learning models;Recurrent neural networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10527645&isnumber=10527410,"P. Varalakshmi, V. S. Sanmitha, S. B. Dhivyadharshni and S. Saravanan, ""Harnessing Deep Learning Techniques for Dysarthria Detection,"" 2024 Third International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS), Krishnankoil, Virudhunagar district, Tamil Nadu, India, 2024, pp. 01-06, doi: 10.1109/INCOS59338.2024.10527645."
"Next-Gen Speech Disorder Diagnostics: CNN Methods for Dysarthria Classification,","Neurological disorders such as amyotrophic lateral sclerosis (ALS) and cerebral palsy (CP) are common causes of dysarthria, the subject of this research article. Dysarthria has a significant impact on communication capacity, making early diagnosis essential for effective therapies and improved patient outcomes. The research used a Convolutional Neural Network (CNN) to categorise speech using the TORGO database, which includes 2000 samples of individuals with and without dysarthria, representing different genders. There are 500 samples from each gender in the collection, representing dysarthric and non-dysarthric people who were recorded during different sessions. Highlighting the promise of deep learning technology in aiding early identification of dysarthria and enabling rapid therapy for affected individuals, the CNN-based approach exhibits an amazing 96% accuracy in discriminating between dysarthric and non-dysarthric cases. This study offers a promising chance to enhance the quality of life for those with speech impairments while also making substantial contributions to the development of diagnostic capabilities.","keywords: {Deep learning;Neurological diseases;Cerebral palsy;Accuracy;Databases;Medical treatment;Speech enhancement;Convolutional neural networks;Medical diagnosis;Stress;Artificial Intelligence;Deep Learning;Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10696797&isnumber=10695957,"G. Verma, K. S. Gill, M. Kumar and R. Rawat, ""Next-Gen Speech Disorder Diagnostics: CNN Methods for Dysarthria Classification,"" 2024 First International Conference on Pioneering Developments in Computer Science & Digital Technologies (IC2SDT), Delhi, India, 2024, pp. 365-369, doi: 10.1109/IC2SDT62152.2024.10696797."
"Advancing Speech Disorder Diagnostics: CNN Innovations in Dysarthria Recognition,","Neurological disorders such as amyotrophic lateral sclerosis (ALS) and cerebral palsy (CP) are common causes of dysarthria, the subject of this research article. The capacity to communicate is greatly affected by dysarthria, thus early diagnosis is critical for effective therapies and improved patient outcomes. The study employed a Convolutional Neural Network (CNN) for speech classification using the TORGO database, which includes 2000 samples of individuals with and without dysarthria, representing different genders. With 500 samples each, collected over distinct sessions, the dataset contains both dysarthric and non-dysarthric people of both sexes. Distinguishing between dysarthric and non-dysarthric occurrences, the CNN-based approach achieves an astonishing 96% accuracy. This shows how deep learning technology may help with early dysarthria identification and enable rapid therapy for affected individuals. The findings of this study have important implications for the development of diagnostic tools and may lead to better living conditions for those who struggle with speaking.","keywords: {Deep learning;Neurological diseases;Cerebral palsy;Technological innovation;Accuracy;Databases;Medical treatment;Speech recognition;Convolutional neural networks;Stress;Artificial Intelligence;Deep Learning;Convolutional Neural Network (CNN) model;Model Training;Dysarthria Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10730941&isnumber=10730804,"A. Kaur, K. S. Gill, M. Kumar and R. Rawat, ""Advancing Speech Disorder Diagnostics: CNN Innovations in Dysarthria Recognition,"" 2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC), Gwalior, India, 2024, pp. 766-770, doi: 10.1109/AIC61668.2024.10730941."
"A preliminary study on self-care telemonitoring of dysarthria in spinal muscular atrophy,","Spinal muscular atrophy (SMA) is a rare neuromuscular disease which may cause impairments in oro-facial musculature. Most of the individuals with SMA present bulbar signs such as flaccid dysarthria which mines their abilities to speak and, as consequence, their psychic balance. To support clinicians, recent work has demonstrated the feasibility of video-based techniques for assessing the oro-facial functions in patients with neurological disorders such as amyotrophic lateral sclerosis. However, no work has so far focused on automatic and quantitative monitoring of dysarthria in SMA. To overcome limitations this work’s aim is to propose a cloud-based store-and-forward telemonitoring system for automatic and quantitative evaluation of oro-facial muscles in individuals with SMA. The system integrates a convolutional neural network (CNN) aimed at identifying the position of facial landmarks from video recordings acquired via a web application by an SMA patient.Clinical relevance— The proposed work is in the preliminary stage, but it represents the first step towards a better understanding of the bulbar-functions’ evolution in patients with SMA.","keywords: {Atrophy;Neurological diseases;Evolution (biology);Neuromuscular;Medical services;Stroke (medical condition);Convolutional neural networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10340908&isnumber=10339939,"L. Migliorelli et al., ""A preliminary study on self-care telemonitoring of dysarthria in spinal muscular atrophy,"" 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Sydney, Australia, 2023, pp. 1-4, doi: 10.1109/EMBC40787.2023.10340908."
"Automatic And Perceptual Discrimination Between Dysarthria, Apraxia of Speech, and Neurotypical Speech,","Automatic techniques in the context of motor speech disorders (MSDs) are typically two-class techniques aiming to discriminate between dysarthria and neurotypical speech or between dysarthria and apraxia of speech (AoS). Further, although such techniques are proposed to support the perceptual assessment of clinicians, the automatic and perceptual classification accuracy has never been compared. In this paper, we investigate a three-class automatic technique and a set of handcrafted features for the discrimination of dysarthria, AoS and neurotypical speech. Instead of following the commonly used One-versus-One or One-versus-Rest approaches for multi-class classification, a hierarchical approach is proposed. Further, a perceptual study is conducted where speech and language pathologists are asked to listen to recordings of dysarthria, AoS, and neurotypical speech and decide which class the recordings belong to. The proposed automatic technique is evaluated on the same recordings and the automatic and perceptual classification performance are compared. The presented results show that the hierarchical classification approach yields a higher classification accuracy than baseline One-versus-One and One-versus-Rest approaches. Further, the presented results show that the automatic approach yields a higher classification accuracy than the perceptual assessment of speech and language pathologists, demonstrating the potential advantages of integrating automatic tools in clinical practice.","keywords: {Databases;Conferences;Support vector machine classification;Tools;Signal processing;Feature extraction;Acoustics;dysarthria;apraxia of speech;support vector machine;hierarchical classification;perceptual classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414283&isnumber=9413350,"I. Kodrasi, M. Pernon, M. Laganaro and H. Bourlard, ""Automatic And Perceptual Discrimination Between Dysarthria, Apraxia of Speech, and Neurotypical Speech,"" ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Toronto, ON, Canada, 2021, pp. 7308-7312, doi: 10.1109/ICASSP39728.2021.9414283."
"Adversarial Auto-Encoders Based Model for Classification of Speech Dysarthria,","Communication is effective based on various parameters, out of which phonetic or oral communication plays a vital role. Slurred speech or improper speech will lead to misunderstanding in speech, which could toss up any situation. There are many people, ranging from children to adults, who are affected with slurred speech, which is technically termed as Speech Dysarthria, a disease which tampers effective oral communication. Distinguishing between people affected with dysarthria and people with normal speech will be tedious process manually. Machine Learning (ML), and Artificial Intelligence (AI), can be pitched in to solve the problem. There are existing methodologies which classify people affected with speech dysarthria and people who communicate in a normal way. Some of the existing technologies used are Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and so on. This paper aims at distinguishing between people affected with speech dysarthria and people with normal speech, using Adversarial Auto Encoders (AAE), a model which has its roots from Variational Auto Encoders (VAE) and Generative Adversarial Networks (GAN). This paper brings out a good result and proves to be effective.","keywords: {Recurrent neural networks;Computational modeling;Machine learning;Phonetics;Speech;Generative adversarial networks;Distance measurement;Convolutional neural networks;Speech processing;Diseases;Speech Dysarthria;Artificial Intelligence;Machine Learning;Adversarial Auto Encoders;Variational Auto Encoders;Generative Adversarial Networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10724410&isnumber=10723316,"V. Kanchana Devi, R. Sreenivas, E. Umamaheshwari and N. Bacanin, ""Adversarial Auto-Encoders Based Model for Classification of Speech Dysarthria,"" 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT), Kamand, India, 2024, pp. 1-7, doi: 10.1109/ICCCNT61001.2024.10724410."
"Summary of Low-Resource Dysarthria Wake-Up Word Spotting Challenge,","In recent years, the rapid advancement and widespread adoption of speech technology have made smart home systems a common feature in many households. However, individuals with dysarthria face difficulties using these technologies due to inconsistent speech patterns. This paper summarizes the Low-Resource Dysarthria Wake-Up Word Spotting (LRDWWS) Challenge at SLT 2024, which aimed to develop effective voice wake-up systems for individuals with dysarthria. The challenge attracted 25 teams from 4 countries, with 7 teams submitting results and 5 providing detailed system descriptions. This paper presents an overview of the dataset, evaluation metrics, and key innovations from participating teams. Our findings highlight the potential of these systems to enhance the accessibility and usability of smart home technologies for individuals with dysarthria. The challenge results underscore the importance of developing specialized solutions to meet the unique needs of this user group.","keywords: {Measurement;Training;Technological innovation;Pathology;Smart homes;Speech enhancement;Feature extraction;Data models;Usability;Faces;Dysarthria;Wake-up Word Spotting;Speaker-dependent Systems;Speech Disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832239&isnumber=10830793,"M. Gao et al., ""Summary of Low-Resource Dysarthria Wake-Up Word Spotting Challenge,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 592-599, doi: 10.1109/SLT61566.2024.10832239."
"Transformer-based Transfer Learning for Enhanced Speech Dysarthria Severity Assessment,","Dysarthria, a neuromuscular communications disorder presented with impaired pronunciation, is a daunting task to identify and quantify the level of dysfunction. Through this paper, a critical study of automated dysarthria severity classification using transformer-based deep learning methods will be discussed. Transfer learning is facilitated by employing three variations of Vision Transformer (ViT) models: ViT-L-16, ViT-L-32, and ViT-B-16, on UA-Speech Corpus and TORGO datasets, achieving remarkable results with 99.01% accuracy for the UA-Speech dataset and 99.39% accuracy for the TORGO database. The study evaluates the models’ performance focusing on accuracy, precision, recall, and F1-scores. Experimental results emphasise the potential for automated diagnostic units in neurology clinical practice and establish a baseline for leading work in dysarthria severity classification with the selection of an efficient ViT model.","keywords: {Deep learning;Neurology;Accuracy;Neuromuscular;Computational modeling;Transfer learning;Speech recognition;Speech enhancement;Transformers;Real-time systems;Transfer Learning;Speech Processing;Dysarthria Severity Detection;Transformers},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10724295&isnumber=10723316,"A. Venkata Siva Manoj, V. Lakshman, A. Kamuju, V. Pulagam and G. Jyothish Lal, ""Transformer-based Transfer Learning for Enhanced Speech Dysarthria Severity Assessment,"" 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT), Kamand, India, 2024, pp. 1-6, doi: 10.1109/ICCCNT61001.2024.10724295."
"Enhancing speech rate estimation techniques to improve dysarthria diagnosis,","This report discusses the implementation of a computerized algorithm specifically designed to measure the syllables-per-minute rate of abnormal speech typically produced by persons suffering from an articulatory disorder known as dysarthria. This speech rate measurement application - which can also serve as a diagnostic tool in itself - has been integrated into the computerised Frenchay Dysarthria Assessment (CFDA) suite of diagnostic tests. It is demonstrated that, when processing dysarthric speech, syllables-per-minute measurements are more accurate when based on vowel transition detection techniques as opposed to using spectral moment computations.","keywords: {Speech;Speech processing;Signal processing algorithms;Production;Estimation;Software algorithms;Time measurement;DSP;diagnosis;dysarthria;speech rate;intelligibility},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8117233&isnumber=8117121,"J. N. Carmichael, ""Enhancing speech rate estimation techniques to improve dysarthria diagnosis,"" 2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), Vancouver, BC, Canada, 2017, pp. 309-313, doi: 10.1109/IEMCON.2017.8117233."
"Personalizing TTS Voices for Progressive Dysarthria,","Amyotrophic lateral sclerosis (ALS) patients experience progressive speech deterioration due to muscle paralysis, leading to eventual loss of verbal communication capability. Text-to-speech synthesis (TTS) is an important technology for speech generating devices, enabling users to communicate using generic electronic voices, but often without the vocal identity of the users. Our work is aimed at personalizing TTS voices for people with ALS induced dysarthria by integrating machine learning and speech processing techniques of voice conversion (VC) and TTS. This is challenging as only small quantities of dysarthric speech are available from individual patients. Our system includes both timbre and prosody conversion for VC, neural TTS to generate TTS speech, and neural feature converter to interface VC and TTS. We collected speech data from 4 ALS target speakers with mild to severe dysarthria. Subjective listening tests showed that on average, our approach improved speech intelligibility by about 72% over the target speakers’ speech, the converted voice was 2 to 3 times more similar to ALS targets than to TTS sources, and the converted speech quality was in the MOS scale of fair to good.","keywords: {Conferences;Natural languages;Machine learning;Muscles;Paralysis;Timbre;Speech processing;voice conversion;feature conversion;neural TTS;dysarthria;amyotrophic lateral sclerosis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9508522&isnumber=9508478,"Y. Zhao, M. Song, Y. Yue and M. Kuruvilla-Dugdale, ""Personalizing TTS Voices for Progressive Dysarthria,"" 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI), Athens, Greece, 2021, pp. 1-4, doi: 10.1109/BHI50953.2021.9508522."
"A Phonological Control Method on A Speech Compensation System for Dysarthria Using A Standardized Space,","We have developed a speech compensation system for dysarthria. The system aims at improving the phonological properties of vowels without losing speaker individuality. We propose a method for phonological control of vowels using a standardized space to control vowels in the normalized articulation space, normalized for speaker individuality. The method maps an original dysarthric speaker's normalized articulation space to a standardized space, then from the standardized space to the target speaker's normalized articulation space assuming normality to improve the phonological properties of vowels. We confirm phonological control of vowels by performing a processing simulation, comparison different target speakers and a processing simulation using a dummy original speaker as a dysarthria.","keywords: {Frequency synthesizers;Process control;Aerospace electronics;Speech synthesis;Informatics;Frequency control;Gravity;vowel formant;articulation space;phonological control;vowel normalization;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9336404&isnumber=9336196,"Y. Hetsugi, T. Sakata and Y. Ueda, ""A Phonological Control Method on A Speech Compensation System for Dysarthria Using A Standardized Space,"" 2020 5th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS), Okinawa, Japan, 2020, pp. 158-162, doi: 10.1109/ICIIBMS50712.2020.9336404."
"Japanese Vowel-mora Visualization for Dysarthria Rehabilitation with Variational Autoencoder,","This work proposes a Variational AutoEncoder (VAE)-based rehabilitation framework that visualizes the vowel-mora for Japanese dysarthria. Traditionally, Speech-Language Pathology (SLP) has shown the guideline of rehabilitation for dysarthria, but they should rely only on clinical experience and case-by-case adaption, highlighting the urgent necessity to push the boundary for showing a subjective guideline, which does not depend on the perspective of SLPs. The proposed framework takes advantage of two-dimensional latent representations of vowel-mora, which is assumed to be pre-processed by mel-spectrogram, via VAE. The experiments highlight the effectiveness of our proposed framework.","keywords: {Training;Pathology;Accuracy;Circuits and systems;Autoencoders;Asia;Data visualization;Speech recognition;Feature extraction;Guidelines;Dysarthria;Variational Autoencoder;Mora visualization},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10808415&isnumber=10808208,"R. Michizoe et al., ""Japanese Vowel-mora Visualization for Dysarthria Rehabilitation with Variational Autoencoder,"" 2024 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS), Taipei, Taiwan, 2024, pp. 494-498, doi: 10.1109/APCCAS62602.2024.10808415."
"Machine Learning Approaches for Automated Detection and Classification of Dysarthria Severity,","Dysarthria, a speech disorder caused by neuro-motor problems resulting in impaired articulation, requires an assessment of its severity for diagnostic and monitoring purposes. Additionally, accurate severity classification facilitates the development of automated dysarthric speech detection and classification systems. This paper presents a comprehensive investigation into detecting dysarthric voices within a collection of normal voice samples, followed by the dysarthria severity classification utilizing neural network frameworks, specifically long short-term memory network (LSTM) and recurrent neural network (RNN). The study employs various features including Mel frequency cepstral coefficients (MFCC), formants, prosodic parameters, and voice quality. The performance of these models is evaluated against a baseline support vector machine (SVM) classifier using the Nemours corpus database. Remarkably, the highest classification accuracy achieved for this corpus is 99.69%. Detailed analysis demonstrates that selecting an appropriate neural network architecture yields superior performance compared to the conventional SVM classifier.","keywords: {Support vector machines;Voice activity detection;Pathology;Recurrent neural networks;Supervised learning;Speech enhancement;Mel frequency cepstral coefficient;Dysarthria classification;SVM;LSTM;RNN;acoustic parameters;automatic speech assessment},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10419588&isnumber=10419198,"A. Hamza, D. Addou and H. Kheddar, ""Machine Learning Approaches for Automated Detection and Classification of Dysarthria Severity,"" 2023 2nd International Conference on Electronics, Energy and Measurement (IC2EM), Medea, Algeria, 2023, pp. 1-6, doi: 10.1109/IC2EM59347.2023.10419588."
"The ISCSLP 2024 Multimodal Dysarthria Severity Assessment (MDSA) Challenge: Dataset, Tracts, Baseline and Results,","To advance multimodal speech assessment and related research in developing objective diagnostic methods, we are launching the Multimodal Dysarthria Severity Assessment (MDSA) Challenge. This paper summarizes the outcomes from the ISCSLP 2024 MDSA Challenge. We first address the necessity of the challenge and then introduce the associated audio-video dataset selected from the MSDM database, including 62 subacute stroke patients and 25 normal controls. We then describe the challenge arrangement and the baseline system. Specifically, we set up a four-classification task for the severity of dysarthria (normal, mild, moderate and severe) with the aim of developing an objective and accurate automatic assessment method to assist in clinical diagnosis and treatment. Finally we summarize the challenge results and provide the major observations from the submitted systems. We hope the open data and challenge will serve as a benchmark and common test-bed for pathological speech assessment.","keywords: {Measurement;Pathology;Image analysis;Data analysis;Databases;Speech recognition;Stroke (medical condition);Clinical diagnosis;Speech processing;Open data;Multimodal;Dysarthria severity;Classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800436&isnumber=10799969,"J. Liu et al., ""The ISCSLP 2024 Multimodal Dysarthria Severity Assessment (MDSA) Challenge: Dataset, Tracts, Baseline and Results,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 136-140, doi: 10.1109/ISCSLP63861.2024.10800436."
"An Analysis of Degenerating Speech Due to Progressive Dysarthria on ASR Performance,","Although personalized automatic speech recognition (ASR) models have recently been improved to recognize even severely impaired speech, model performance may degrade over time for persons with degenerating speech. The aims of this study were to (1) analyze the change of performance of ASR over time in individuals with degrading speech, and (2) explore mitigation strategies to optimize recognition throughout disease progression. Speech was recorded by four individuals with degrading speech due to amyotrophic lateral sclerosis (ALS). Word error rates (WER) across recording sessions were computed for three ASR models: Unadapted Speaker Independent (U-SI), Adapted Speaker Independent (A-SI), and Adapted Speaker Dependent (A-SD or personalized). The performance of all models degraded significantly over time as speech became more impaired, but the A-SD model improved markedly when updated with recordings from the severe stages of speech progression. Recording additional utterances early in the disease before significant speech degradation did not improve the performance of A-SD models. This emphasizes the importance of continuous recording (and model retraining) when providing personalized models for individuals with progressive speech impairments.","keywords: {Degradation;Adaptation models;Error analysis;Computational modeling;Acoustics;Recording;Speech processing},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10097195&isnumber=10094560,"K. Tomanek, K. Seaver, P. -P. Jiang, R. Cave, L. Harrell and J. R. Green, ""An Analysis of Degenerating Speech Due to Progressive Dysarthria on ASR Performance,"" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10097195."
"Dysarthria Voice Disorder Detection Using Mel Frequency Logarithmic Spectrogram and Deep Convolution Neural Network,","Dysarthric speech recognition (DSR), often known as DSR, is an important tool that enables persons with vocal impairments to participate in voice-based automation systems and human-computer interaction. As a result of the poor intelligibility of handicapped speakers, the limited availability of datasets, and the low intra-class and inter-class variability in the speech samples, DSR is an essential component. This research provides a DSR based on the Mel Frequency Logarithmic Spectrogram (MFLS) and Deep Convolutional Neural Network (DCNN). The suggested MFLS+DCNN offers an improved representation of the voice signal in terms of its spectral and temporal characteristics. The results of the MFLS-DCNN scheme are validated using the UASpeech dataset, which was developed based on accuracy, recall, precision, and F1-score. With an accuracy of 96.83%, a recall performance of 0.97, a precision performance of 0.96, and an F1-score of 0.97, the scheme that is proposed has shown a considerable improvement above the conventional state-of-the-art.","keywords: {Human computer interaction;Pathology;Accuracy;Automation;Convolution;Neural networks;Speech recognition;Convolutional neural networks;Speech processing;Spectrogram;Deep learning;automatic speech recognition;affective computing;deep convolution neural networks;dysarthric speech recognition;voice pathology are some of the topics that are being discussed},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10698732&isnumber=10697984,"S. Yadav and D. Yadav, ""Dysarthria Voice Disorder Detection Using Mel Frequency Logarithmic Spectrogram and Deep Convolution Neural Network,"" 2024 First International Conference on Electronics, Communication and Signal Processing (ICECSP), New Delhi, India, 2024, pp. 1-6, doi: 10.1109/ICECSP61809.2024.10698732."
"Wav2vec-Based Detection and Severity Level Classification of Dysarthria From Speech,","Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).","keywords: {Databases;Cepstral analysis;Feature extraction;Medical diagnosis;Speech processing;Task analysis;Spectrogram;Dysarthria;Severity level classification;Wav2vec 2.0;MFCCs},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10094857&isnumber=10094560,"F. Javanmardi, S. Tirronen, M. Kodali, S. R. Kadiri and P. Alku, ""Wav2vec-Based Detection and Severity Level Classification of Dysarthria From Speech,"" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10094857."
"Improved Speaker Independent Dysarthria Intelligibility Classification Using Deepspeech Posteriors,","Individuals with dysarthria are unable to control rapid movement of the velum leading to reduction in intelligibility, audibility, naturalness and efficiency of vocal communication. Automatic intelligibility assessment of dysarthric patients allows clinicians diagnose the impact of therapy and medication and also to plan future course of action. Earlier works have concentrated on building speaker dependent machine learning systems for intelligibility assessment, due to limited availability of data. However, a speaker independent assessment system is of greater use by clinicians. Motivated by this observation, we propose a speaker independent intelligibility assessment system which relies on a novel set of features obtained by processing the output of DeepSpeech, an end to end Speech-to-Text engine. All experiments have been performed on the Universal Access Speech database. An accuracy of 53.9% was obtained using Support Vector Machine based four-class classification system for the speaker independent scenario while the accuracy obtained for the speaker dependent scenario is 97.4%.","keywords: {Support vector machines;Medical treatment;Machine learning;Signal processing;Speech processing;Medical diagnostic imaging;Engines;Dysarthria;intelligibility assessment;openS-MILE;deepspeech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9054492&isnumber=9052899,"A. Tripathi, S. Bhosale and S. K. Kopparapu, ""Improved Speaker Independent Dysarthria Intelligibility Classification Using Deepspeech Posteriors,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 6114-6118, doi: 10.1109/ICASSP40776.2020.9054492."
"PB-LRDWWS System For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge,","For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting (LRDWWS) Challenge, we introduce the PB-LRDWWS system. This system combines a dysarthric speech content feature extractor for prototype construction with a prototype-based classification method. The feature extractor is a fine-tuned HuBERT model obtained through a three-stage fine-tuning process using cross-entropy loss. This fine-tuned HuBERT extracts features from the target dysarthric speaker’s enrollment speech to build prototypes. Classification is achieved by calculating the cosine similarity between the HuBERT features of the target dysarthric speaker’s evaluation speech and prototypes. Despite its simplicity, our method demonstrates effectiveness through experimental results. Our system achieves second place in the final Test-B of the LRDWWS Challenge.","keywords: {Conferences;Buildings;Prototypes;Feature extraction;Keyword spotting;wake-up word detection;dysarthria;fine-tuning;prototype-based classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832235&isnumber=10830793,"S. Wang, J. Zhou, S. Zhao and Y. Qin, ""PB-LRDWWS System For the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 586-591, doi: 10.1109/SLT61566.2024.10832235."
"On the use of array learners towards Automatic Speech Recognition for dysarthria,","Providing Automatic Speech Recognition (ASR) systems for dysarthria is a challenging task since the normal and the disabled speech have different attributes; hence, using ASR systems designed and trained for normal speakers is not an effective approach. It is important to craft ASR technologies specifically for the speech disabled. Nonetheless, because of the complexity and variability of dysarthric speech, previous studies failed to achieve adequate performance. In this paper we investigated the applications of array learners towards dysarthric speech recognition. The array was implemented by several neural networks that configured to work in parallel. The proposed approach was verified by using the speech materials of seven dysarthric subjects with speech intelligibility from 2% to 86%. For comparison, the results were compared with a dysarthric ASR based on the legacy single-learner approach as the reference model. It is shown that the array learner-based dysarthric ASR improved the mean word recognition rate of 10.41% over the reference model, and decreased the error rate of 4.84%.","keywords: {Speech;Speech recognition;Arrays;Feature extraction;Artificial neural networks;Training;Neurons;Learners array;Dysarthria;Artificial neural networks;Automatic speech recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7334306&isnumber=7334072,"S. R. Shahamiri and S. K. Ray, ""On the use of array learners towards Automatic Speech Recognition for dysarthria,"" 2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA), Auckland, New Zealand, 2015, pp. 1283-1287, doi: 10.1109/ICIEA.2015.7334306."
"Automated assessment and treatment of speech rate and intonation in dysarthria,","Prosody assessment and treatment in dysarthria is clinically relevant, since prosodic impairment can have a negative impact on speech intelligibility and thus on participation in daily life conversation. We propose a speech-technology based software tool that provides automated numerical and visual feedback on two important aspects of prosody: speech rate and intonation. The tool includes speech rate and intonation algorithms, both specifically developed for the analysis of Dutch dysarthric speech. The tool enables speech-language pathologists to obtain objective measures of these prosodic aspects in a standardized and fast way, and enables dysarthric speakers to practise their prosodic skills intensively without the presence of a speech-language pathologist being required.","keywords: {Classification algorithms;Speech;dysarthria;speech rate;intonation;sentence modality;automated assessment;automated treatment},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6563972&isnumber=6563889,"H. Martens, G. Van Nuffelen, M. De Bodt, T. Dekens, L. Latacz and W. Verhelst, ""Automated assessment and treatment of speech rate and intonation in dysarthria,"" 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops, Venice, Italy, 2013, pp. 382-384, doi: 10.4108/icst.pervasivehealth.2013.252366."
"Identification of hypokinetic dysarthria using acoustic analysis of poem recitation,","Up to 90% of patients with Parkinson's disease (PD) suffer from hypokinetic dysarthria (HD). In this work, we analysed the power of conventional speech features quantifying imprecise articulation, dysprosody, speech dysfluency and speech quality deterioration extracted from a specialized poem recitation task to discriminate dysarthric and healthy speech. For this purpose, 152 speakers (53 healthy speakers, 99 PD patients) were examined. Only mildly strong correlation between speech features and clinical status of the speakers was observed. In case of univariate classification analysis, sensitivity of 62.63 % (imprecise articulation), 61.62% (dysprosody), 71.72% (speech dysfluency) and 59.60% (speech quality deterioration) was achieved. Multivariate classification analysis improved the classification performance. Sensitivity of 83.42% using only two features describing imprecise articulation and speech quality deterioration in HD was achieved. We showed the promising potential of the selected speech features and especially the use of poem recitation task to quantify and identify HD in PD.","keywords: {Speech;High definition video;Feature extraction;Correlation;Parkinson's disease;acoustic analysis;binary classification;hypokinetic dysarthria;Parkinson's disease;poem recitation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8076086&isnumber=8075917,"J. Mucha et al., ""Identification of hypokinetic dysarthria using acoustic analysis of poem recitation,"" 2017 40th International Conference on Telecommunications and Signal Processing (TSP), Barcelona, Spain, 2017, pp. 739-742, doi: 10.1109/TSP.2017.8076086."
"Respiratory and laryngeal influences on voice in post-stroke dysarthria: a pilot study,","Dysarthria is a common disorder among stroke-patients that affects a wide range of speech and voice production processes. While voice disorders in post-stroke dysarthric patients have been well documented, few research has set out to investigate the complicated cause of the disorders, which potentially involve altered respiratory and laryngeal functions and their interaction. In this paper, we report a pilot study that preliminarily examined the respiratory and laryngeal influences on vocal performances in post-stroke dysarthric patients and healthy controls. Respiratory, laryngeal and vocal measures were collected in a maximum phonation time task and analyzed with linear mixed-effect regressions. The results suggested that pathways of influence may be established from respiratory to laryngeal functions and from laryngeal to vocal functions. Patients demonstrated increased laryngeal effort in response to reduced respiratory volumes and expiratory speed, which in turn may have contributed to a reduced voice quality and stability among patients. In addition, a few other findings suggested that reduced respiratory functions may also influence vocal performance through alternative pathways, although more work is needed to establish a clear chain of influence in these cases by unveiling the precise relations between the respiratory, laryngeal and vocal abnormalities in question.","keywords: {Production;Stability analysis;Time measurement;Behavioral sciences;Task analysis;Speech processing;post-stroke dysarthria;voice disorder;respiratory function;laryngeal function},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10037959&isnumber=10037573,"T. Zhao, X. Du, J. Liu, R. Su, N. Yan and L. Wang, ""Respiratory and laryngeal influences on voice in post-stroke dysarthria: a pilot study,"" 2022 13th International Symposium on Chinese Spoken Language Processing (ISCSLP), Singapore, Singapore, 2022, pp. 364-368, doi: 10.1109/ISCSLP57327.2022.10037959."
"Assessing freezing of gait in parkinson's disease using analysis of hypokinetic dysarthria,","Hypokinetic dysarthria (HD) and freezing of gait (FOG) are frequent symptoms of Parkinson's disease (PD). The aim of this work is to reveal pathological mechanisms common for HD and FOG, and use acoustic analysis of dysarthric speech to assess the gait difficulties in PD. We used a correlation analysis to investigate a relationship between speech features and FOG evaluated by freezing of gait questionnaire (FOG-Q). We found speech features quantifying reduced mobility of the articulatory organs significantly correlated with all parts of the questionnaire. Next, we built multivariate regression models to estimate the FOG-Q total score. With this approach, mean estimation error rate of 14.71% was achieved. We confirmed the previous findings of a close relationship between HD and FOG in PD. Furthermore, we showed it is possible to accurately (with the error of approximately 0.5 points) estimate FOG-Q using a reasonable number of conventional speech features.","keywords: {Speech;High definition video;Correlation;Legged locomotion;Turning;Intellectual property;Parkinson's disease;acoustic analysis;freezing of gait;hypokinetic dysarthria;Parkinson's disease;regression},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8076085&isnumber=8075917,"Z. Galaz et al., ""Assessing freezing of gait in parkinson's disease using analysis of hypokinetic dysarthria,"" 2017 40th International Conference on Telecommunications and Signal Processing (TSP), Barcelona, Spain, 2017, pp. 735-738, doi: 10.1109/TSP.2017.8076085."
"SARNet: Speaker-Attentive ResNet for Quantification of Dysarthria Severity,","Analyzing differences in audio data of individuals with articulation disorders from the perspective of human speech and employing objective methods for automated dysarthria evaluation can significantly aid doctors in early patient screening and diagnosis. This proactive approach enables timely intervention and treatment during the initial stages of the condition. This paper presents a speech recognition network(named Speaker-Attentive ResNet (SARNet)) that aggregates and propagates features from different hierarchical levels using an attention-based statistical pooling module, built upon the ResNet architecture. This network aims to extract subtle features specific to individual speakers. The proposed method is evaluated using the TORGO dysarthric speech database, employing eight different acoustic features as well as features aggregated from these eight. Comparative experiments with two other models, Time-Delay Neural Network (TDNN) and Panns-CNN10(Large-Scale Pretrained Audio Neural Networks), demonstrate that due to the superior ability of MFCC(Mel-scaleFrequency Cepstral Coefficients) to emulate human auditory features, the proposed approach achieves a classification accuracy of $98 \%-99 \%$ in measuring speech intelligibility in healthy people, patients, and the severity of patient’s articulation disorders.","keywords: {Accuracy;Databases;Neural networks;Speech recognition;Medical services;Feature extraction;Robustness;Dysarthria;Quantification;Speaker-Attentive ResNet},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10662779&isnumber=10661196,"H. Yin, X. Zhang, Y. Yu and N. Yu, ""SARNet: Speaker-Attentive ResNet for Quantification of Dysarthria Severity,"" 2024 43rd Chinese Control Conference (CCC), Kunming, China, 2024, pp. 8465-8470, doi: 10.23919/CCC63176.2024.10662779."
"Data Augmentation for Dysarthric Speech Recognition Based on Text-to-Speech Synthesis,","In the field of automatic speech recognition (ASR) for people with dysarthria, it is problematic that not enough training speech data can be collected from people with dysarthria. To solve this problem, we propose a method of data augmentation using text-to-speech (TTS) synthesis. In the proposed data augmentation method, a deep neural network (DNN)-based TTS model is trained by utilizing speech data recorded from a speaker with dysarthria, and the trained TTS model is then used to generate the speaker’s speech data for training the ASR model for the speaker. The results of a speech recognition experiment on a person having spinal muscular atrophy (SMA) showed that the speech recognition error rate was improved by using the proposed data augmentation.","keywords: {Training;Deep learning;Error analysis;Conferences;Neural networks;Training data;Speech recognition;speech recognition;data augmentation;dysarthria;speaking disorder;speech synthesis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754798&isnumber=9754592,"Y. Matsuzaka, R. Takashima, C. Sasaki and T. Takiguchi, ""Data Augmentation for Dysarthric Speech Recognition Based on Text-to-Speech Synthesis,"" 2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech), Osaka, Japan, 2022, pp. 399-400, doi: 10.1109/LifeTech53646.2022.9754798."
"Analysis and Classification of Dysarthric Speech,","Classifying dysarthria using neural networks is challenging due to several factors inherent to the nature of dysarthria, the complexity of speech signals, and the requirements of effective machine learning models. Impaired speech classification is challenging for two main reasons: firstly, the data is scarce, and secondly, it is heterogeneous. In this paper, we have trained two different architectures on a dysarthric speech database. A comparison of results shows that according to precision, recall, f1-score, and accuracy, the Deep Neural Network (DNN) model outperforms the classical Convolutional Neural Network (CNN) model, even with a small database. For people with dysarthria, a DNN can improve the performance metrics during the classification of impaired speech by 99% compared to the classical architecture. This improvement is more than that provided by CNN. We have used the TORGO database for this work.","keywords: {Measurement;Databases;Machine learning;Complexity theory;Convolutional neural networks;Deep Neural Network;Convolutional Neural Network;speech classification;dysarthria;impaired speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10482956&isnumber=10482911,"V. Tyagi, A. Dev and P. Bansal, ""Analysis and Classification of Dysarthric Speech,"" 2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Delhi, India, 2023, pp. 1-6, doi: 10.1109/O-COCOSDA60357.2023.10482956."
"Speech Recognition for a Person With Cerebral Palsy Using Whisper Fine-Tuned on Japanese and English Dysarthric Speech,","People with cerebral palsy often have dysarthria, and this makes it hard for them to speak as they wish. In this paper, we present an automatic speech recognition (ASR) model for a person with cerebral palsy based on Whisper. Whisper is highly accurate when performing speech recognition for Japanese speech, but recognition accuracy for Japanese dysarthric speech tends to be low. One possible solution to this problem is to fine-tune Whisper using Japanese dysarthric speech. However, there is a problem in that it is difficult to record a large amount of speech for people with dysarthria. Therefore, in our proposed method, English dysarthric speech is utilized for training Whisper in addition to Japanese dysarthric speech. The results of our proposed method showed an improvement in the error rate of about 1% compared to using only Japanese dysarthric speech as training data.","keywords: {Training;Cerebral palsy;Accuracy;Error analysis;Training data;Speech recognition;Consumer electronics;Automatic speech recognition;speech recognition;multilingual;dysarthria;cerebral palsy;Whisper},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10760680&isnumber=10760245,"K. Haze, R. Takashima and T. Takiguchi, ""Speech Recognition for a Person With Cerebral Palsy Using Whisper Fine-Tuned on Japanese and English Dysarthric Speech,"" 2024 IEEE 13th Global Conference on Consumer Electronics (GCCE), Kitakyushu, Japan, 2024, pp. 419-420, doi: 10.1109/GCCE62371.2024.10760680."
"A joint-feature learning-based voice conversion system for dysarthric user based on deep learning technology,","Dysarthria speakers suffer from poor communication, and voice conversion (VC) technology is a potential approach for improving their speech quality. This study presents a joint feature learning approach to improve a sub-band deep neural network-based VC system, termed J_SBDNN. In this study, a listening test of speech intelligibility is used to confirm the benefits of the proposed J_SBDNN VC system, with several well-known VC approaches being used for comparison. The results showed that the J_SBDNN VC system provided a higher speech intelligibility performance than other VC approaches in most test conditions. It implies that the J_SBDNN VC system could potentially be used as one of the electronic assistive technologies to improve the speech quality for a dysarthric speaker.","keywords: {Speech processing;Feature extraction;Training;Biological system modeling;Artificial neural networks;Task analysis;Mel frequency cepstral coefficient},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8856560&isnumber=8856280,"K. -C. Chen, H. -W. Yeh, J. -Y. Hang, S. -H. Jhang, W. -Z. Zheng and Y. -H. Lai, ""A joint-feature learning-based voice conversion system for dysarthric user based on deep learning technology,"" 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Berlin, Germany, 2019, pp. 1838-1841, doi: 10.1109/EMBC.2019.8856560."
"A multi-smartwatch system for assessing speech characteristics of people with dysarthria in group settings,","Speech-language pathologists (SLPs) frequently use vocal exercises in the treatment of patients with speech disorders. Patients receive treatment in a clinical setting and need to practice outside of the clinical setting to generalize speech goals to functional communication. In this paper, we describe the development of technology that captures mixed speech signals in a group setting and allows the SLP to analyze the speech signals relative to treatment goals. The mixed speech signals are blindly separated into individual signals that are preprocessed before computation of loudness, pitch, shimmer, jitter, semitone standard deviation and sharpness. The proposed method has been previously validated on data obtained from clinical trials of people with Parkinson disease and healthy controls.","keywords: {Speech;Speech processing;Monitoring;Random variables;Blind source separation;Acoustics;Estimation;dysarthria;jitter;knowledge-based speech processing;loudness;multi-smartwatch system;perceptual speech quality;pitch;semitone standard deviation;sharpness;shimmer},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7454559&isnumber=7454459,"H. Dubey, J. C. Goldberg, K. Mankodiya and L. Mahler, ""A multi-smartwatch system for assessing speech characteristics of people with dysarthria in group settings,"" 2015 17th International Conference on E-health Networking, Application & Services (HealthCom), Boston, MA, USA, 2015, pp. 528-533, doi: 10.1109/HealthCom.2015.7454559."
"Weak Speech Supervision: A case study of Dysarthria Severity Classification,","Machine Learning methodologies are making a remarkable contribution, and yielding state-of-the-art results in different speech domains. With this exceptionally significant achievement, a large amount of labeled data is the largest bottleneck in the deployment of these speech systems. To generate massive data, hand-labeling training data is an intensively laborious task. This is problematic for clinical applications where obtaining such data labeled by speech pathologists is expensive and time-consuming. To overcome these problems, we introduce a new paradigm called Weak Speech Supervision (WSS), a first-of-its-kind system that helps users to train state-of-the-art classification models without hand-labeling training data. Users can write labeling functions (i.e., weak rules) to generate weak data from the unlabeled training set. In this paper, we provide the efficiency of this methodology via showing the case study of the severity-based binary classification of dysarthric speech. In WSS, we train a classifier on trusted data (labeled with 100% accuracy) via utilizing the weak data (labeled using weak supervision) to make our classifier model more efficient. Analysis of the proposed methodology is performed on Universal Access (UA) corpus. We got on an average 35.68% and 43.83% relative improvement in terms of accuracy and F1-score w.r.t. baselines, respectively.","keywords: {Training;Training data;Machine learning;Signal processing;Data models;Task analysis;Speech processing;Dysarthria;Severity-based Classification;Data Scarcity;Weak Supervision;CNN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9287502&isnumber=9287310,"M. Purohit, M. Parmar, M. Patel, H. Malaviya and H. A. Patii, ""Weak Speech Supervision: A case study of Dysarthria Severity Classification,"" 2020 28th European Signal Processing Conference (EUSIPCO), Amsterdam, Netherlands, 2021, pp. 101-105, doi: 10.23919/Eusipco47968.2020.9287502."
"End-to-end Dysarthric Speech Recognition Using Multiple Databases,","We present in this paper an end-to-end automatic speech recognition (ASR) system for a person with an articulation disorder resulting from athetoid cerebral palsy. In the case of a person with this type of articulation disorder, the speech style is quite different from that of a physically unimpaired person, and the amount of their speech data available to train the model is limited because their burden is large due to strain on the speech muscles. Therefore, the performance of ASR systems for people with an articulation disorder degrades significantly. In this paper, we propose an end-to-end ASR framework trained by not only the speech data of a Japanese person with an articulation disorder but also the speech data of a physically unimpaired Japanese person and a non-Japanese person with an articulation disorder to relieve the lack of training data of a target speaker. An end-to-end ASR model encapsulates an acoustic and language model jointly. In our proposed model, an acoustic model portion is shared between persons with dysarthria, and a language model portion is assigned to each language regardless of dysarthria. Experimental results show the merit of our proposed approach of using multiple databases for speech recognition.","keywords: {Hidden Markov models;Acoustics;Data models;Databases;Speech recognition;Training;Computational modeling;Speech recognition;multilingual;assistive technology;end-to-end model;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8683803&isnumber=8682151,"Y. Takashima, T. Takiguchi and Y. Ariki, ""End-to-end Dysarthric Speech Recognition Using Multiple Databases,"" ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 6395-6399, doi: 10.1109/ICASSP.2019.8683803."
"Modification to correct distortions in stops of dysarthrie speech using TMS320C6713 DSK,","Communication is a bridge between people which enables them to share facts, ideas, feelings etc. with each other. Speech communication is easier and simpler compared to other types of communication. But speech disorders affect one's ability to communicate. Dysarthria is a neuro-motor disorder, where one losses his/her ability to articulate words normally due to tongue/muscle weakness or stroke etc. It results in distorted speech which is hard to understand. Dysarthria affect more on the articulation of consonants (stops in particular) than on the articulation of vowels. Intelligibility varies greatly depending on the extent of neurological damage. As consonants are important than vowels in measuring the intelligibility, distortions in consonant production of dysarthric speech are studied and modifications are performed. This work aims at correcting devoicing of voiced stop (voiced stops are pronounced as unvoiced stops with the same place of articulation) by detecting important time instants such as glottal onset, glottal offset and burst onset thereby modifying the distortions in dysarthric speech using TMS320C6713 DSK with CC Studio 5.5. After the dysarthric speech has been recorded on the DSK 6713 processor modification is done to improve its intelligibility. Two databases were used in the experiment such as Universal Access (UA) database and Torgo database. Out of 65 bursts expected from UA database and 128 bursts expected from Torgo database, 84.6% and 82.81% bursts were detected correctly within 0.3 s intervals respectively.","keywords: {Speech;Databases;Distortion;Spectrogram;Discrete Fourier transforms;Optimization;Speech processing;Landmarks;Dysarthria;Rate of Rise;Glottis;Bursts},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8284657&isnumber=8284511,"A. Raj, A. Anjum, Chethan, Lakshmi, V. Karjigi and M. Rao, ""Modification to correct distortions in stops of dysarthrie speech using TMS320C6713 DSK,"" 2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT), Mysuru, India, 2017, pp. 158-163, doi: 10.1109/ICEECCOT.2017.8284657."
"Prediction of Parkinson’s Disease Using Machine Learning Based on Vocal Frequency,","Parkinson’s disease (PD) is a neurological disorder that affects most people after Alzheimer’s.The ageing neurodegenerative condition leads to PD that reduces dopamine levels in the brain are a defining feature. Tremor, stiffness, bradykinesia, and postural instability are the core characteristics of PD. PD patient’s(PWP) quality of life is impacted by both motor and non-motor symptoms, which may also have an indirect impact on family and caregivers. The patient’s quality of life can be improved and maintained with an early PD diagnosis. There is currently no cure, however there are therapies to control the illness, such as dopaminergic medications. Speech problems associated with PD are categorized under the term hypokinetic dysarthria for the diagnosis of PD.The traditional approach is based on their clinical history and also with their physical examination. With the help of automatic analysis tools, practitioners may diagnose patients, monitor their progress, and conduct regular, economical assessments that are objective. This study uses a machine learning approach to conduct a pilot experiment to identify the existence of dysarthria in speech and gauge its severity. With the help of SVM Algorithm and HyperTunningdisplays 93% accuracy on KAGGLE database test samples.","keywords: {Support vector machines;Neurological diseases;Machine learning algorithms;Magnetic resonance imaging;Medical treatment;Machine learning;Prediction algorithms;Dysarthria detection;Machine learning;SVM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10128416&isnumber=10128171,"N. P. Saravanan, P. Deepika, P. Dhanush and P. Dhanvarsini, ""Prediction of Parkinson’s Disease Using Machine Learning Based on Vocal Frequency,"" 2023 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 2023, pp. 1-6, doi: 10.1109/ICCCI56745.2023.10128416."
"Acoustic Space in Motor Disorders of Speech: Two Case Studies,","Studies on acoustic space have strengthened the view that vowels are acoustically and perceptually defined in terms of their relative positioning in vowel space. Every speaker identifies an optimal vowel space within which perceptual, phonological contrast is maintained. This is an interdisciplinary study involving speech pathology, physics of speech and neurology of speech. Two case studies of dysarthria presented in this paper are -- one Parkinson's disease and one case of acute ischemic stroke with age-gender-language matched controls. A detailed acoustic analysis shows how acoustic space gets considerably reduced, in both PD and stroke, and in these two very different kinds of dysarthrias the acoustic space is also modified very differently. The study also examines the third formant to show that the higher formants are consistently lowered in both PD and stroke. Hypokinetic speech production in these cases is reflected in lower intensity. The results have significant applications in clinical acoustics and in the theoretical fields of neurology of speech, linguistics and phonology.","keywords: {Acoustics;Speech;Aerospace electronics;Production;Parkinson's disease;Nervous system;PD control;Acoustic Space;Dysarthria;Stroke;Parkinson's disease;Intensity;Formants},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6121505&isnumber=6121452,"V. Narang, D. Misra and G. Dalal, ""Acoustic Space in Motor Disorders of Speech: Two Case Studies,"" 2011 International Conference on Asian Language Processing, Penang, Malaysia, 2011, pp. 211-215, doi: 10.1109/IALP.2011.25."
"Robust Assessment of Dysarthrophonic Voice with RASTA-PLP Features: A Nonlinear Spectral Measures,",This paper presents an artificial intelligence based speech signal processing technique to identify dysarthrophonic voice with relative spectral-perceptual linear prediction (RASTA-PLP) features. Dysarthria is a neural motor speech disorder caused by muscular weakness. Voice analysis of dysarthrophonic patients is challenging as this disease has multidimensional effects on the human voice generation system. Conventional spectral analysis is unable to accurately characterize the pathology associated with nonlinear dynamicity of human voice. This work investigates the suitability of RASTA-PLP features excerpted from speech signals to identify dysarthrophonic patients. The speech samples of healthy and dysarthrophonic patients are collected from the Saarbrücken Voice Database (SVD). Several machine learning and Artificial neural network (ANN) based algorithms are developed to evaluate the classification performance of the proposed system. The designed system can achieve excellent performance in terms of accuracy (100%) considering female and male subjects separately.,"keywords: {Support vector machines;Pathology;Machine learning algorithms;Signal processing algorithms;Artificial neural networks;Machine learning;Feature extraction;Accuracy;ANN;classifier;dysarthria;dysarthophonia;deep learning;machine learning;pathology;PLP;RASTA-PLP;speech;vocal disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10126695&isnumber=10126146,"R. Islam and M. Tarique, ""Robust Assessment of Dysarthrophonic Voice with RASTA-PLP Features: A Nonlinear Spectral Measures,"" 2023 2nd International Conference on Mechatronics and Electrical Engineering (MEEE), Abu Dhabi, United Arab Emirates, 2023, pp. 74-78, doi: 10.1109/MEEE57080.2023.10126695."
"Comparison of English and Chinese Speech Recognition Using High-Density Electromyography,","Speaking different languages requires different ways of pronunciation, and the muscular activities associated with phonation show different articulation styles. Therefore, clarifying the contributions of the articulatory muscles in different regions, such as the face and neck, is helpful for automatic speech recognition. However, it remains unclear how the articulatory muscles at different positions affect the classification accuracies of speech recognition across different languages. In this study, the technique of high-density surface electromyography (HD sEMG) was proposed to investigate the role of different articulatory muscles in classifying English and Chinese speaking tasks, respectively. The HD sEMG signals were recorded by 120 electrodes evenly placed on the facial and neck muscles across six subjects while they were speaking five English and Chinese daily words. Four time-domain features were extracted from sEMG recordings and used to construct a linear-discriminant-analysis classifier for speech recognition. The results showed that the classification accuracies of using neck sEMG were higher than that of using facial sEMG in both English and Chinese recognition tasks. The accuracies for Chinese speaking tasks were significantly higher than that for English when using facial sEMG only. Moreover, there was no significant difference in accuracies between the two types of languages when using neck sEMG. This study might provide useful information about the contributions of different articulatory muscles, and pave the way for automatic speech recognition across different languages for patients with dysarthria.","keywords: {Muscles;Task analysis;Neck;Speech recognition;Electrodes;Facial muscles;Face recognition;high-density surface electromyography;speech recognition;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9047706&isnumber=9047668,"M. Zhu et al., ""Comparison of English and Chinese Speech Recognition Using High-Density Electromyography,"" 2019 13th International Conference on Sensing Technology (ICST), Sydney, NSW, Australia, 2019, pp. 1-5, doi: 10.1109/ICST46873.2019.9047706."
"Hybrid CNN-LSTM network to detect Dysarthria using Mel-Frequency Cepstral Coefficients,","Dysarthria is a speech problem acquired at birth due to cerebral palsy (CP) or developed after severe brain damage. Dysarthria affects more than 70% of Parkinson's patients and 10% to 65% of people with traumatic brain injury. It is critical to detect dysarthria and other voice speech difficulties early to diagnose the underlying cause. Intelligent systems capable of identifying dysarthria with incredible precision have been developed using audio processing techniques and various deep learning models. This paper presents a hybrid CNN-LSTM model for classifying patients with dysarthria using audio recordings. The CNN-LSTM combination helps capture spatial and temporal information where CNN acts as a feature extractor while LSTM functions as a classifier. The proposed model was trained on the publicly available 9184 audio recordings from the TORGO dataset, and various audio augmentation techniques were employed to generate synthetic data. A total of 128 features were extracted using Mel Frequency Cepstral Coefficients (MFCC) and fed into the architecture as inputs. The K-fold cross-validation technique was used to avoid overfitting and increase the generalization capability of the model. The proposed architecture achieved a state-of-the-art 99.59% accuracy on the dataset. The presented work will minimize the workload of speech pathologists and help them detect dysarthria precisely and effectively.","keywords: {Pediatrics;Parkinson's disease;Education;Feature extraction;Brain modeling;Audio recording;Convolutional neural networks;Dysarthria;Audio Processing;Feature Extraction;Convolution Neural Network;Long Short Term Network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10039514&isnumber=10039478,"K. Vora, D. Padalia, D. Mehta and D. Sharma, ""Hybrid CNN-LSTM network to detect Dysarthria using Mel-Frequency Cepstral Coefficients,"" 2022 5th International Conference on Advances in Science and Technology (ICAST), Mumbai, India, 2022, pp. 615-621, doi: 10.1109/ICAST55766.2022.10039514."
"Optimizing Dysarthria Wake-Up Word Spotting: an End-to-End Approach For SLT 2024 LRDWWS Challenge,","Speech has emerged as a widely embraced user interface across diverse applications. However, for individuals with dysarthria, the inherent variability in their speech poses significant challenges. This paper presents an end-to-end Pretrain-based Dual-filter Dysarthria Wake-up word Spotting (PD-DWS) system for the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge. Specifically, our system improves performance from two key perspectives: audio modeling and dual-filter strategy. For audio modeling, we propose an innovative 2 branch- d 2 v 2 model based on the pre-trained data2vec $2(\mathrm{~d} 2 \mathrm{v} 2)$, which can simultaneously model automatic speech recognition (ASR) and wake-up word spotting (WWS) tasks through a unified multi-task finetuning paradigm. Additionally, a dual-filter strategy is introduced to reduce the false accept rate (FAR) while maintaining the same false reject rate (FRR). Experimental results demonstrate that our PD-DWS system achieves an FAR of 0.00321 and an FRR of 0.005, with a total score of 0.00821 on the test-B eval set, securing first place in the challenge.","keywords: {Conferences;User interfaces;Multitasking;Data augmentation;Automatic speech recognition;LRDWWS challenge;2brach-d2v2;dualfilter;wake-up word spotting},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832263&isnumber=10830793,"S. Liu et al., ""Optimizing Dysarthria Wake-Up Word Spotting: an End-to-End Approach For SLT 2024 LRDWWS Challenge,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 578-585, doi: 10.1109/SLT61566.2024.10832263."
"Enhancing Dysarthria Diagnosis With Deep Learning Techniques,","Dysarthria is characterized by delayed, slurred speech that can be challenging to comprehend. This condition is frequently brought on by nerve injury that affects the muscles used to produce speech. Depending on which muscles are affected and the underlying cause, symptoms can differ greatly. Talk therapy can help with early detection and intervention, which can enhance treatment outcomes. Convolutional neural networks (CNNs) are the method suggested here for detecting dysarthria from audio data. The method highlights the distinctions in speech patterns between people with and without dysarthria by using feature extraction, notably Mel-frequency cepstral coefficients (MFCC), and audio visualization approaches. These properties are used in the development and training of several neural network models, such as CNN, Long Short Term Memory (LSTM), Gated Recurrent Units (GRU), Bidirectional LSTM, SimpleRNN, and Deep Neural Networks (DNN). These models’ performance is assessed with the use of confusion matrices and classification reports. This all-inclusive dysarthria detection pipeline includes advanced deep learning approaches for evaluation, model training, and data preprocessing. The objective is to develop a trustworthy dysarthria detection system that will help healthcare professionals identify and treat the ailment early on.","keywords: {Training;Deep learning;Accuracy;Telemedicine;Muscles;Feature extraction;Software;Convolutional neural networks;Object recognition;Testing;Dysarthria Detection;Deep Learning Techniques;MFCC;Spectrogram;Neural network},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10725823&isnumber=10723316,"S. Keerthika, N. Abinaya, S. Santhiya, K. Nithika, T. Dhanush and C. B. Arvind, ""Enhancing Dysarthria Diagnosis With Deep Learning Techniques,"" 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT), Kamand, India, 2024, pp. 1-6, doi: 10.1109/ICCCNT61001.2024.10725823."
"Using Novel Hybrid Convolutional Neural Network for Dysarthria Diagnosis,","Dysarthria is a motor speech disorder characterized by articulation and phonation difficulties resulting from speech muscle weakness, paralysis, or incoordination. A precise and timely diagnosis of dysarthria is essential for effective treatment and management of the condition, as it may deteriorate over time or be a precursor to a much more serious disease. On the other hand, this is becoming a severe problem in recent times owing to the rising ageing population and the prevalence of neurological disorders among such people. Hence early detection of dysarthria is deemed essential for the timely management of the disease. In recent years, Artificial Intelligence (AI) applications have shown promising results in various audio processing tasks and incorporated into pathological voice analysis for disease diagnosis. The majority of previous studies on dysarthria detection employed Machine Learning (ML) and Deep Learning (DL) models as the disease classification models. In light of this, this study presents a novel hybrid approach for classifying dysarthria based on audio data using Convolutional Neural Network (CNN) and Support Vector Machine (SVM). According to the experimental results, the proposed classification schema achieves an accuracy of 98.25 % compared to previous research work. Overall, our proposed method aims to automate the classification process, enabling faster and more reliable diagnoses.","keywords: {Support vector machines;Analytical models;Soft sensors;Sociology;Motors;Feature extraction;Data models;Artificial Intelligence;Convolutional Neural Network;Dysarthria;Speech Classification;Machine Learning;Deep Learning;SVM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10487652&isnumber=10487139,"N. N. Thilakarathne, K. Galajit, J. Karnjana, W. P. Pa and H. Yassin, ""Using Novel Hybrid Convolutional Neural Network for Dysarthria Diagnosis,"" 2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE), Nadi, Fiji, 2023, pp. 01-06, doi: 10.1109/CSDE59766.2023.10487652."
"Breathiness Indices for Classification of Dysarthria Based on Type and Speech Intelligibility,","Dysarthria classification based on intelligibility level is useful for speech pathologists for deciding the therapy. However, intelligibility assessment also depends on perceptual attributes like hypernasality, breathiness, slow rate, short pauses etc. These perceptual attributes vary depending on the cause for dysarthria giving rise to different types of dysarthria. In this work, we explore the use of breathiness features for intelligibility assessment of dysarthria and for distinguishing type of dysarthria. Voiced segments from two controlled speakers, two dysarthric speakers with low and mid intelligibility level each from UA database are used in the work. Features were analysed for use in dysarthria intelligibility assessment vs. distinguishing type of dysarthria.","keywords: {Databases;Harmonic analysis;Feature extraction;Jitter;Perturbation methods;Correlation;Support vector machines;Intelligibilty;Breathiness;Cerebral palsy;Spastic;Athetoid},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9032852&isnumber=9032713,"H. M. Chandrashekar, V. Karjigi and N. Sreedevi, ""Breathiness Indices for Classification of Dysarthria Based on Type and Speech Intelligibility,"" 2019 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET), Chennai, India, 2019, pp. 266-270, doi: 10.1109/WiSPNET45539.2019.9032852."
"Cross-lingual Dysarthria Severity Classification for English, Korean, and Tamil,","Data scarcity hinders research on dysarthria severity classification due to the limited size of datasets. While the crosslingual approach has been applied to alleviate the problem, the roles of language-specific features have been underestimated. This paper proposes a cross-lingual classification method for English, Korean, and Tamil, which employs both language-independent features and language-unique features. First, we extract thirty-nine features from diverse speech dimensions such as voice quality, pronunciation, and prosody. Second, feature selections are applied to identify the optimal feature set for each language. A set of shared features and a set of distinctive features are distinguished by comparing the feature selection results of the three languages. Lastly, automatic severity classification is performed, utilizing the two feature sets. Notably, the proposed method removes different features by languages to prevent the negative effect of unique features for other languages. Accordingly, eXtreme Gradient Boosting (XGBoost) algorithm is employed for classification, due to its strength in imputing missing data. In order to validate the effectiveness of our proposed method, two baseline experiments are conducted: experiments using the intersection set of mono-lingual feature sets (Intersection) and experiments using the union set of monolingual feature sets (Union). According to the experimental results, our method achieves better performance with a 67.14% F1 score, compared to 64.52% for the Intersection experiment and 66.74% for the Union experiment. Further, the proposed method attains better performances than mono-lingual classifications for all three languages, achieving 17.67%, 2.28%, 7.79% relative percentage increases for English, Korean, and Tamil, respectively. The result specifies that commonly shared features and language-specific features must be considered separately for cross-language dysarthria severity classification.","keywords: {Training;Deep learning;Neural networks;Interference;Information processing;Feature extraction;Boosting},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9980124&isnumber=9979808,"E. J. Yeo, K. Choi, S. Kim and M. Chung, ""Cross-lingual Dysarthria Severity Classification for English, Korean, and Tamil,"" 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 2022, pp. 566-574, doi: 10.23919/APSIPAASC55919.2022.9980124."
"The Open-Access Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) Database for Intelligent Assessment,","Early objective identification and assessment of dysarthria due to neurological deficits are essential for neurorehabilitation. Developing a system to achieve this requires a large-scale database of pathological information with detailed labeling. In the present study, a high-quality Chinese multimodal audio-visual database, consisting of 64 subacute stroke patients and 25 healthy participants, named the “Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) database”, was established. The materials of MSDM include a series of speech tasks such as syllables, characters, words, sentences, and spontaneous speech. All audio-visual data in this database were manually annotated and simultaneously verified by experienced researchers. Additionally, comprehensive clinical assessments of speech-motor function (e.g., Frenchay Dysarthria Assessment) and cognitive function (e.g., Montreal Cognitive Assessment) for each individual were included in the database. In conclusion, the MSDM database is believed to provide sufficient data resources for developing automatic assessment and speech recognition methods and contribute to understanding the pathological mechanisms of dysarthria.","keywords: {Pathology;Databases;Speech recognition;Stroke (medical condition);Neurorehabilitation;Object recognition;Labeling;Mandarin;Subacute stroke;Dysarthria;Audio-video database},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10799983&isnumber=10799969,"J. Liu et al., ""The Open-Access Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) Database for Intelligent Assessment,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 131-135, doi: 10.1109/ISCSLP63861.2024.10799983."
"Significance of Filterbank Structure for Capturing Dysarthric Information through Cepstral Coefficients,","The short-term Fourier transform magnitude spectra (STFT-MS) computed from the dysarthric speech deviates nonlinearly from the normal speech in different frequency bands depending on underlying sound units. This discriminating information can be captured by segmenting the STFT-MS into different frequency bands following the power spectra of board categories of sound units. Motivated by this observation in this study, we have computed the cepstral coefficients by analyzing the STFT-MS in 0–500 Hz, 500–2000 Hz, 2000–4000 Hz, and 4000 – 8000Hz, respectively for 16 kHz sampled speech data. Each of the selected frequency bands is analyzed by using a 30 channel Mel filterbank. The log filterbank energies computed for each sub-band are then polled together and discrete cosine transform (DCT) is applied to compute the cepstral coefficients, here termed as sub-band enhanced Mel frequency cepstral coefficients (SE-MFCC). The i-vector based dysarthric intelligibility assessment system reported in this study shows that the SEMFCC outperforms the conventional Mel frequency cepstral coefficients (MFCC), and the cepstral coefficients computed using inverse-Mel filterbank (IMFCC), and linear filterbank (LFCC). The score level combination of SE-MFCC with the MFCC further improves the overall performance.","keywords: {Fourier transforms;Conferences;Filter banks;Signal processing;Discrete cosine transforms;Mel frequency cepstral coefficient;Task analysis;Cepstral coefficients;Dysarthria;Filterbank;Inverse-Mel scale;Linear scale;Mel scale;Sub-band spectra},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9840837&isnumber=9840511,"L. P. Sahu and G. Pradhan, ""Significance of Filterbank Structure for Capturing Dysarthric Information through Cepstral Coefficients,"" 2022 IEEE International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2022, pp. 1-5, doi: 10.1109/SPCOM55316.2022.9840837."
"Data Augmentation Based on Frequency Warping for Recognition of Cleft Palate Speech,","In this paper, we present an automatic speech recognition (ASR) system for the speech of a person with a cleft lip and palate (CLP). The accuracy of speech recognition for a person with CLP is lower than that of a physically-unimpaired (PU) person because the CLP speech has characteristics that differ from those of a PU person; moreover, the amount of available training data is quite limited. In the field of ASR for PU people, data augmentation and self-supervised learning have been studied to tackle this problem of data scarcity. In this paper, we evaluate the effectiveness of those approaches on CLP speech recognition, and propose a data augmentation technique based on frequency warping. The formant of CLP speech tends to fluctuate compared to that of PU people. In order to compensate for the large variety of formant components, our data augmentation method stretches or contracts the spectrogram through the frequency axis. The experimental results on an ASR task with two CLP subjects showed that both data augmentation and self-supervised learning were effective for CLP speech recognition, and our proposed method further improved the performance of those two approaches based on conventional SpecAugment techniques.","keywords: {Lips;Training data;Speech recognition;Information processing;Character recognition;Task analysis;Speech processing;speech recognition;data augmentation;self-supervised learning;cleft lip and palate;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9689552&isnumber=9689213,"K. Fujiwara et al., ""Data Augmentation Based on Frequency Warping for Recognition of Cleft Palate Speech,"" 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Tokyo, Japan, 2021, pp. 471-476."
"Improving Pronunciation Clarity of Dysarthric Speech Using CycleGAN with Multiple Speakers,","In this paper, we propose a method that improves pronunciation clarity of dysarthric speech using CycleGAN based non-parallel voice conversion. This method converts dysarthric speech into healthy speech using CycleGAN. We considered the use of single and multiple speakers as healthy speech. The subjective evaluations showed the effectiveness of using multiple speakers as healthy speech.","keywords: {Conferences;Training data;Consumer electronics;Dysarthria;Pronunciation clarity;Voice conversion;CycleGAN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292041&isnumber=9291703,"S. Imai, T. Nose, A. Kanagaki, S. Watanabe and A. Ito, ""Improving Pronunciation Clarity of Dysarthric Speech Using CycleGAN with Multiple Speakers,"" 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), Kobe, Japan, 2020, pp. 366-367, doi: 10.1109/GCCE50665.2020.9292041."
"Adaptation of a Pronunciation Dictionary for Dysarthric Speech Recognition,","In the general framework of an automatic speech recognition system, a pronunciation dictionary, that is a mapping table from a phoneme sequence to a word, is used both in the processes of training and recognition. However, this pronunciation dictionary is not always adequate in the case of dysarthric speech recognition because dysarthric people often have difficulty pronouncing words in the same way they are pronounced in the dictionary. In this paper, we investigate the adaptation of a pronunciation dictionary to an individual dysarthric person and evaluate the effectiveness of adapting the dictionary using a dysarthric speech recognition task. In the proposed method, in order to find rules we can use to modify a dictionary, we analyze the pattern of mis-recognition in the phoneme recognition results. By following the extracted rules, we add pronunciations to the dictionary for the target dysarthric person. We evaluate the effectiveness of the adapted pronunciation dictionary on a continuous speech recognition task and demonstrate that the adapted dictionary can decrease the word error rate.","keywords: {Training;Dictionaries;Error analysis;Conferences;Speech recognition;Life sciences;Task analysis;Speech recognition;dysarthria;pronunciation dictionary;lexicon},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754848&isnumber=9754592,"Y. Sawa, R. Takashima and T. Takiguchi, ""Adaptation of a Pronunciation Dictionary for Dysarthric Speech Recognition,"" 2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech), Osaka, Japan, 2022, pp. 631-635, doi: 10.1109/LifeTech53646.2022.9754848."
"Comparative Analysis of Glottal and Vocal Tract Features in Dysarthria,","Dysarthria is a neurological disorder associated with the muscles in the vocal tract system that is caused by problems with coordination. It affects the vocal tract system and the glottis with different levels of severity. This research deals with the role of vocal tract (i.e., filter) and glottal excitation source in dysarthric speech, using Glottal Flow Model with Iterative Adaptive Inverse Filtering (GFM-IAIF). By decomposing the vocal tract and glottal source into two separate components, we were able to identify which component (source or filter) is affected the most. Our research showed that the vocal tract system is the most affected part, determining a better classification of features the magnitude spectrum-based Mel frequency cepstral coefficient (MFCC) resulted in 95.75% accuracy for vocal tract components and lower accuracy of 86.5% for glottal source components. In the same way, Modified Group Delay Cepstral Coefficients (MGDCC) correlated with a test accuracy of 94.43% of the vocal tract in comparison with 88.70% of the glottal source. These outcomes reveal that the vocal tract gets most damaged due to dysarthria and thus, emphasis the use of specific diagnostic and therapy interventions targeted at this area. The research not only explores feature fusion but also recommends further efforts to refine dysarthria diagnosis and treatment.","keywords: {Neurological diseases;Adaptation models;Accuracy;Filtering;Medical treatment;Information processing;Muscles;Iterative algorithms;Delays;Mel frequency cepstral coefficient;Dysarthria;glottal flow model using the iterative adaptive inverse filtering algorithm;phase;magnitude-based components},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10849291&isnumber=10848533,"G. S. Sahasra, K. Swapna, A. Srivastava, A. Pusuluri and H. A. Patil, ""Comparative Analysis of Glottal and Vocal Tract Features in Dysarthria,"" 2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Macau, Macao, 2024, pp. 1-6, doi: 10.1109/APSIPAASC63619.2025.10849291."
"A Fuzzy Cognitive Map Hierarchical Model for Differential Diagnosis of Dysarthrias and Apraxia of Speech,","This paper presents a novel soft computing system for differential diagnosis of the dysarthrias and apraxia of speech based on well accepted dysarthrias' classification system used by speech and language pathologists. The dysarthrias and apraxia are complex disorders of speech because they represent a variety of neurological disturbances that can potentially affect every component of speech production. Since an accurate diagnosis is a very challenging task for the clinician, the under development system based on hierarchical fuzzy cognitive maps (FCMs) will be used as a ""second opinion"" or training system. The hierarchical FCM differential diagnosis system is capable of differentiating between the six types of dysarthria as well as apraxia. The system was tested using published case studies and real patients and examples are presented here","keywords: {Fuzzy cognitive maps;Speech;Parkinson's disease;Natural languages;Muscles;Production systems;Medical treatment;Educational technology;Auditory system;System testing;Fuzzy Cognitive Maps;Differential Diagnosis;Knowledge-Based Systems;Decision Support Systems},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1616954&isnumber=33900,"V. C. Georgopoulos and G. A. Malandraki, ""A Fuzzy Cognitive Map Hierarchical Model for Differential Diagnosis of Dysarthrias and Apraxia of Speech,"" 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference, Shanghai, China, 2005, pp. 2409-2412, doi: 10.1109/IEMBS.2005.1616954."
"A Novel Gamified Approach for Collecting Speech Data from Young Children with Dysarthria: Feasibility and Positive Engagement Evaluation,","Dysarthria is a common and treatable speech problem in children, and computer-assisted speech therapy is a promising way for children's speech therapy. However, data collection poses a significant challenge for computer-assisted therapy, especially when it comes to gathering speech data from young children, particularly those with dysarthria. Finding a better way to collect young children's speech data is, therefore, an urgent need. This paper prompted a gamified speech collection method and carried out an experiment to compare the participation time, error rate, and collection efficiency with the gamified method and with a traditional method where adults are imitated. Moreover, we also explore whether the gamified collection methods increase the children's positive engagement. A feasibility study including 10 children with dysarthria and 10 children without speech problems was conducted. Their participation duration, number of spoken utterances, number of mispronunciation utterances, and a questionnaire about children's engagement attitude were recorded. The findings indicate that the gamified collecting method reduces pronunciation mistake rates in children with dysarthria while also increasing their engagement to participate.","keywords: {Visualization;Error analysis;Medical treatment;Games;Data collection;Assistive technologies;Speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10776229&isnumber=10776063,"N. Liu, E. Barakova and T. Han, ""A Novel Gamified Approach for Collecting Speech Data from Young Children with Dysarthria: Feasibility and Positive Engagement Evaluation,"" 2024 17th International Convention on Rehabilitation Engineering and Assistive Technology (i-CREATe), Shanghai, China, 2024, pp. 1-5, doi: 10.1109/i-CREATe62067.2024.10776229."
"Edge Computing Solutions Supporting Voice Recognition Services for Speakers with Dysarthria,","In the framework of Automatic Speech Recognition (ASR), the synergism between edge computing and artificial intelligence has led to the development of intelligent objects that process and respond to human speech. This acts as a key enabler for multiple application scenarios, such as smart home automation, where the user’s voice is an interface for interacting with appliances and computer systems. However, for millions of speakers with dysarthria worldwide, such a voice interaction is impossible because nowadays ASR technologies are not robust to their atypical speech commands. So these people, who also live with severe motor disabilities, are unable to benefit from many voice assistant services that might support their everyday life. To cope with the above challenges, this paper proposes a deep learning approach to isolated word recognition in the presence of dysarthria conditions, along with the deployment of customized ASR models on machine learning powered edge computing nodes. In this way, we work toward a low-cost, portable solution with the potential to operate next to the user with a disability, e.g., in a wheelchair or beside a bed, in an always active mode. Finally, experiments show the goodness (in terms of word error rate) of our speech recognition solution in comparison with other studies on isolated word recognition for impaired speech.","keywords: {Deep learning;Home appliances;Automation;Error analysis;Computational modeling;Wheelchairs;Smart homes;artificial intelligence;dysarthria;edge computing;assistive technology;smart home automation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10181220&isnumber=10181105,"D. Mulfari, L. Carnevale, A. Galletta and M. Villari, ""Edge Computing Solutions Supporting Voice Recognition Services for Speakers with Dysarthria,"" 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW), Bangalore, India, 2023, pp. 231-236, doi: 10.1109/CCGridW59191.2023.00047."
"Dysarthria Diagnosis and Dysarthric Speaker Identification Using Raw Speech Model,","Dysarthria is a medical condition that causes difficulty in producing coherent speech due to muscle paralysis or weakness. This article presents a unique approach to identifying dysarthric speakers using a deep learning model that works directly with unprocessed speech waveforms. By eliminating the need for feature extractions, the model's resistance to noise and voice variability is increased. The proposed approach utilizes a SincNet layer model with multiple initializations including Mel, Erb, and Bark scales for dysarthria detection (DD) and dysarthric speaker identification (DSI). Bark scaling, aligning better with human auditory perception and capturing distinctive acoustic features, notably outperforms other initialization methods. When Bark scaling was employed, the study's results demonstrated outstanding accuracy rates of 97.0% for DD and 88.0% for DSI. The results demonstrated exceptional performance, that surpassing existing literature benchmarks.","keywords: {Deep learning;Noise;Muscles;Benchmark testing;Feature extraction;Acoustics;Paralysis;Diagnosis of dysarthria;dysarthric speaker identification;SincNet;raw waveforms;Bark scaling;deep learning},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10485694&isnumber=10485661,"S. Sajiha, K. Radha, D. V. Rao, V. Akhila and N. Sneha, ""Dysarthria Diagnosis and Dysarthric Speaker Identification Using Raw Speech Model,"" 2024 National Conference on Communications (NCC), Chennai, India, 2024, pp. 1-6, doi: 10.1109/NCC60321.2024.10485694."
"Acoustic and Kinematic Examination of Dysarthria in Cantonese Patients of Parkinson’s Disease,","Hypokinetic dysarthria is one of the core symptoms of Parkinson's disease, characterized by reduced loudness, slurred speech and distorted consonant productions. Dopaminergic medication for Parkinson's disease (PD) has been proven to be effective in treating limb and gross motor movement problems. However, the literature sees contradictory findings regarding dopaminergic effects on speech problems associated with PD. Previous perceptual and acoustic studies of PD hypokinetic dysarthria mostly involved heterogeneous population and variations of speech tasks which complicated data interpretation. Also, the lack of kinematic data limited our understanding of the details of articulation errors associated with PD, as well as the medication effects. Electromagnetic articulography (EMA) enabled the examination of PD articulatory patterns with high degree of accuracy and safety. The aim of the present study was to address these inconsistencies by providing an integrative description of basic kinematic and acoustic parameters of speech production about the dopaminergic effect on early PD speech by using EMA. The results revealed a significant improvement of articulatory function on dopaminergic effects for PD speech, evidenced by increased vowel space, the velocity and accelerated velocity initiation and coordination of articulation during bilabial or alveolar productions.","keywords: {Kinematics;Acoustics;Auditory system;Sun;Parameter extraction;Schedules;Lips;Parkinson’;s disease, hypokinetic dysarthria, dopaminergic medication, kinematic analysis, acoustic analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8706615&isnumber=8706262,"Y. Sun, M. L. Ng, C. Lian, L. Wang, F. Yang and N. Yan, ""Acoustic and Kinematic Examination of Dysarthria in Cantonese Patients of Parkinson’s Disease,"" 2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP), Taipei, Taiwan, 2018, pp. 354-358, doi: 10.1109/ISCSLP.2018.8706615."
"In-Domain Data Augmentation to Enhance Severity Level Classification of Dysarthria from Speech,","In this paper, we present our endeavor to construct an automatic dysarthria severity level classification system tai-lored for low-resource dysarthric speech datasets. The scarcity of available speech data from dysarthric speakers poses a significant challenge to training an effective classification system. Addressing this challenge, we devised a robust baseline system by blending a distinctive set of features, encompassing temporal, prosodic, and spectral information, with the traditional MFCC features. This amalgamation aptly captures the nuanced characteristics of dysarthric speech, facilitating efficient model training. To tackle the constraints of low-resource conditions, we explored four prominent augmentation techniques: Speaking Rate, Pitch, Formant, and Vocal Tract Length Perturbation (VTLP) modification based data augmentation for the task of severity classification. The explored data augmentation gives a significant reduction in the classification error rate (CER), and VTLP-based data augmentation is superior among others. Further, we also investigated combinations of explored data augmentation methods, fortifying the reliability of our dysarthria severity classification system. The combined novel augmentation gives a noteworthy relative improvement of 42.86% over the baseline on the dysarthric severity classification.","keywords: {Training;Error analysis;System performance;Training data;Speech enhancement;Signal processing;Data augmentation;Dysarthria;severity classification;speech mod-ification;speech augmentation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10631597&isnumber=10631292,"B. Karumuru, P. Sapkota and H. Kathania, ""In-Domain Data Augmentation to Enhance Severity Level Classification of Dysarthria from Speech,"" 2024 International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2024, pp. 1-5, doi: 10.1109/SPCOM60851.2024.10631597."
"A modern approach to dysarthria classification,","This work deals with the assessment of neurological diseases known as dysarthrias, using a novel approach based on objective and perceptual features extracted from pathological speech signals. A methodology for the classification of dysarthria is developed in which digital signal processing algorithms are used to appraise the severity of those features less reliably judged by the clinicians, while the others are taken directly from perceptual judgments or medical records. The assessment process evaluates the performance of two different classifiers and compares them with the traditional assessment system. The first approach is based on the lineal discriminant analysis and the second is a non-lineal technique based on self-organizing maps. The non-lineal classifier provided the highest percent of correct classification and the most accurate information on the relevance of the features in the classifier decision. It also provided a bi-dimensional representation of de data that allows a better understanding of the correspondence between the speech deviations and the location of the damage in the peripheral or central nervous system.","keywords: {Speech processing;Speech analysis;Databases;Diseases;Lesions;Pathology;Appraisal;Medical diagnostic imaging;Central nervous system;Personal communication networks},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1280248&isnumber=28615,"E. Castillo Guerra and D. F. Lovey, ""A modern approach to dysarthria classification,"" Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No.03CH37439), Cancun, Mexico, 2003, pp. 2257-2260 Vol.3, doi: 10.1109/IEMBS.2003.1280248."
"Observations from a Simple Vocal-Tract-Model's Behaviour for PD-Dysarthric Speech: Applicability,","The uniform-element tube model of a speaker's vocal tract is a by-product from LPC speech analysis. It is used here to observe a speaker's articulation. The aim is an insight into possible articulatory weaknesses of PD (Parkinson-Disease) patients who suffer from dysarthria, i.e., speaking problems. Approved auditive methods exist for evaluation of the patients' speech handicaps, applying also instrumental signal features. But a direct view on vocal-tract movements can be an additional diagnostic aid. Due to its real-time potential, the simple estimation of vocal-tract areas from LPC analysis is regarded here, despite its known impreciseness. In a first measurement set, it is checked with fluent speech of healthy persons, whether any reaction appears on articulatory changes between clear and intentionally mumbled speech. Then, with sustained vowels of healthy and slightly as well as strongly handicapped speakers, the potential of the approach to display such differences is examined.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578023&isnumber=8577984,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578023&isnumber=8577984,"U. Heute and G. Schmidt, ""Observations from a Simple Vocal-Tract-Model's Behaviour for PD-Dysarthric Speech: Applicability,"" Speech Communication; 13th ITG-Symposium, Oldenburg, Germany, 2018, pp. 1-5."
"A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information,","Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses.","keywords: {Accuracy;Databases;Signal processing;Phonetics;Acoustics;Speech processing;Dysarthria;Multi-modal;Cross-Attention;Pronunciation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10889515&isnumber=10887541,"A. M, K. Gurugubelli, K. V and A. K. Vuppala, ""A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information,"" ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025, pp. 1-5, doi: 10.1109/ICASSP49660.2025.10889515."
"Processing of pathological changes in speech caused by dysarthria,","Computer analysis of voice isolated sounds may lead to identification of parameters correlated with neurological diseases. This paper presents results of preliminary research of voice pathological changes caused by dysarthria. The selection of linguistic material was characterized according to the place and manner of articulation in the phonetic system of Polish. Results of clinical examination allowed to determine simple markers of neurodegenerative diseases, which serves as a basis for construction of objective examination model.","keywords: {Pathology;Speech processing;Diseases;Tongue;Centralized control;Control systems;Nervous system;Isolation technology;Materials science and technology;Speech analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1595343&isnumber=33566,"T. Orzechowski, A. Izworski, R. Tadeusiewicz, K. Chmurzynska, P. Radkowski and I. Gatkowska, ""Processing of pathological changes in speech caused by dysarthria,"" 2005 International Symposium on Intelligent Signal Processing and Communication Systems, Hong Kong, China, 2005, pp. 49-52, doi: 10.1109/ISPACS.2005.1595343."
"Significance of Entropy Based Features For Dysarthric Severity Level Classification,","Dysarthria is a motor speech disorder arising from impairment of muscles that makes difficult to form or pronounce words while speaking. In this paper, we introduce an approach of multiband entropy based features extracted from dysarthric speech signals for dysarthric severity level classification. Generally, entropy is measured as the number of bits of information contained in each message signal. The information content of these signal measures how much randomness or uncertainity contains in a signal. Extending this, we use a frame-wise processing technique to divide the speech signal into short frames which allows for detailed analysis of different characteristics of speech signal. Furthermore we divide frames into equal sub bands and compute the entropy and zero mean entropy in each sub band using Gabor filterbank. It is expected that the mean entropy of very low dysarthric severity is lower compared to high dysarthric severity level indicating that randomness is increasing as the severity level of dysarthria is increasing. Experimental analysis were conducted on extensively used dataset namely UA Speech. Results were carried out by Convolutional Neural Network (CNN), along with 5-cross validation. The results are compared against standard MFCC, LFCC, and glottal source based LFRCC. It was observed that the addition of entropy information boosted the performance of MFCC by 4.38%, LFCC by 2.54%, and LFRCC by 1.88% compared to their traditional techniques indicating the crucial information captured by the entropy for dysarthria severity classification.","keywords: {Filter banks;Muscles;Feature extraction;Speech;Entropy;Noise robustness;Convolutional neural networks;Speech processing;Mel frequency cepstral coefficient;Standards;Dysarthria;Entropy;Gabor Filterbank},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10848870&isnumber=10848533,"M. Avula, A. Pusuluri and H. A. Patil, ""Significance of Entropy Based Features For Dysarthric Severity Level Classification,"" 2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Macau, Macao, 2024, pp. 1-6, doi: 10.1109/APSIPAASC63619.2025.10848870."
"Automated Acoustic Analysis in Parkinson’s Disease Using a Smartphone*,","Dysarthria is a common speech disorder in Parkinson’s Disease (PD). The Dysarthria Analyzer software has emerged as a viable tool for automatic speech analysis in PD and quantification of dysarthria severity. However, most studies use the Dysarthria Analyzer with recordings obtained under tightly controlled conditions and high-quality microphones, and the utility of the Dysarthria Analyzer when used with recordings acquired under non-ideal conditions, such as in busy clinical settings, remains unexplored. This study investigates the Dysarthria Analyzer’s performance in a setting more akin to a clinical environment using a smartphone. We obtained data from three groups, including healthy controls (HC), PD patients with their deep brain stimulation on (ON-DBS), and PD patients with their DBS off (OFF-DBS). We found a significant decrease in pitch variability and an increase in speech rate for the OFF-DBS group compared to the HC. Furthermore, most of the estimated values for the speech markers fall within the reported values in the literature. Our findings demonstrate that the Dysarthria Analyzer effectively extracts relevant speech markers even when used with recordings obtained under non-ideal conditions, emphasizing its potential for widespread clinical adoption.Clinical Relevance— Our findings demonstrate the potential of using smartphone recordings obtained in clinical environments for automatic objective speech analysis. These findings are relevant for developing a clinical tool that can be widely accessible and easily implemented during routine clinical visits of PD to improve the assessment of dysarthria in PD.","keywords: {Speech analysis;Deep brain stimulation;Software;Acoustics;Recording;Engineering in medicine and biology;Diseases;Microphones},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10782673&isnumber=10781494,"G. T. Acevedo T. et al., ""Automated Acoustic Analysis in Parkinson’s Disease Using a Smartphone*,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10782673."
"An Optimal Speech Recognition Module for Patient's Voice Monitoring System in Smart Healthcare Applications,","During recent years, health care domain has rapidly developed in which patients and medical resources are directly connected with the smart way that enables Smart Health Care. The growth in design and development of a speech automated system will provide a life assistant service in smart health care environment. In automating the speech system, speech recognition is one of the basic steps to understand the human recognition and their behaviors. These speech recognition systems will be very much accessible for speakers who suffer from dysarthria, a neurological disability that damages the control of motor speech articulators. In this paper, the main objective is to develop an efficient speech recognition module based on the Voice Input Voice Output Communication Aid (VIVOCA) architecture that can device a support aid to the people with DYSARTHRIA. Totally there are seven features extracted from each noise eliminated real time bilingual isolated word speech signal data uttered by a speaker both in Tamil and English languages. Vector Quantization based Genetic Algorithm codebook is created for the recognition modeling. Optimization of Hidden Markov Model (HMM) is done based on Particle Swarm Optimization (PSO) method to improve the recognition accuracy compared to the conventional HMM and also experiment results of the proposed module shows 95% of accuracy. The proposed module will be very much useful for developing a speech recognition system that facilitates the patients and persons with special needs for communication. The proposed module is also evaluated for its complexity which will be therefore efficient for low consumption of energy.","keywords: {Hidden Markov models;Speech recognition;Feature extraction;Genetic algorithms;Vector quantization;Training;Particle swarm optimization;Dysarthria;Smart Healthcare Applications;Isolated speech recognition;Particle Swarm Optimization (PSO);Vector Quantization (VQ) and Hidden Markov Model (HMM)},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8488841&isnumber=8488772,"M. Krishnaveni, P. Subashini, J. Gracy and M. Manjutha, ""An Optimal Speech Recognition Module for Patient's Voice Monitoring System in Smart Healthcare Applications,"" 2018 Renewable Energies, Power Systems & Green Inclusive Economy (REPS-GIE), Casablanca, Morocco, 2018, pp. 1-6, doi: 10.1109/REPSGIE.2018.8488841."
"Optical force and distance sensing in intraoral devices for stroke rehabilitation: a distance calibration and force classification approach,","Stroke survivors often suffer from oro-facial impairments, affecting swallowing function and speech production. Measuring tongue pressure and position intraorally can help to improve therapy for both symptoms, but space inside the oral cavity is extremely limited and such devices can easily be prohibitively large and obstructive if too many sensors are needed. In this work, we present our efforts to sense the force of the tongue exerted against the hard palate and the tongue-palate distance, using only optical proximity sensors. To explore the feasibility and accuracy of this approach and to evaluate the selected sensor, we conducted a study with 10 subjects and measured the sensor's response to 10 discrete distances ranging from 0mm to 30mmbetween tongue and sensor, and to a continuously increasing tongue force against the sensor from 0.1N to 8N. For distance measurements, an existing in-situ calibration method was applied and verified that yielded errors of less than 2mm for the estimated distances in nearly every case. For force measurements, a Bayesian classification approach was adopted to map sensor data to two force regions (below and above a certain boundary value), where up to 84.1% (average: 71.7 %) of ADC values were classified correctly within-sample.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578052&isnumber=8577984,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8578052&isnumber=8577984,"C. Wagner, S. Stone and P. Birkholz, ""Optical force and distance sensing in intraoral devices for stroke rehabilitation: a distance calibration and force classification approach,"" Speech Communication; 13th ITG-Symposium, Oldenburg, Germany, 2018, pp. 1-5."
"Abstract of Presentations on March 9th,",Presents abstracts for the articles comprising the conference proceedings.,URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754844&isnumber=9754592,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9754844&isnumber=9754592,"""Abstract of Presentations on March 9th,"" 2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech), Osaka, Japan, 2022, pp. 89-94, doi: 10.1109/LifeTech53646.2022.9754844."
"Speech task based automatic classification of ALS and Parkinson’s Disease and their severity using log Mel spectrograms,","We consider the task of speech based classification of patients with amyotrophic lateral sclerosis (ALS), Parkinson's disease (PD) and healthy controls (HC). Recent work in convolutional neural networks (CNN) to solve image classification problems raises the possibility of utilizing spectral representation of speech for detection of neurological diseases. In this paper, a spectrogram based approach is used. Feeding overlapping windows to the CNN makes sure that the temporal aspects are considered by using short signal segments or wide analysis filters. A three class (ALS, PD or HC) dysarthria classification is performed. In addition, we perform two severity classification experiments for ALS (5 class) and PD (3 class) respectively. Experiments are conducted on both baseline MFCC data [1] and log Mel spectrograms. Classification results show that for several audio lengths, models trained on log Mel spectrograms consistently outperform those of MFCC's. The ability of the network to accurately classify different classes is evaluated via the area under receiver operating characteristic curve [2],[3]. The findings from this study could aid in better detection and monitoring of ALS and PD diseases.","keywords: {Mel frequency cepstral coefficient;Task analysis;Spectrogram;Neurons;Muscles;Parkinson's disease;spectrograms;CNN;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9179503&isnumber=9179490,"B. Suhas et al., ""Speech task based automatic classification of ALS and Parkinson’s Disease and their severity using log Mel spectrograms,"" 2020 International Conference on Signal Processing and Communications (SPCOM), Bangalore, India, 2020, pp. 1-5, doi: 10.1109/SPCOM50965.2020.9179503."
"A Review and Classification of Amyotrophic Lateral Sclerosis with Speech as a Biomarker,","Amyotrophic Lateral Sclerosis (ALS) is a motor system neurodegenerative disease that affects speech impairment, spinal, respiratory and swallowing difficulties in patients. It has gradually increased in elderly people in recent years and is not easy to diagnose. The ALS bulbar form system is based on detecting dysarthria speech classification in discriminating healthy subjects from ALS patients. To construct the classification model by using various machine learning techniques, the studies used datasets related to speech impairment recordings of ALS patients and healthy subjects. Early diagnosis of ALS can somewhat improve the patient’s quality of life to an extent. Sustained vowel phonation is very useful for classifying ALS and healthy control (HC). In this study, jitter and shimmer were used to extract features. A support vector machine classifier was applied, and it classified ALS/HC with an accuracy of 98.5%. This study presents a detailed review of various machine learning techniques applied to the speech signal for the diagnosis of ALS and their impact on future research in this direction.","keywords: {Support vector machines;Focusing;Machine learning;Jitter;Speech enhancement;Feature extraction;Recording;Dysarthria;ALS;Speech Impairment;Machine learning;Speech disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10308048&isnumber=10306339,"S. M. Shabber, M. Bansal and K. Radha, ""A Review and Classification of Amyotrophic Lateral Sclerosis with Speech as a Biomarker,"" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-7, doi: 10.1109/ICCCNT56998.2023.10308048."
"Teager Energy Cepstral Coefficients For Classification of Dysarthric Speech Severity-Level,","Dysarthria is a neuro-motor speech impairment that renders speech unintelligibility, which is generally imperceptible to humans w.r.t severity-levels. Dysarthric speech classification acts as a diagnostic tool for evaluating the advancement in a patient's severity condition and also aids in automatic dysarthric speech recognition systems (an important assistive speech technology). This study investigates the significance of Teager Energy Cepstral Coefficients (TECC) in dysarthric speech classification using three deep learning architectures, namely, Convolutional Neural Network (CNN), Light-CNN (LCNN), and Residual Networks (ResNet). The performance of TECC is compared with state-of-the-art features, such as Short-Time Fourier Transform (STFT), Mel Frequency Cepstral Coefficients (MFCC), and Linear Frequency Cepstral Coefficients (LFCC). In addition, this study also investigate the effectiveness of cepstral features over the spectral features for this problem. The highest classification accuracy achieved using UA-Speech corpus is 97.18%, 94.63%, and 98.02% (i.e., absolute improvement of 1.98%, 1.41%, and 1.69%) with CNN, LCNN, and ResNet, respectively, as compared to the MFCC. Further, we evaluate feature discriminative capability using $F1$-score, Matthew's Correlation Coefficient (MCC), Jaccard index, and Hamming loss. Finally, analysis of latency period w.r.t. state-of-the-art feature sets indicates the potential of TECC for practical deployment of the severity-level classification system.","keywords: {Correlation coefficient;Fourier transforms;Deep architecture;Speech recognition;Information processing;Convolutional neural networks;Indexes;Dysarthria;UA-Speech Corpus;TEO Profiles;TECC. 1},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9980322&isnumber=9979808,"A. Kachhi, A. Therattil, A. T. Patil, H. B. Sailor and H. A. Patil, ""Teager Energy Cepstral Coefficients For Classification of Dysarthric Speech Severity-Level,"" 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Chiang Mai, Thailand, 2022, pp. 1462-1468, doi: 10.23919/APSIPAASC55919.2022.9980322."
"Sequence-to-Sequence Models in Italian Atypical Speech Recognition,","In the domain of automatic speech recognition (ASR), we explore the usage of a state-of-the-art transformer-based sequence-to-sequence model to build a speaker-dependent isolated word recognizer for native Italian speakers with a speech disorder, such as dysarthria. In particular, this paper is concerned with a self-supervised learning approach, where the Wav2Vec2 has been fine-tuned on our private Italian corpus containing a total of 41 hours of speech contributions authored by 191 individuals with a disability and atypical speech. The discussed approach has been also evaluated thanks to collaboration of sixteen speakers with diverse degrees of speech disorders (mild, moderate, severe), and our analysis has shown a remarkable performance of our ASR system, with an overall word recognition accuracy of 97.4%.","keywords: {Computers;Dictionaries;Databases;Collaboration;Speech recognition;Self-supervised learning;Computer architecture;Transformers;Recording;Context modeling;Atypical speech recognition;self-supervised learning;dysarthria;transformers;ASR},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10733600&isnumber=10733557,"D. Mulfari, L. Carnevale and M. Villari, ""Sequence-to-Sequence Models in Italian Atypical Speech Recognition,"" 2024 IEEE Symposium on Computers and Communications (ISCC), Paris, France, 2024, pp. 1-6, doi: 10.1109/ISCC61673.2024.10733600."
"Monitoring Progress of Parkinson's Disease Based on Changes in Phonation: a Pilot Study,","Hypokinetic dysarthria (HD) is a frequent symptom of idiopathic Parkinson's disease (PD). Although it is hypothesized its progress is tightly linked with changes in other motor/non-motor features of PD, it has not been proved yet. The aim of this work is to employ acoustic analysis of sustained phonation in order to identify significant correlates between phonatory measures and motor/non-motor deficits in a two-year follow-up study. For this purpose, we repeatedly quantified a sustained vowel/a/ in 51 PD patients who were also assessed by 5 common clinical scales. In addition, a multivariate regression model was trained to predict the motor/non-motor deficits in the horizon of two years. Results suggest that mainly instability in vocal folds oscillation increases with the progress of PD and with overall cognitive decline. Based on the acoustic analysis, the change in clinical scores could be predicted with the error in the range of 11.83-19.60 %.","keywords: {Acoustics;Indexes;Correlation;Parkinson's disease;Multivariate regression;Monitoring;Standards;acoustic analysis;follow-up study;hypokinetic dysarthria;Parkinson's disease;phonation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8441307&isnumber=8441166,"Z. Galaz et al., ""Monitoring Progress of Parkinson's Disease Based on Changes in Phonation: a Pilot Study,"" 2018 41st International Conference on Telecommunications and Signal Processing (TSP), Athens, Greece, 2018, pp. 1-5, doi: 10.1109/TSP.2018.8441307."
"An automatic diagnosis and assessment of dysarthric speech using speech disorder specific prosodic features,","To diagnose and classify the dysarthric speech, speech language pathologist (SLP) conducts a listening test. On the basis of the scores given by listeners the dysarthria is diagnosed and assessed. The above mentioned method is costly, time consuming and not very accurate. Unlike the traditional method, this research proposes an automatic diagnosis and assessment of dysarthria. The aim of this paper is to diagnose and classify the severity of dysarthria. The speech disorder specific prosodic features are selected by using genetic algorithm. The diagnosis and assessment of dysarthric speech is done by support vector machines. During diagnosis the classification accuracy of 98% has been achieved. And 87% of the dysarthric speech utterances are correctly classified. The standard UASPEECH database has been used in this work.","keywords: {Speech;Feature extraction;Testing;Genetic algorithms;Support vector machines;Databases;Training;Dysarthric speech;diagnosis;assessment;speech disorder;prosodic features;support vector machines},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7760933&isnumber=7760810,"G. Vyas, M. K. Dutta, J. Prinosil and P. Harár, ""An automatic diagnosis and assessment of dysarthric speech using speech disorder specific prosodic features,"" 2016 39th International Conference on Telecommunications and Signal Processing (TSP), Vienna, Austria, 2016, pp. 515-518, doi: 10.1109/TSP.2016.7760933."
"Signal Analysis for Voice Evaluation in Parkinson’s Disease,","Parkinson's Disease (PD) is a neurodegenerative disorder that is frequently correlated with vowel articulation difficulties. The phonation problem arises in patients affected by PD is commonly known as Parkinsonian Dysarthria and identifiedby vocal signal analysis. The analysis supporte physicians and specialists in early detection and monitoring of dysarthria aiming, to increase patients life quality and to evaluate the efficacy of treatments. We investigate on vocal signal analysis correlation with speech patterns related to PD. Vowel parameters are considered as discriminant elements among PD patients and healthy subjects. Aim of this work is to define possible indicators for dysarthria in PD patients.","keywords: {Tongue;Signal analysis;Diseases;Speech;Electronic mail;Correlation;Jitter;Parkinson Disease;vocal signal analysis;vowel metric;acoustic analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8031207&isnumber=8031114,"D. Mirarchi, P. Vizza, G. Tradigo, N. Lombardo, G. Arabia and P. Veltri, ""Signal Analysis for Voice Evaluation in Parkinson’s Disease,"" 2017 IEEE International Conference on Healthcare Informatics (ICHI), Park City, UT, USA, 2017, pp. 530-535, doi: 10.1109/ICHI.2017.72."
"Two-stage and Self-supervised Voice Conversion for Zero-Shot Dysarthric Speech Reconstruction,","Dysarthria is a motor speech disorder commonly associated with conditions such as cerebral palsy, Parkinson’s disease, amyotrophic lateral sclerosis, and stroke. Individuals with dysarthria typically exhibit significant speech difficulties, including imprecise articulation, lack of fluency, slow speech rate, and decreased volume and clarity, which can hinder their ability to communicate effectively with others. We propose a two-stage Voice Conversion method to enhance the reconstruction of dysarthric speech. In the first stage, we develop a KNN-VC approach based on a same-gender-retrieval strategy to preliminarily repair the dysarthric speech. In this stage, we match the dysarthric speech only with normal speech of the same gender. In the second stage, we adapt so-vits-svc to restore the speaker’s timbre and improve the sound quality of the speech repaired in the first stage. Both objective and subjective evaluations were conducted on the dataset of the Low Resource Dysarthria Wake-Up Word Spotting Challenge (LRDWWS Challenge) shows that the proposed approach can achieve some improvements in terms of speaker similarity, speech intelligibility and naturalness for unknown speakers, and these evaluations also show our method has a good Zero-shot performance. Our audio samples can be accessed online 1.","keywords: {Cerebral palsy;Nearest neighbor methods;Maintenance engineering;Speech enhancement;Motors;Timbre;Diseases;dysarthric speech reconstruction;Any-to-any;Zero-shot;voice conversion},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10661160&isnumber=10660673,"D. Liu, Y. Lin, H. Bu and M. Li, ""Two-stage and Self-supervised Voice Conversion for Zero-Shot Dysarthric Speech Reconstruction,"" 2024 International Conference on Asian Language Processing (IALP), Hohhot, China, 2024, pp. 423-427, doi: 10.1109/IALP63756.2024.10661160."
"Computer aided methods for diagnosis and therapy of speech breathing disorders,","Computer-aided methods for the diagnosis and therapy of speech breathing disorders caused by brain damage (dysarthrias) are described. These methods make it possible to evaluate a speech disorder for diagnostic purposes using objective parameters. Special therapeutic tasks which proceed in accordance with the principle of biofeedback make it possible for the patient to carry out self-therapy through individualized exercise packages. Pathological speech patterns can be readily recognized, classified and quantified using the methods presented. It is concluded that these methods are clinically practical and show promise of becoming a useful clinical tool for the treatment of dysarthrias.<>","keywords: {Medical treatment;Belts;Speech processing;Military computing;Biological control systems;Immune system;Automatic control;Context;Speech analysis;Pathology},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=95920&isnumber=3080,"M. Finsterwald, M. Vogel and K. Trondle, ""Computer aided methods for diagnosis and therapy of speech breathing disorders,"" Images of the Twenty-First Century. Proceedings of the Annual International Engineering in Medicine and Biology Society,, Seattle, WA, USA, 1989, pp. 663-664 vol.2, doi: 10.1109/IEMBS.1989.95920."
"Automatic detection of voice onset time in dysarthric speech,","Although a number of speech disorders reflect varying involvement of brain areas, recently published automatic speech analyses have primarily been limited to hypokinetic dysarthria in Parkinson's disease (PD). Therefore, the aim of the present study was to provide an automatic algorithm suitable for the assessment of voice onset time (VOT) in various dysarthria types. Twenty-four PD participants with hypokinetic dysarthria and 40 Huntington's disease (HD) subjects with hyperkinetic dysarthria were included. These two types of dysarthria were selected in the design of a robust algorithm as they contain most of the dysarthric patterns found among all dysarthria subtypes. For a 10 ms threshold, the proposed algorithm reached approximately 90% accuracy in PD speakers and 80% accuracy in HD speakers. The accuracy of 80% obtained in HD was superior to the performance of 55% achieved by a previous algorithm designed particularly for hypokinetic dysarthria in PD.","keywords: {High definition video;Speech;Diseases;Algorithm design and analysis;Estimation;Robustness;Accuracy;Voice Onset Time;Dysarthria;Parkinson's disease;Huntington's disease;Speech disorder},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178790&isnumber=7177909,"M. Novotný, J. Pospíšil, R. Čmejla and J. Rusz, ""Automatic detection of voice onset time in dysarthric speech,"" 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), South Brisbane, QLD, Australia, 2015, pp. 4340-4344, doi: 10.1109/ICASSP.2015.7178790."
"Hybrid CNN-GRU Model for Predicting Dysarthric Speech Using Deep Learning Approaches,","This research study presents the novel method for dysarthria speech detection and classification. In order to create systems that are capable of accurately categorizing speech patterns that are impacted by dysarthria, it is vital to have speech detection and classification capabilities for dysarthria. The condition known as dysarthria, which is characterized by difficulties in articulation, phonation, resonance, and prosody, is a disorder that leads to a reduction in the intelligibility of speech. This method of classification necessitates the development of machine learning or deep learning models, more specifically a combination of Convolutional Neural Networks (CNN) and Gated Recurrent Units (GRU), in order to accomplish the objective of automatically classifying dysarthric speech into categories such as severity or presence/absence of dysarthria. The primary purpose of dysarthria speech categorization is to bring about the development of a reliable instrument that can be used for identifying, categorizing, and managing dysarthric speech. As an added benefit, the classification of dysarthric speech allows for the adaptation of treatment methods and the acquisition of information about the underlying causes of the condition. The individuals who are affected by this speech difficulty are the target audience for this strategy, which ultimately aims to enhance the quality of life of those individual. The study addresses the challenge of classifying dysarthric speech caused by neurological impairments using a Hybrid CNN-GRU model, achieving accuracies of 70.3% (high-severity), 38% (severe-moderate), 34.5% (moderate), and 10.3% (mild dysarthria).","keywords: {Voice activity detection;Deep learning;Accuracy;Transfer learning;Speech recognition;Speech enhancement;Predictive models;Feature extraction;Robustness;Convolutional neural networks;Dysarthria Speech disordered;Convolutional Neural Networks;Gated Recurrent Units;Natural language processing},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10915057&isnumber=10914686,"S. K. Chelliah and A. N, ""Hybrid CNN-GRU Model for Predicting Dysarthric Speech Using Deep Learning Approaches,"" 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), Bengaluru, India, 2025, pp. 1880-1885, doi: 10.1109/IDCIOT64235.2025.10915057."
"Smart Voice Assistance for Speech disabled and Paralyzed People,","People who are paralyzed, confront numerous challenges in meeting their basic necessities on a daily basis. It is very difficult to understand the speech of people with dysarthria, amyotrophic lateral sclerosis (ALS) and similar conditions. Automatic speech command recognition system will enhance the lifestyle of people with voice disorder like dysarthria and paraplegics. The proposed work will convert the speech command of paralyzed people into text and send it to the care taker's mobile with the help of Twilio message services. Algorithms like Support Vector Machine (SVM) and Convolutional Neural network (CNN) model is used for speech command identification and speech to text conversion. CNN model yields an accuracy of 90.62%, whereas the SVM algorithm gives a very low accuracy. The developed TensorFlow model is deployed in the flask server.","keywords: {Support vector machines;Text categorization;Speech recognition;Speech enhancement;Feature extraction;Message service;Convolutional neural networks;text classification;raspberry pi;convolutional neural network;speech disability;MFCC},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9740922&isnumber=9740653,"L. T, I. R, A. A, A. K. S and S. S, ""Smart Voice Assistance for Speech disabled and Paralyzed People,"" 2022 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 2022, pp. 1-5, doi: 10.1109/ICCCI54379.2022.9740922."
"A Tool for Training Speech Imitation Accuracy,","Dysarthria is a neurological motor speech disorder that commonly results in reduced intelligibility. Communication partners can learn to better understand the speech of someone with dysarthria through perceptual training. Vocal imitation of the degraded speech during perceptual training has been shown to elevate this learning. A tool that provides the learner with real-time feedback regarding the accuracy of their imitation attempts during training may further enhance this learning. We describe a training tool that compares dysarthric speech productions with the imitation attempts of healthy subjects, using a two-level dynamic warp that accounts for both spectral and temporal degradation. Feature vectors derived from both the spectrogram and LPC are examined.","keywords: {Training;Tools;Distortion measurement;Heuristic algorithms;Distortion;Microsoft Windows;Feature extraction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8645286&isnumber=8645065,"A. -W. Al-Dulaimi, S. Budge, S. A. Borrie, T. K. Moon and J. H. Gunther, ""A Tool for Training Speech Imitation Accuracy,"" 2018 52nd Asilomar Conference on Signals, Systems, and Computers, Pacific Grove, CA, USA, 2018, pp. 1086-1090, doi: 10.1109/ACSSC.2018.8645286."
"Leveraging OpenAI Whisper Model to Improve Speech Recognition for Dysarthric Individuals,","Automatic Speech Recognition (ASR) systems are pivotal in facilitating human-technology interactions through voice commands. However, individuals with dysarthria face significant challenges in benefiting from these technologies due to their speech disorder. This paper proposes finetuning the Whisper model for Dysarthric Speech Recognition (DSR) by incorporating additional features extracted from Mel-frequency cepstral coefficients (MFCCs). By combining spectrograms and MFCCs within an attention mechanism, the model creates a richer feature representation, with spectrograms providing broader context and MFCCs highlighting crucial formant frequencies. The attention mechanism dynamically weighs the importance of each feature based on specific speech segments and dysarthric speech characteristics. Furthermore, a hierarchical attention approach is adopted, which encompasses a two-stage attention mechanism. This mechanism directs attention at both local and global levels, facilitating the capture of both fine-grained details and broader contextual information within the speech signal. This study involved the development and training of 45 speaker-adaptive dysarthric ASR systems. The proposed model achieves an average Word Recognition Accuracy (WRA) of 74.08%, showing a notable enhancement compared to the benchmark of 69.23%. The findings underscore the efficacy of the proposed approach in addressing dysarthria-related challenges in ASR systems.","keywords: {Training;Technological innovation;Attention mechanisms;Accuracy;Face recognition;Benchmark testing;Speech enhancement;DSR;MFCC;UASpeech;WRA;Whisper},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10673628&isnumber=10673418,"V. R, H. D and L. D. V. Anand, ""Leveraging OpenAI Whisper Model to Improve Speech Recognition for Dysarthric Individuals,"" 2024 Asia Pacific Conference on Innovation in Technology (APCIT), MYSORE, India, 2024, pp. 1-5, doi: 10.1109/APCIT62007.2024.10673628."
"Empirical Analysis of Machine Learning Models on Parkinson’s Speech Dataset,","Parkinson’s disease (PD) is a chronic and progressive neurodegenerative disorder that worsens over time. Diagnosing PD primarily relies on clinical assessments, which can be costly, time-consuming, and invasive. These evaluations may also be subjective and vulnerable to inaccuracy. Dysarthria, a common condition characterised by delayed and distorted speech, frequently coexists with PD. This opens up the possibility of using speech features for diagnostic reasons. This research paper explores different machine learning models trained on numerical data of changes in speech patterns due to Dysarthria. These models are based on classifiers such as Artificial Neural Networks (ANN), Multi-Layer Perceptron (MLP), Random Forests, and Decision Trees. Additionally, we compare the performance of a newly introduced HyperTab classifier with the existing models. Our findings demonstrate the significant potential of machine learning in diagnosing PD based on speech analysis. This progress holds the promise of creating a cost-effective tool to expedite disease detection. Furthermore, this research is of utmost importance in offering essential support to regions with limited access to specialized medical facilities.","keywords: {Analytical models;Biological system modeling;Artificial neural networks;Predictive models;Feature extraction;Numerical models;Recording;HyperTab;Classification;Evaluation Metrics;Deep Learning;Parkinson’s Disease},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10482963&isnumber=10482911,"B. Sachdeva, H. Rathee, P. Gambhir and P. Bansal, ""Empirical Analysis of Machine Learning Models on Parkinson’s Speech Dataset,"" 2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Delhi, India, 2023, pp. 1-5, doi: 10.1109/O-COCOSDA60357.2023.10482963."
"Investigation of Cross Modality Feature Fusion for Audio-Visual Dysarthric Speech Assessment,","Dysarthria, a speech disorder resulting from neurological conditions, presents significant obstacles to speech intelligibility and daily communication. Automatic dysarthria assessment has the capability to provide low-cost diagnosis and treatment assistant support for such diseases as Parkinson's disease, Alzheimer's disease, and stroke. This study investigates the efficacy of cross-modality feature fusion using audio-visual data for the automatic assessment of dysarthric speech. Leveraging advanced self-supervised learning models, AV-HuBERT and Wav2Vec 2.0, we develop a multimodal system to enhance dysarthria severity classification. Utilizing the Mandarin Subacute Stroke Dysarthria Multimodal (MSDM) dataset, which includes synchronized audio and lip movement video recordings, our system achieves promising performance. Experimental results demonstrate that our back-end fusion and feature fusion approaches both outperform traditional single-modality methods, with the best back-end fusion system achieving a speaker-level F1 score of 0.841 while the best feature-level fusion system achieving a speaker-level F1 score of 0.772. This study marks the first application of pre-trained self-supervised learning models for multimodal dysarthria assessment, highlighting the potential for the assistance of diagnosis and treatment.","keywords: {Visualization;Parkinson's disease;Lips;Self-supervised learning;Feature extraction;Synchronization;Alzheimer's disease;Video recording;dysarthria speech;automatic assessment;modality fusion;AV-HuBERT;Wav2Vec 2.0},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800618&isnumber=10799969,"Y. Jiang et al., ""Investigation of Cross Modality Feature Fusion for Audio-Visual Dysarthric Speech Assessment,"" 2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP), Beijing, China, 2024, pp. 141-145, doi: 10.1109/ISCSLP63861.2024.10800618."
"Experimental Investigation on STFT Phase Representations for Deep Learning-Based Dysarthric Speech Detection,","Mainstream deep learning-based dysarthric speech detection approaches typically rely on processing the magnitude spectrum of the short-time Fourier transform of input signals, while ignoring the phase spectrum. Although considerable insight about the structure of a signal can be obtained from the magnitude spectrum, the phase spectrum also contains inherent structures which are not immediately apparent due to phase discontinuity. To reveal meaningful phase structures, alternative phase representations such as the modified group delay (MGD) and instantaneous frequency (IF) spectra have been investigated in several applications. The objective of this paper is to investigate the applicability of the unprocessed phase, MGD, and IF spectra for dysarthric speech detection. Experimental results show that dysarthric cues are present in all considered phase representations. Further, it is shown that using phase representations as complementary features to the magnitude spectrum is beneficial for deep learning-based dysarthric speech detection, with the combination of magnitude and IF spectra yielding a high performance. The presented results should raise awareness in the research community about the potential of the phase spectrum for dysarthric speech detection and motivate research into novel architectures which optimally exploit magnitude and phase information.","keywords: {Voice activity detection;Fourier transforms;Conferences;Signal processing;Feature extraction;Acoustics;Delays;phase;modified group delay;instantaneous frequency;CNN;dysarthria},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747205&isnumber=9746004,"P. Janbakhshi and I. Kodrasi, ""Experimental Investigation on STFT Phase Representations for Deep Learning-Based Dysarthric Speech Detection,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 6477-6481, doi: 10.1109/ICASSP43922.2022.9747205."
"Speech Recognition-Based Feature Extraction For Enhanced Automatic Severity Classification in Dysarthric Speech,","Due to the subjective nature of current clinical evaluation, the need for automatic severity evaluation in dysarthric speech has emerged. DNN models outperform ML models but lack user-friendly explainability. ML models offer explainable results at a feature level, but their performance is comparatively lower. Current ML models extract various features from raw waveforms to predict severity. However, existing methods do not encompass all dysarthric features used in clinical evaluation. To address this gap, we propose a feature extraction method that minimizes information loss. We introduce an ASR transcription as a novel feature extraction source. We finetune the ASR model for dysarthric speech, then use this model to transcribe dysarthric speech and extract word segment boundary information. It enables capturing finer pronunciation and broader prosodic features. These features demonstrated an improved severity prediction performance to existing features: balanced accuracy of 83.72%.","keywords: {Accuracy;Conferences;Speech recognition;Speech enhancement;Predictive models;Feature extraction;Data mining;Dysarthria severity classification;dysarthric speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10832261&isnumber=10830793,"Y. Choi, J. Lee and M. -W. Koo, ""Speech Recognition-Based Feature Extraction For Enhanced Automatic Severity Classification in Dysarthric Speech,"" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 953-960, doi: 10.1109/SLT61566.2024.10832261."
"The Nemours database of dysarthric speech,","The Nemours database is a collection of 814 short nonsense sentences; 74 sentences spoken by each of 11 male speakers with varying degrees of dysarthria. Additionally, the database contains two connected-speech paragraphs produced by each of the 11 speakers. The database was designed to test the intelligibility of dysarthric speech before and after enhancement by various signal processing methods, and is available on CD-ROM. It can also be used to investigate general characteristics of dysarthric speech such as production error patterns. The entire database has been marked at the word level and sentences for 10 of the 11 talkers have been marked at the phoneme level as well. The paper describes the database structure and techniques adopted to improve the performance of a Discrete Hidden Markov Model (DHMM) labeler used to assign initial phoneme labels to the elements of the database. These techniques may be useful in the design of automatic recognition systems for persons with speech disorders, especially when limited amounts of training data are available.","keywords: {Databases;Signal design;Testing;Speech enhancement;Speech processing;Signal processing;CD-ROMs;Hidden Markov models;Automatic speech recognition;Training data},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=608020&isnumber=13325,"X. Menendez-Pidal, J. B. Polikoff, S. M. Peters, J. E. Leonzio and H. T. Bunnell, ""The Nemours database of dysarthric speech,"" Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96, Philadelphia, PA, USA, 1996, pp. 1962-1965 vol.3, doi: 10.1109/ICSLP.1996.608020."
"Corpus Design and Automatic Speech Recognition for Deaf and Hard-of-Hearing People,","This study describes automatic speech recognition (ASR) for the deaf and hard-of-hearing people. In the relevant literature, ASR for the deaf has been studied in a manner similar to the recognition of speech by people with dysarthria. However, prior studies have been conducted over a small number of deaf speakers. Therefore, to date, it remains unclear how the performance of ASR varies with different speakers. We conducted phoneme recognition experiments using speech from 12 deaf students to obtain analytical results from the perspective of ASR performance.","keywords: {Conferences;Consumer electronics;Automatic speech recognition;ASR;deaf speech;end-to-end;classification},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9621959&isnumber=9621353,"A. Kobayashi, K. Yasu, H. Nishizaki and N. Kitaoka, ""Corpus Design and Automatic Speech Recognition for Deaf and Hard-of-Hearing People,"" 2021 IEEE 10th Global Conference on Consumer Electronics (GCCE), Kyoto, Japan, 2021, pp. 17-18, doi: 10.1109/GCCE53005.2021.9621959."
"Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition,","Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)","keywords: {Medical treatment;Speech recognition;Predictive models;Stroke (medical condition);Speech enhancement;Signal processing;IP networks;Dysarthric Speech;Inappropriate Pause Detection;Pause Detection;Speech Recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10447681&isnumber=10445803,"J. Lee, Y. Choi, T. -J. Song and M. -W. Koo, ""Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition,"" ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 12486-12490, doi: 10.1109/ICASSP48485.2024.10447681."
"Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals,","Brain-computer interfaces (BCIs) can potentially restore lost function in patients with neurological injury. A promising new application of BCI technology has focused on speech restoration. One approach is to synthesize speech from the neural correlates of a person who cannot speak, as they attempt to do so. However, there is no established gold-standard for quantifying the quality of BCI-synthesized speech. Quantitative metrics, such as applying correlation coefficients between true and decoded speech, are not applicable to anarthric users and fail to capture intelligibility by actual human listeners; by contrast, methods involving people completing forced-choice multiple-choice questionnaires are imprecise, not practical at scale, and cannot be used as cost functions for improving speech decoding algorithms. Here, we present a deep learning-based “AI Listener” that can be used to evaluate BCI speech intelligibility objectively, rapidly, and automatically. We begin by adapting several leading Automatic Speech Recognition (ASR) deep learning models - Deepspeech, Wav2vec 2.0, and Kaldi - to suit our application. We then evaluate the performance of these ASRs on multiple speech datasets with varying levels of intelligibility, including: healthy speech, speech from people with dysarthria, and synthesized BCI speech. Our results demonstrate that the multiple-language ASR model XLSR-Wav2vec 2.0, trained to output phonemes, yields superior performance in terms of speech transcription accuracy. Notably, the AI Listener reports that several previously published BCI output datasets are not intelligible, which is consistent with human listeners.","keywords: {Measurement;Deep learning;Correlation coefficient;Neural engineering;Cost function;Brain-computer interfaces;Decoding;Brain Computer Interface (BCI);Automatic Speech Recognition (ASR);Speech Intelligibility;Speech Synthesis;Performance Metrics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10123751&isnumber=10123710,"S. Varshney, D. Farias, D. M. Brandman, S. D. Stavisky and L. M. Miller, ""Using Automatic Speech Recognition to Measure the Intelligibility of Speech Synthesized From Brain Signals,"" 2023 11th International IEEE/EMBS Conference on Neural Engineering (NER), Baltimore, MD, USA, 2023, pp. 1-6, doi: 10.1109/NER52421.2023.10123751."
"Contemporary speech/speaker recognition with speech from impaired vocal apparatus,","Speech is the effective form of communication between human and its environment. Speech also has potential of being important mode of interaction with computer. This review paper deals with both speech and speaker recognition of persons with speech motor disorders. Normally speaker recognition consists of speaker verification and speaker identification. Speaker identification is the process of determining which registered speaker provides a given input sample. Speaker verification is the process of accepting or rejecting the identity claim of a speaker. On the other hand, the speech recognition system deals with the following challenges such as speech representation, feature extraction techniques, speech classifiers, databases and performance evaluation. Motor speech disorders are a class of speech disorder that disturbs the body's natural ability to speak. These disturbances vary in their etiology based on the integrity and integration of cognitive, neuromuscular, and musculoskeletal activities. There are various types of speech disorders like Apraxia, Cluttering (similar to stuttering), Dyspraxia, Dysarthria, Dysprosody and so on. The main objective of this review paper is to summarize and compare the well known methods used in various stages of speech and speaker recognition system.","keywords: {Speech;Hidden Markov models;Speech recognition;Databases;Speaker recognition;Feature extraction;Markov processes;Speech recognition;Speaker recognition;Speech motor disorders;Feature extraction;Speech classifiers},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7062754&isnumber=7062711,"S. Selva Nidhyananthan, R. Shantha Selvakumari and V. Shenbagalakshmi, ""Contemporary speech/speaker recognition with speech from impaired vocal apparatus,"" 2014 International Conference on Communication and Network Technologies, Sivakasi, India, 2014, pp. 198-202, doi: 10.1109/CNT.2014.7062754."
"Speech training system based on resonant frequencies of vocal tract,","Speech sounds are air pressure vibrations produced by air exhaled from the lungs and modulated and shaped by the vibrations of the glottal cords and the vocal tract as it is pushed out through the lips and nose. Speech signals, in addition to communicating the linguistic information, convey a multitude of other information including gender, age, accent, intent, emotion, humor and the state of health of the speaker. There are several neurological (e.g., aphasia, dysarthria, and apraxia) or anatomical (e.g., cleft lip and palate) factors that could affect the intelligibility and audibility of the human speech. In this paper we present our work in developing a training system based on the resonant frequencies (widely known as formants) of the vocal tract to help a human subject with speech impairment train himself or herself to improve the intelligibility and audibility of his or her speech.","keywords: {Speech;Cepstrum;Discrete Fourier transforms;Training;Speech processing;Resonant frequency;formants;impaired speech;speech training system;vocal tract resonse;cepstrum},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5745902&isnumber=5745722,"V. S. Selvam, V. Thulasibai and R. Rohini, ""Speech training system based on resonant frequencies of vocal tract,"" 13th International Conference on Advanced Communication Technology (ICACT2011), Gangwon, Korea (South), 2011, pp. 674-679."
"Analysis of adverse effects of stimulation during DBS surgery by patient-specific FEM simulations,","Deep brain stimulation (DBS) represents today a well-established treatment for movement disorders. Nevertheless the exact mechanism of action of DBS remains incompletely known. During surgery, numerous stimulation tests are frequently performed in order to evaluate therapeutic and adverse effects before choosing the optimal implantation site for the DBS lead. Anatomical structures responsible for the induced adverse effects have been investigated previously, but only based on stimulation data obtained with the implanted DBS lead. The present study introduces a methodology to identify these anatomical structures during intraoperative stimulation tests based on patient-specific electric field simulations and visualization on the patient specific anatomy. The application to 4 patients undergoing DBS surgery and presenting dysarthria, paresthesia or pyramidal effects shows the different anatomical structures, which might be responsible for the adverse effects. Several of the identified structures have been previously described in the literature. To draw any statistically significant conclusions, the methodology has to be applied to further patients. Together with the visualization of the therapeutic effects, this new approach could assist the neurosurgeons in the future in choosing the optimal implant position.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512796&isnumber=8512178,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512796&isnumber=8512178,"A. A. Shah et al., ""Analysis of adverse effects of stimulation during DBS surgery by patient-specific FEM simulations,"" 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, USA, 2018, pp. 2222-2225, doi: 10.1109/EMBC.2018.8512796."
"Targeting Friedreich Ataxia: A Sustainable Path to Safer and Smarter Therapeutics Through Integrated Docking and Toxicology,","Friedreich's ataxia (FA) is an autosomal recessive disorder affecting the nervous and cardiovascular systems, characterized by progressive ataxia, dysarthria, and muscle weakness. It results from a GAA trinucleotide repeat expansion in the FXN gene, leading to epigenetic silencing via heterochromatin formation, which reduces frataxin production-a mitochondrial protein essential for iron-sulphur cluster biogenesis and cellular energy production. Frataxin deficiency causes oxidative stress and mitochondrial dysfunction. Current treatments offer limited effectiveness, primarily addressing symptoms. Our study aims to develop novel therapeutic candidates by leveraging computational protein-ligand docking through the Galaxy EU platform, targeting frataxin deficiencies. High-throughput docking and toxicology studies have identified several promising compounds with potential therapeutic efficacy, offering hope for more effective and sustainable treatments for FA.","keywords: {Proteins;Cardiovascular system;Toxicology;Gallium arsenide;Production;Muscles;Epigenetics;Compounds;Information technology;Stress},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10816941&isnumber=10816707,"H. B. Attel, R. P, A. K, B. S, Shivandappa and S. Manokaran, ""Targeting Friedreich Ataxia: A Sustainable Path to Safer and Smarter Therapeutics Through Integrated Docking and Toxicology,"" 2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS), Bengaluru, India, 2024, pp. 1-4, doi: 10.1109/CSITSS64042.2024.10816941."
"Check Your Audio Data: Nkululeko for Bias Detection,","We present a new release of the software tool Nkululeko. New additions enable users to automatically perform sanity checks, data cleaning, and bias detection in the data based on machine learning predictions. Two open-source databases from the medical domain are investigated: the Androids de-pression corpus and the UASpeech dysarthria corpus. Results show that both databases have some bias, but not in a severe manner.","keywords: {Databases;Machine learning;Cleaning;Software tools;open-source tool;machine learning;bias detection;speaker characteristics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10800580&isnumber=10799972,"F. Burkhardt, B. T. Atmaja, A. Derington, F. Eyben and B. Schuller, ""Check Your Audio Data: Nkululeko for Bias Detection,"" 2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Hsinchu City, Taiwan, 2024, pp. 1-6, doi: 10.1109/O-COCOSDA64382.2024.10800580."
"Extraction of patients subpopulations with psychiatric symptoms using a transformer architecture,","In this paper, we demonstrate a novel pipeline for identifying and extracting patient subpopulations from unstructured physician’s notes. We validate the method by extracting patients with psychiatric issues from a general patient population. This method first uses a clinical metathesaurus to select terms of interest from reports, then vectorizes the terms using a transformer model. These vectors’ dimensions are reduced using Uniform Manifold Approximation and Projection (UMAP), and the results grouped by optimal cluster selection methods. We demonstrate this technique on a freely-available collection of deidentified patient notes (MIMIC IV), extracting and clustering “mental or behavioral dysfunctions”. Our results show that it is possible to select user-defined groups of patients from unstructured text with minimal model oversight to group patients with similar profiles. In our study cohort, the models automatically segmented the patients into two groups: patients with more physical symptoms (alcohol/drug abuse, dysarthria, tongue-biting, eating disorders) and patients with mental/emotional symptoms. By detecting the underlying similarities in patient profiles, we believe this method can be utilized for symptom prediction tasks, as well as curating treatment plans based on their cluster profiles. Such a system can assist in clinical decision-making without the need for individually-created NLP models.","keywords: {Training;Manifolds;Dimensionality reduction;Pipelines;MIMICs;Eating disorders;Transformers;Data mining;Reliability;Engineering in medicine and biology;NLP;transformers;EHR extraction;clustering;dimensionality reduction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10781648&isnumber=10781494,"B. Holmes, M. Raymer and T. Banerjee, ""Extraction of patients subpopulations with psychiatric symptoms using a transformer architecture,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10781648."
"Signal Recognition for Parkinson's Disease Diagnosis Based on Multi-Source Deep Domain Generalization,","Parkinson's disease (PD) is an incurable neurodegenerative disorder. Dysarthria, stemming from PD, in speech offers an accessible and non-invasive diagnostic indicator. However, PD speech data, with its limited sample size and high aliasing, presents challenges for machine learning. Thought multi-source deep transfer learning methods show promise in PD speech recognition, they typically rely on labelled target domain data. To overcome this limitation, our study proposes an unsupervised transfer learning approach, namely multi-source deep domain generalization (MDDG). MDDG comprises four key modules: feature extraction, inter-domain and classes adversarial learning, and classifier training solely on multisource datasets. Leveraging adversarial networks, MDDG extracts invariant features from source domains, minimizing both inter-domain distribution discrepancies and intra-class difference while maximizing the distance between different classes from multi-source domains. Experimental results demonstrate the superiority of the MDDG over recent methodologies, where MDDG achieves remarkable metrics: highest accuracy (75.80%), precision (66.48%), and specificity (87.61%), all with the lowest standard error, underscoring the effectiveness and stability of MDDG offering valuable support to medical professionals in efficient PD diagnosis and monitoring.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10830707&isnumber=10830612,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10830707&isnumber=10830612,"L. Li, Y. Rao, H. Li and Y. Liu, ""Signal Recognition for Parkinson's Disease Diagnosis Based on Multi-Source Deep Domain Generalization,"" BIBE 2024; The 7th International Conference on Biological Information and Biomedical Engineering, Hohhot, China, 2024, pp. 21-26."
"Notice of Violation of IEEE Publication Principles: A voice-input voice-output communication assists in favor of people with harsh verbal communication destruction,","An actual VOICE INPUT VOICE OUTPUT Correspondence Help identifies the actual disordered talk from the person as well as develops communications that are changed into artificial talk. Tests demonstrated this technique works within producing great acknowledgement overall performance (mean precision 98 percentage) upon extremely disordered talk, even if acknowledgement perplexity is actually elevated. The actual VOICE INPUT VOICE OUTPUT Communication Assists (VIVOCA) had been examined inside an area test through people with reasonable in order to serious dysarthria as well as verified that they'll utilize the gadget to create intelligible talk result through disordered talk enter. The actual test outlined a few problems that restrict the actual overall performance as well as user friendliness from the gadget whenever used within actual utilization circumstances, along with imply acknowledgement precision associated with 85 percentages within these types of conditions. Once after receiving the clear speech in English as input, the same can be translated to any other language as an output, with the same meaning as it is in input. These types of restrictions are going to be tackled within long term function.",URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7034078&isnumber=7033740,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7034078&isnumber=7033740,"M. Babu and V. D. Kumar, ""Notice of Violation of IEEE Publication Principles: A voice-input voice-output communication assists in favor of people with harsh verbal communication destruction,"" International Conference on Information Communication and Embedded Systems (ICICES2014), Chennai, India, 2014, pp. 1-6, doi: 10.1109/ICICES.2014.7034078."
"A Wearable System for Monitoring Neurological Disorder Events with Multi-Class Classification Model in Daily Life,","Dysphagia and dysarthria are the prominent sequelae of neurological disorders. Treatment and rehabilitation of these impairments necessitate continuously monitoring symptoms related to swallowing and speaking. However, current medical technologies require large and diverse equipment to record these symptoms, which are predominantly limited to clinical environments. In this study, we propose an innovative wearable system for distinguishing neurological disorder events using a mechano-acoustic (MA) sensor and multi-class ensemble classification model. The MA sensor exhibits a high sensitivity to neck vibration without any interference from ambient sounds. A multi-class classification model was also developed to discern the symptoms from the recorded signals accurately. The proposed classification model is an ensemble neural network trained on waveforms and mel spectrograms. As a result, we achieve a high classification accuracy of 91.94%, surpassing the performance of previous single neural networks.","keywords: {Neurological diseases;Vibrations;Accuracy;Time series analysis;Vibration measurement;Neck;Biomedical monitoring;Monitoring;Biological neural networks;Spectrogram;Neurological disorder symptoms;wearable monitoring system;mechano-acoustic sensor;multi-class classification model},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10782047&isnumber=10781494,"Y. Song, I. Yun, S. Giovanoli, C. A. Easthope and Y. Chung, ""A Wearable System for Monitoring Neurological Disorder Events with Multi-Class Classification Model in Daily Life,"" 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10782047."
"Tongue-Trackpad: Tongue Rehabilitation Through Quantitative Personalized Visual-Feedback Intervention,","Effective rehabilitation of tongue movement relies on delivering personalized interventions and quantitatively assessing the intervention's impact. Current approaches primarily focus on enhancing tongue movement control and strength through oromotor exercises and audio feedback. Here, we introduce the Tongue- Trackpad a novel solution for tongue movement rehabilitation and progress tracking. The Tongue- Trackpad is a wireless intra-oral device that provides real-time visual feedback of tongue movement while quantifying the movement, thus enabling personalized interventions. In this preliminary feasibility study, a stroke survivor diag-nosed with dysarthria participated in eighteen sessions of per-sonalized visual-feedback pursuit intervention using the Tongue-Trackpad. The intervention was customized to the participant's tongue movement deficit areas identified during the initial evaluation. Despite the participant's severe speech disorder, characterized by a Diadochokinetic rate of zero, the results indicated a modest positive change in tongue movement both within a single session and over the intervention period. The results showed an average increase of 3.5 $\pm 4.7{\%}$ in coverage area, a reduction of 1.5 $\pm 1.9{\%}$ in the deficit area, and a 1.7 $\pm$ 3.1 % reduction in the excess area within a single session. Similar modest trends were observed throughout the entire intervention period. Although further studies with more participants are needed for robust conclusions, this preliminary feasibility study suggests provided preliminary that providing personalized visual-feedback intervention using the Tongue- Trackpad holds promise for tongue movement rehabilitation.","keywords: {Wireless communication;Visualization;Body sensor networks;Tongue;Tracking;Market research;Real-time systems;Intra-Oral Technology;Tongue Rehabilitation;Personalized Intervention;Tongue Movement;Visual-Feedback},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10780592&isnumber=10780450,"A. Scarpellini, A. R. Carroll, E. M. Babbitt, J. Patton and H. Esmailbeigi, ""Tongue-Trackpad: Tongue Rehabilitation Through Quantitative Personalized Visual-Feedback Intervention,"" 2024 IEEE 20th International Conference on Body Sensor Networks (BSN), Chicago, IL, USA, 2024, pp. 1-4, doi: 10.1109/BSN63547.2024.10780592."
"Classifying Speech Disorders Using Voice Signals and Machine Learning,","This study explores the application of machine learning (ML) and advanced voice signal analysis to classify speech disorders, including vocal tremors, dysarthria, and stuttering. These disorders pose significant challenges to communication and quality of life, requiring precise and reliable diagnostic tools. To address the inherent challenges of limited data availability in this domain, the study employs synthetic voice data generated using mathematical models to augment real-world recordings. This hybrid approach ensures a more comprehensive dataset, enabling robust training and evaluation of machine learning models.Key machine learning algorithms such as Support Vector Machines (SVMs), Random Forest, and Gradient Boosting are utilized to extract and analyze a wide range of acoustic features from speech samples. These methods are selected for their ability to handle complex, nonlinear patterns and to identify subtle distinctions between normal and disordered speech. By leveraging these techniques, the study investigates their performance in accurately classifying speech disorders, emphasizing their potential in improving diagnostic outcomes.The findings highlight the transformative potential of machine learning in the field of speech pathology. ML models demonstrated notable improvements in the precision, efficiency, and consistency of diagnosing speech disorders compared to traditional methods. This breakthrough not only enhances diagnostic reliability but also equips clinicians with objective tools that reduce subjectivity in assessments. Moreover, the integration of these technologies promotes earlier detection of disorders, enabling timely and tailored therapeutic interventions.By facilitating more accurate and accessible diagnostic practices, this work paves the way for significant advancements in patient care. The use of ML-powered tools can bridge the gap between clinical expertise and technological innovation, empowering healthcare professionals to offer personalized treatment plans. Ultimately, this approach has the potential to improve patient outcomes, reduce the stigma associated with speech disorders, and ensure more equitable access to high-quality care for individuals worldwide.","keywords: {Training;Support vector machines;Pathology;Accuracy;Machine learning algorithms;Boosting;Real-time systems;Mathematical models;Random forests;Synthetic data;machine learning;speech disorders;acoustic features;classification;synthetic data;diagnosis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10940483&isnumber=10940074,"D. Al Masri, A. Yousef, L. Turkistani, T. Tadmori, E. Barkat and N. Kabbaj, ""Classifying Speech Disorders Using Voice Signals and Machine Learning,"" 2025 22nd International Learning and Technology Conference (L&T), jeddah, Saudi Arabia, 2025, pp. 349-353, doi: 10.1109/LT64002.2025.10940483."
"Speech Formation Objectivisation Methods Development and Analysis for Correct Phonation Diagnostics,","In connection with various infringements speech production people have requirement for development of methods of an objective estimation of the vocal apparatus functional condition. Methods should be directed on revealing of the attributes forming articulation-acoustic characteristics of correct harmonization of themes. The given problem is especially important for children for whom increase speech pathologies (legasthenia, dysgraphia, dysarthria, phonasthenia, hoarseness, etc.) is observed. As results of the carried out research, it is possible to come to a conclusion that for the analysis of voice pathologies it is expedient to use the offered method.","keywords: {Speech analysis;Frequency;Pathology;Educational institutions;State estimation;Appraisal;Parameter estimation;Cities and towns;Microphones;Passband},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4292395&isnumber=4292363,"O. G. Fetisova, D. V. Lamtyugin, V. K. Makukha, V. T. Maslov and E. M. Voronin, ""Speech Formation Objectivisation Methods Development and Analysis for Correct Phonation Diagnostics,"" 2006 8th International Conference on Actual Problems of Electronic Instrument Engineering, Novosibirsk, Russia, 2006, pp. 123-123, doi: 10.1109/APEIE.2006.4292395."
"Synthesizing Dysarthric Speech Using Multi-Speaker Tts For Dysarthric Speech Recognition,","Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems may help dysarthric talkers communicate more effectively. To have robust dysarthria-specific ASR, sufficient training speech is required, which is not readily available. Recent advances in Text-To-Speech (TTS) synthesis multi-speaker end-to-end systems suggest the possibility of using synthesis for data augmentation. In this paper, we aim to improve multi-speaker end-to-end TTS systems to synthesize dysarthric speech for improved training of a dysarthria-specific DNN-HMM ASR. In the synthesized speech, we add dysarthria severity level and pause insertion mechanisms to other control parameters such as pitch, energy, and duration. Results show that a DNN-HMM model trained on additional synthetic dysarthric speech achieves WER improvement of 12.2% compared to the baseline, the addition of the severity level and pause insertion controls decrease WER by 6.5%, showing the effectiveness of adding these parameters. Audio samples are available at https://mohammadelc.github.io/SpeechGroupUKY/","keywords: {Training;Databases;Conferences;Web pages;Training data;Speech recognition;Production;Dysarthria;speech recognition;Speech-To-Text;Synthesized speech;Data augmentation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746585&isnumber=9746004,"M. Soleymanpour, M. T. Johnson, R. Soleymanpour and J. Berry, ""Synthesizing Dysarthric Speech Using Multi-Speaker Tts For Dysarthric Speech Recognition,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 7382-7386, doi: 10.1109/ICASSP43922.2022.9746585."
"The Change of Vocal Tract Length in People with Parkinson’s Disease,","Hypokinetic dysarthria is one of the early symptoms of Parkinson’s disease (PD) and has been proposed for early detection and also for monitoring of the progression of the disease. PD reduces the control of vocal tract muscles such as the tongue and lips and, therefore the length of the active vocal tract is altered. However, the change in the vocal tract length due to the disease has not been investigated. The aim of this study was to determine the difference in the apparent vocal tract length (AVTL) between people with PD and age-matched control healthy people. The phoneme, /a/ from the UCI Parkinson’s Disease Classification Dataset and the Italian Parkinson’s Voice and Speech Dataset were used and AVTL was calculated based on the first four formants of the sustained phoneme (F1-F4). The results show a correlation between Parkinson’s disease and an increase in vocal tract length. The most sensitive feature was the AVTL calculated using the first formants of sustained phonemes (F1). The other significant finding reported in this article is that the difference is significant and only appeared in the male participants. However, the size of the database is not sufficiently large to identify the possible confounding factors such as the severity and duration of the disease, medication, age, and comorbidity factors.Clinical relevance—The outcomes of this research have the potential to improve the identification of early Parkinsonian dysarthria and monitor PD progression.","keywords: {Tongue;Correlation;Databases;Lips;Muscles;Larynx;Biomedical monitoring},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10340263&isnumber=10339939,"N. D. Pah, M. A. Motin, G. C. Oliveira and D. K. Kumar, ""The Change of Vocal Tract Length in People with Parkinson’s Disease,"" 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Sydney, Australia, 2023, pp. 1-4, doi: 10.1109/EMBC40787.2023.10340263."
"Weak-Supervised Dysarthria-Invariant Features for Spoken Language Understanding Using an Fhvae and Adversarial Training,","The scarcity of training data and the large speaker variation in dysarthric speech lead to poor accuracy and poor speaker generalization of spoken language understanding systems for dysarthric speech. Through work on the speech features, we focus on improving the model generalization ability with limited dysarthric data. Factorized Hierarchical Variational Auto-Encoders (FHVAE) trained unsupervisedly have shown their advantage in disentangling content and speaker representations. Earlier work showed that the dysarthria shows in both feature vectors. Here, we add adversarial training to bridge the gap between the control and dysarthric speech data domains. We extract dysarthric and speaker invariant features using weak supervision. The extracted features are evaluated on a Spoken Language Understanding task and yield a higher accuracy on unseen speakers with more severe dysarthria compared to features from the basic FHVAE model or plain filterbanks.","keywords: {Training;Conferences;Training data;Filter banks;Speech recognition;Feature extraction;Generators;Dysarthric speech;FHVAE;adversarial training;weak supervision;end-to-end spoken language understanding},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10023085&isnumber=10022330,"J. Qi and H. Van hamme, ""Weak-Supervised Dysarthria-Invariant Features for Spoken Language Understanding Using an Fhvae and Adversarial Training,"" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 375-381, doi: 10.1109/SLT54892.2023.10023085."
"Dysarthric Speech Detection Using Hybrid Models,","Dysarthria is a speech disorder caused by weak or poorly coordinated speech-related muscles. Dysarthria can be caused by various factors such as stroke, multiple sclerosis, or cerebral palsy, as well as brain injury or certain medications. The ultimate goal of our paper is to make a suitable and accurate tool for detecting dysarthric speech that can be used in clinical settings for early diagnosis, treatment planning for dysarthric individuals. In this work, the machine learning models namely CNN (Convolutional Neural Network), and hybrid models, such as CNN, combined with LSTM (Long Short-Term Memory), and CNN combined with GRU (Gated Recurrent Unit) are trained on TORGO dataset, to classify the dysarthric speech from non-dysarthric speech. The model's performance is evaluated on the testing data and all of these models produce more than 95% accuracy rate. From this study, we understood that this methodology can be used in developing Automatic Speech Recognition(ASR) for people with dysarthria, which would become an essential technology for many applications.","keywords: {Voice activity detection;Multiple sclerosis;Computational modeling;Speech recognition;Muscles;Data models;Planning;Dysarthria;Speech;CNN;LSTM;GRU;ASR},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10308386&isnumber=10306339,"S. H. Fazil and S. D, ""Dysarthric Speech Detection Using Hybrid Models,"" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-5, doi: 10.1109/ICCCNT56998.2023.10308386."
"Towards Improving the Performance of Dysarthric Speech Severity Assessment System,","Dysarthria is a kind of speech disorder caused by difficulties in muscular control of the speech mechanism, owing to the impairment of the central or peripheral nervous systems. Dysarthria is a speaking impairment due to the weak muscles of the patient due to brain injury. Face, throat muscles, lips are important to generate intelligible speech in humans. The severity of the disorder decides the understandability of the dysarthric speech, as the speech can be slow and unclear. This affects dysarthric people's life. This creates a communication gap between the common people and dysarthric patients. This work proposes a prototype that will improve the intelligibility of dysarthric speech. The main objective of the prototype is to enhance the rehabilitation of dysarthric people's livelihood. From the severity assessment, the clinician and caretakers can provide an improved speech treatment for dysarthric patients. Audio features like MFCC (Mel Frequency Spectral Coefficients), ZCR (Zero Crossing Rate), Spectral centroid, spectral rolloff, Mel spectrogram are used for feature extraction process. The KNN (K-nearest neighbor) and SVM (Support Vector Machine) classifiers are used to categorize the severity level of dysarthria based upon the features obtained.","keywords: {Support vector machines;Peripheral nervous system;Lips;Prototypes;Muscles;Feature extraction;Mel frequency cepstral coefficient;Dysarthria;MFCC;ZCR;Spectral centroid;feature extraction;SVM;KNN;speech therapy;speech severity assessment},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9740812&isnumber=9740653,"A. B. B, S. A. Kumar, K. T, S. Sasikala and K. P. C. V, ""Towards Improving the Performance of Dysarthric Speech Severity Assessment System,"" 2022 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 2022, pp. 1-6, doi: 10.1109/ICCCI54379.2022.9740812."
"Effects of acoustic features modifications on the perception of dysarthric speech — Preliminary study (Pitch, intensity and duration modifications),","Marking stress is important in conveying meaning and drawing listener's attention to specific parts of a message. Extensive research has shown that healthy speakers mark stress using three main acoustic cues; pitch, intensity, and duration. The relationship between acoustic and perception cues is vital in the development of a computer-based tool that aids the therapists in providing effective treatment to people with Dysarthria. It is, therefore, important to investigate the acoustic cues deficiency in dysarthric speech and the potential compensatory techniques needed for effective treatment. In this paper, the relationship between acoustic and perceptive cues in dysarthric speech are investigated. This is achieved by modifying stress marked sentences from 10 speakers with Ataxic dysarthria. Each speaker produced 30 sentences using the 10 SubjectVerb-Object-Adjective (SVOA) structured sentences across three stress conditions. These stress conditions are stress on the initial (S), medial (O) and final (A) target words respectively. To effectively measure the deficiencies in Dysarthria speech, the acoustic features (pitch, intensity, and duration) are modified incrementally. The paper presents the techniques involved in the modification of these acoustic features. The effects of these modifications are analysed based on steps of 25% increments in pitch, intensity and duration. For robustness and validation, 50 untrained listeners participated in the listening experiment. The results and the relationship between acoustic modifications (what is measured) and perception (what is heard) in Dysarthric speech are discussed.","keywords: {Stress marking;perception;dysarthria;acoustics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8361566&isnumber=8329307,"T. B. Ijitona, J. J. Soraghan, A. Lowit, G. Di-Caterina and H. Yue, ""Effects of acoustic features modifications on the perception of dysarthric speech — Preliminary study (Pitch, intensity and duration modifications),"" IET 3rd International Conference on Intelligent Signal Processing (ISP 2017), London, 2017, pp. 1-6, doi: 10.1049/cp.2017.0363."
"FBSE-FTFCWT-Based Novel Automated Framework for Dysarthric Speech Detection,","Neurological injuries or neurodegenerative diseases can lead to dysarthria, a condition that impairs speech intelligibility. Accurate detection of dysarthria and its severity from speech signals are crucial for advancing smart healthcare solutions. This study presents an automated system for dysarthria detection and severity classification, using a Fourier-Bessel series expansion-based flexible time-frequency coverage wavelet transform (FBSE-FTFCWT) and an autoencoder. Initially, FBSE-FTFCWT decomposes the speech signal into 16 sub-band signals, which are used as an input in the form of tensor for autoencoders to generate latent representation. This latent representation is subsequently used for dysarthric speech and its severity level detection. The proposed framework outperformed the current state-of-the-art in classifying dysarthric and normal speech on the UA-speech dataset, achieving 3.28% higher accuracy. Additionally, for the dysarthric severity detection task using speech signals from the same dataset, it showed 3.1% improvement in accuracy over existing methods.","keywords: {Wavelet transforms;Voice activity detection;Time-frequency analysis;Accuracy;Tensors;Autoencoders;Medical services;Signal processing;Feature extraction;Injuries;Autoencoder;Dysarthria;FBSE-FTFCWT;Severity-level detection;UA-speech},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10889688&isnumber=10887541,"A. Vijay, R. B. Pachori, B. Appina and N. Tiwari, ""FBSE-FTFCWT-Based Novel Automated Framework for Dysarthric Speech Detection,"" ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025, pp. 1-5, doi: 10.1109/ICASSP49660.2025.10889688."
"Study of the Automatic Detection of Parkison’s Disease Based on Speaker Recognition Technologies and Allophonic Distillation,","The use of new tools to detect Parkinson's Disease (PD) from speech articulatory movements can have a considerable impact in the diagnosis of patients. In this study, a novel approach involving speaker recognition techniques with allophonic distillation is proposed and tested separately in four parkinsonian speech databases (205 patients and 186 controls in total). This new scheme provides values between 72% and 94% of accuracy in the automatic detection of PD, depending on the database, and improvements up to 9% respect to baseline techniques. Results not only point towards the importance of the segmentation of the speech for the differentiation of parkinsonian and control speakers but confirm previous findings about the relevance of plosives and fricatives in the detection of parkinsonian dysarthria.","keywords: {Databases;Liquids;Diseases;Task analysis;Acoustics;Speech recognition;Speaker recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8512562&isnumber=8512178,"L. Moro-Velazquez et al., ""Study of the Automatic Detection of Parkison’s Disease Based on Speaker Recognition Technologies and Allophonic Distillation,"" 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, USA, 2018, pp. 1404-1407, doi: 10.1109/EMBC.2018.8512562."
"Monitoring the Effect of Levodopa Using Sustained Phonemes in Parkinson’s Disease Patients,","Parkinson's disease (PD) is a neurological disease identified by multiple symptoms, and levodopa is one of the most effective medications for treating the disease. To determine the dosage of levodopa, it is necessary to meet on a regular basis and observe motor function. The early detection and progression of the disease have been proposed using hypokinetic dysarthria. However, previous studies have not examined the effects of levodopa on speech rigorously and have provided inconsistent results. In this study, three sustained phonemes of PD patients were investigated for the effect of medication. A set of features characterizing vocal fold dynamics as well as the vocal tract coordinators were extracted from the sustained phonemes /of 28 PD patients during levodopa medication off and on states. All the features were statistically investigated and classified using a linear discriminant analysis (LDA) classifier. LDA classifier identified medication on from medication off based on the combined features from phoneme /a/, /o/ and /m/ with the accuracy=82.75% and F1-score=82.18%. Voice recording of PD patients during sustained phonemes /a/, /o/ and /m/ has the potential for identifying whether the patients are in On state or Off state of medication.Clinical Relevance— The outcomes of this study have the potential to monitor the effect and progress of levodopa on PD patients.","keywords: {Neurological diseases;Parkinson's disease;Cepstral analysis;Feature extraction;Biology;Recording;Linear discriminant analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10340507&isnumber=10339939,"M. A. Motin, N. D. Pah and D. K. Kumar, ""Monitoring the Effect of Levodopa Using Sustained Phonemes in Parkinson’s Disease Patients,"" 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Sydney, Australia, 2023, pp. 1-4, doi: 10.1109/EMBC40787.2023.10340507."
"Adapting acoustic and lexical models to dysarthric speech,","Dysarthria is a motor speech disorder resulting from neurological damage to the part of the brain that controls the physical production of speech. It is, in part, characterized by pronunciation errors that include deletions, substitutions, insertions, and distortions of phonemes. These errors follow consistent intra-speaker patterns that we exploit through acoustic and lexical model adaptation to improve automatic speech recognition (ASR) on dysarthric speech. We show that acoustic model adaptation yields an average relative word error rate (WER) reduction of 36.99% and that pronunciation lexicon adaptation (PLA) further reduces the relative WER by an average of 8.29% on a large vocabulary task of over 1500 words for six speakers with severe to moderate dysarthria. PLA also shows an average relative WER reduction of 7.11% on speaker-dependent models evaluated using 5-fold cross-validation.","keywords: {Speech;Speech recognition;Adaptation models;Data models;Acoustics;Hidden Markov models;Databases;dysarthria;dysarthric speech;pronunciation lexicon adaptation;speech recognition},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947460&isnumber=5946226,"K. T. Mengistu and F. Rudzicz, ""Adapting acoustic and lexical models to dysarthric speech,"" 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Prague, Czech Republic, 2011, pp. 4924-4927, doi: 10.1109/ICASSP.2011.5947460."
"Improving the intelligibility of dysarthric speech towards enhancing the effectiveness of speech therapy,","Dysarthria is a neuro-motor disorder in which the muscles used for speech production and articulation are severely affected. Dysarthric patients are characterized by slow or slurred speech that is difficult to understand. This work aims at enhancing the intelligibility of dysarthric speech towards developing an effective speech therapy tool. In this therapy tool, enhanced speech is used for providing auditory feedback with a delay to instill confidence in the patients, so that they can improve their speech intelligibility gradually through relearning. Feature level transformation techniques based on linear predictive coding (LPC) coefficient mapping and frequency warping of LPC poles are experimented in this work. Speech utterances from Nemours dataset with mild and moderate dysarthria are used to study the effectiveness of the proposed algorithms. The quality of the transformed speech is evaluated using subjective and objective measures. A significant improvement in the intelligibility of speech was observed. Our method henceforth could be used to enhance the effectiveness of speech therapy, by encouraging the dysarthric patients talk more, thus helping in their fast rehabilitation.","keywords: {Speech;Speech enhancement;Databases;Production;Medical treatment;Linear predictive coding;Error analysis;Dysarthria;intelligibility;speech enhancement;speech therapy;delayed auditory feedback;Linear prediction coefficients;dynamic time warping;frequency warping},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7732175&isnumber=7732013,"S. A. Kumar and C. S. Kumar, ""Improving the intelligibility of dysarthric speech towards enhancing the effectiveness of speech therapy,"" 2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI), Jaipur, India, 2016, pp. 1000-1005, doi: 10.1109/ICACCI.2016.7732175."
"A Novel Approach for Intelligibility Assessment in Dysarthric Subjects,","Dysarthria is a motor speech impairment caused by muscle weakness. Individuals, with this condition, are unable to control rapid movement of the velum leading to reduction in intelligibility, audibility, naturalness and efficiency of vocal communication. Systems that can assess intelligibility of dysarthric speech can help clinicians diagnose the impact of therapy and medication. In the paper, we propose a usable novel method to assess intelligibility of dysarthric speakers. The approach is based on the observation that the performance of a speech recognition engine deteriorates with increase in severity of the disorder. The mismatch between the original word and the recognized string is exploited to compute the dysarthria intelligibility score. Experiments on UA speech corpus show that the computed intelligibility score exhibits a significant correlation with perceptually assessed intelligibility scores. We further show that a small set of words spoken by the dysarthric subject is sufficient to assess the speech intelligibility reliably.","keywords: {Medical treatment;Speech recognition;Signal processing;Muscles;Reliability;Speech processing;Medical diagnostic imaging;Dysarthria;intelligibility assessment;diagnosis;deepspeech;ASR},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9053339&isnumber=9052899,"A. Tripathi, S. Bhosale and S. K. Kopparapu, ""A Novel Approach for Intelligibility Assessment in Dysarthric Subjects,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 6779-6783, doi: 10.1109/ICASSP40776.2020.9053339."
"Intelligibility modification of dysarthric speech using HMM-based adaptive synthesis system,","Dysarthria is a manifestation of an inability to control and coordinate on one or more articulatory subsystems, which results in poorly articulated, slurred, and unintelligible speech. In order to enable a dysarthric speaker to communicate more efficiently with others, a text-to-speech synthesis system that generates speech in his voice, but without the errors he makes would be desirable. In this regard, the current work proposes a system, where the dysarthric speech is first recognized by an HMM-based speech recognition system. A sentence-level network is used to ensure 100% recognition accuracy. The recognized text is then synthesized by a speech synthesis system adapted to the dysarthric speaker's voice. This system replaces the sound units wrongly uttered by the dysarthric speaker, thereby improving intelligibility. The rate of synthesized speech is quite low for speakers with moderate and severe dysarthria. Therefore, the speech rate is modified using time-domain pitch synchronous overlap add (TD-PSOLA) technique. Degradation mean opinion score (DMOS) is used to prove that wrongly uttered sound units are replaced by correct sound units and that the synthetic speech is made more intelligible with the speaker's identity.","keywords: {Speech;Hidden Markov models;Speech recognition;Databases;Adaptation models;Speech synthesis;Acoustics;Dysarthria;perceptual analysis;hidden Markov model (HMM);speech recognition and synthesis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7235130&isnumber=7235123,"M. Dhanalakshmi and P. Vijayalakshmi, ""Intelligibility modification of dysarthric speech using HMM-based adaptive synthesis system,"" 2015 2nd International Conference on Biomedical Engineering (ICoBE), Penang, Malaysia, 2015, pp. 1-5, doi: 10.1109/ICoBE.2015.7235130."
"Enhancement of dysarthric speech for developing an effective speech therapy tool,","Dysarthria is a neuromuscular disorder that results from weakened movement of muscles used in speech production. This results in poor articulation causing the dysarthric speech to be slurred and difficult to understand. The natural auditory feedback makes the patients understand that their speech is of low quality, and this lowers their self confidence and they become more and more introverted causing the disorder to aggravate. In this work, we enhance the dysarthric speech and provide the enhanced speech to the patient through auditory feedback. This helps the patients to feel comfortable with their speech and gradually develop confidence to speak more and hence achieve a speedy rehabilitation. The utterances are analyzed using linear predictive coding (LPC). The LPC features in the acoustic space of dysarthric speaker are mapped to the feature space of the normative population using constrained maximum likelihood linear regression (CMLLR) before they are used for re-synthesising the enhanced speech. We then evaluated the quality of the enhanced speech using subjective and objective measures, DMOS and PESQ, and obtained an improvement of 63% and 43.4% for DMOS and PESQ measures respectively. Clinical trials are being pursued at Amrita Institute of Medical Sciences, Kochi on patients with dysarthria to evaluate the effectiveness of the proposed approach for faster rehabilitation of the patients. The results of these clinical trials will be reported in due course of time.","keywords: {Speech;Speech enhancement;Tools;Medical treatment;Maximum likelihood linear regression;Conferences;Production;Dysarthria;feature space mapping;constrained maximum likelihood linear regression;instantaneous auditory feedback;gaussian mixture mapping},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8300222&isnumber=8299705,"S. Sivaram, C. S. Kumar and A. A. Kumar, ""Enhancement of dysarthric speech for developing an effective speech therapy tool,"" 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET), Chennai, India, 2017, pp. 2548-2551, doi: 10.1109/WiSPNET.2017.8300222."
"Cochlear Filter-Based Cepstral Features for Dysarthric Severity-Level Classification,","Severity-level classification of dysarthria helps in diagnosing a patient and choosing an appropriate course of treatment. This would also aid in redirecting the speech to an appropriate dysarthric Automatic Speech Recognition (ASR), as traditional ASR does not perform well on dysarthric speech. In the recent past, several approaches have been used to study the severity-level classification of dysarthria using state-of-the-art features, such as Short-Time Fourier Transform (STFT) and Mel Frequency Cepstral Coefficients (MFCC). This study investigates novel auditory transform-based Cochlear Filter Cepstral Coefficients (CFCC) features for dysarthric severity-level classification. Three DNN-based classifiers, namely, Convolutional Neural Network (CNN), Light-CNN (LCNN), and Residual Neural Network (ResNet) were employed on UA-Speech Corpus and TORGO corpus. Our proposed CFCC feature set yields an improved classification accuracy of 97.46% (98.99%), 94.92% (94.97%), and 96.66% (98.93%) on UA (Torgo)-corpus using CNN, LCNN and ResNet classifiers respectively. Furthermore, performance metrics, such as the Jaccard index, Matthew's Correlation Coefficient (MCC), $F$1-score, and Hamming loss are used to examine feature discrimination power of CFCC. Finally, latency period of CFCC was also analysed for practical deployment of system.","keywords: {Measurement;Correlation coefficient;Fourier transforms;Europe;Signal processing;Convolutional neural networks;Indexes;Dysarthria;UA-Speech Corpus;TORGO Corpus;CFCC;LFCC;MFCC;CNN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10289888&isnumber=10289713,"S. Rathod, P. Gupta, A. Kachhi and H. A. Patil, ""Cochlear Filter-Based Cepstral Features for Dysarthric Severity-Level Classification,"" 2023 31st European Signal Processing Conference (EUSIPCO), Helsinki, Finland, 2023, pp. 1095-1099, doi: 10.23919/EUSIPCO58844.2023.10289888."
"Dysarthric Speech Recognition using Depthwise Separable Convolutions: Preliminary Study,","As a neurological disability that affects muscles involved in articulation, dysarthria is a speech impairment that leads to reduced speech intelligibility. In severe cases, these individuals could also be handicapped and unable to interact with digital devices. For such individuals, Automatic Speech Recognition (ASR) technologies could be life changing by enabling them to communicate with others as well as computing devices via voice commands. Nonetheless, ASR systems designed to recognize healthy speech have shown very poor performance to transcribe dysarthric speech, signaling the need to design ASR specifically tailored for dysarthria. Dysarthric Speech Recognition (DRS) research has progressed gradually because of the challenges the research community faces such as the scarcity of dysarthric speech that does not allow the researchers to design deeper acoustic models needed to better learn dysarthric speech variations. In this paper we report on our preliminary findings to improve our previous DSR called Speech Vision and study the effects of Separable Convolutional neurons to improve its acoustic model. Speech Vision is a novel Dysarthric Speech Recognition system that learns to recognize the shape of the words uttered by dysarthric speakers instead of recognizing phone sequences and then mapping them to words. Experiments conducted on the utterances provided by all UA-Speech dysarthric speakers indicate the proposed Depthwise separable architecture provided better word recognition accuracies compared to the original Speech Vision’s architecture across all dysarthric speech intelligibility classes.","keywords: {Performance evaluation;Convolution;Shape;Training data;Personal digital devices;Speech recognition;Computer architecture;dysarthria;dysarthric speech recognition;depthwise separable convolution;speech vision},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10314894&isnumber=10314866,"S. R. Shahamiri, K. Mandal and S. Sarkar, ""Dysarthric Speech Recognition using Depthwise Separable Convolutions: Preliminary Study,"" 2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD), Bucharest, Romania, 2023, pp. 78-82, doi: 10.1109/SpeD59241.2023.10314894."
"Feature selection in Parkinson's disease: A rough sets approach,","Parkinson's disease is a neurodegenerative disorder with a long time course and a significant prevalence, which increases significantly with age. Although the etiology is currently unknown, the disease presents with neurodegeneration of regions of the basal ganglia. the onset occurs later in life, and the disease progresses slowly. The disease is diagnosed clinically, requiring the identification of several factors such as distal resting tremor, rigidity, and bradykinesia. The common thread throughout the range of symptoms is motor dysfunction, and recent reports have focused on dysphonia, the impairment in voice production as a diagnostic measure. In this paper, a number of features associated with speech have been collected through clinical studies from both healthy and people with Parkinson's (PWP) and analysed in order to determine if one or more of them can be used to diagnose PWP. The feature set is analysed using the rough sets paradigm, which maps feature vectors associated with objects onto decision classes. The results from applying rough sets is a set of rules that map features via rules into a decision support system - performing classification of objects. the results FOM this study indicate that a subset of typical voice derived features is adequate to differentiate healthy from PWP with 100% accuracy. These result are important in that they imply that a diagnosis can be automated and performed remotely. This work will be extended to determine if this approach can be utilised with the same effectiveness for the diagnosis of parkinsonism disorders - a collection-diseases with Parkinson's like symptoms.","keywords: {Parkinson's disease;Rough sets;Mathematics;Computer science},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5352688&isnumber=5352681,"K. Revett, F. Gorunescu and A. -B. M. Salem, ""Feature selection in Parkinson's disease: A rough sets approach,"" 2009 International Multiconference on Computer Science and Information Technology, Mragowo, Poland, 2009, pp. 425-428, doi: 10.1109/IMCSIT.2009.5352688."
"Universal Paralinguistic Speech Representations Using self-Supervised Conformers,","Many speech applications require understanding aspects beyond the words being spoken, such as recognizing emotion, detecting whether the speaker is wearing a mask, or distinguishing real from synthetic speech. In this work, we introduce a new state-of-the-art paralinguistic representation derived from large-scale, fully self-supervised training of a 600M+ parameter Conformer-based architecture. We benchmark on a diverse set of speech tasks and demonstrate that simple linear classifiers trained on top of our time-averaged representation outperform nearly all previous results, in some cases by large margins. Our analyses of context-window size demonstrate that, surprisingly, 2 second context-windows achieve 96% the performance of the Conformers that use the full long-term context on 7 out of 9 tasks. Furthermore, while the best per-task representations are extracted internally in the network, stable performance across several layers allows a single universal representation to reach near optimal performance on all tasks.","keywords: {Training;Emotion recognition;Conferences;Speech recognition;Signal processing;Benchmark testing;Acoustics;speech;representation learning;self-supervised learning;paralinguistics;transformer},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747197&isnumber=9746004,"J. Shor, A. Jansen, W. Han, D. Park and Y. Zhang, ""Universal Paralinguistic Speech Representations Using self-Supervised Conformers,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 3169-3173, doi: 10.1109/ICASSP43922.2022.9747197."
"Feature extraction using pre-trained convolutive bottleneck nets for dysarthric speech recognition,","In this paper, we investigate the recognition of speech uttered by a person with an articulation disorder resulting from athetoid cerebral palsy based on a robust feature extraction method using pre-trained convolutive bottleneck networks (CBN). Generally speaking, the amount of speech data obtained from a person with an articulation disorder is limited because their burden is large due to strain on the speech muscles. Therefore, a trained CBN tends toward overfitting for a small corpus of training data. In our previous work, the experimental results showed speech recognition using features extracted from CBNs outperformed conventional features. However, the recognition accuracy strongly depends on the initial values of the convolution kernels. To prevent overfitting in the networks, we introduce in this paper a pre-training technique using a convolutional restricted Boltzmann machine (CRBM). Through word-recognition experiments, we confirmed its superiority in comparison to convolutional networks without pre-training.","keywords: {Feature extraction;Convolution;Speech;Speech recognition;Europe;Kernel;Articulation disorders;feature extraction;convolutional neural networks;bottleneck feature;convolutional restricted Boltzmann machine},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7362616&isnumber=7362087,"Y. Takashima, T. Nakashika, T. Takiguchi and Y. Ariki, ""Feature extraction using pre-trained convolutive bottleneck nets for dysarthric speech recognition,"" 2015 23rd European Signal Processing Conference (EUSIPCO), Nice, France, 2015, pp. 1411-1415, doi: 10.1109/EUSIPCO.2015.7362616."
"Modeling pathological speech perception from data with similarity labels,","The current state of the art in judging pathological speech intelligibility is subjective assessment performed by trained speech pathologists (SLP). These tests, however, are inconsistent, costly and, oftentimes suffer from poor intra- and inter-judge reliability. As such, consistent, reliable, and perceptually-relevant objective evaluations of pathological speech are critical. Here, we propose a data-driven approach to this problem. We propose new cost functions for examining data from a series of experiments, whereby we ask certified SLPs to rate pathological speech along the perceptual dimensions that contribute to decreased intelligibility. We consider qualitative feedback from SLPs in the form of comparisons similar to statements “Is Speaker A's rhythm more similar to Speaker B or Speaker C?” Data of this form is common in behavioral research, but is different from the traditional data structures expected in supervised (data matrix + class labels) or unsupervised (data matrix) machine learning. The proposed method identifies relevant acoustic features that correlate with the ordinal data collected during the experiment. Using these features, we show that we are able to develop objective measures of the speech signal degradation that correlate well with SLP responses.","keywords: {Speech;Pathology;Vectors;Cost function;Prediction algorithms;Feature extraction;Acoustics},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6853730&isnumber=6853544,"V. Berisha, J. Liss, S. Sandoval, R. Utianski and A. Spanias, ""Modeling pathological speech perception from data with similarity labels,"" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014, pp. 915-919, doi: 10.1109/ICASSP.2014.6853730."
"Towards a clinical tool for automatic intelligibility assessment,","An important, yet under-explored, problem in speech processing is the automatic assessment of intelligibility for pathological speech. In practice, intelligibility assessment is often done through subjective tests administered by speech pathologists; however research has shown that these tests are inconsistent, costly, and exhibit poor reliability. Although some automatic methods for intelligibility assessment for telecommunications exist, research specific to pathological speech has been limited. Here, we propose an algorithm that captures important multi-scale perceptual cues shown to correlate well with intelligibility. Nonlinear classifiers are trained at each time scale and a final intelligibility decision is made using ensemble learning methods from machine learning. Preliminary results indicate a marked improvement in intelligibility assessment over published baseline results.","keywords: {Speech;Feature extraction;Pathology;Support vector machine classification;Distortion measurement;Speech processing;intelligibility assessment;speech pathology;machine learning;multi-scale analysis},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6638172&isnumber=6637585,"V. Berisha, R. Utianski and J. Liss, ""Towards a clinical tool for automatic intelligibility assessment,"" 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, 2013, pp. 2825-2828, doi: 10.1109/ICASSP.2013.6638172."
"Semantic Analysis of NIH Stroke Scale using Machine Learning Techniques,","In particular, stroke is a major disease leading to death in adults and elderly people, as well as disability. Rapid detection of stroke is very difficult because the cause and cause of the onset are different for each individual. In this paper, we design and implement a system for semantic analysis of early detection of stroke and recurrence of stroke in Koreans over 65 years old, based on the National Institutes of Health (NIH) Stroke Scale. Using C4.5 of the decision tree series represented by the analytics algorithm of machine learning technique, we conduct a semantic interpretation that analyzes and extracts the semantic rules of the execution mechanism that are additionally provided by C4.5. The C4.5 algorithm is used to construct a classification and prediction model using the information gain of the NIH stroke scale features, and to obtain additional NIH Stroke Scale feature reduction effects.","keywords: {Stroke (medical condition);Machine learning;Predictive models;Diseases;Medical diagnostic imaging;Semantics;Senior citizens;National Institutes of Health (NIH) Stroke Scale;Machine Learning;Medical Big Data Analysis;Stroke Disease Prediction},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8668961&isnumber=8668955,"J. Yu et al., ""Semantic Analysis of NIH Stroke Scale using Machine Learning Techniques,"" 2019 International Conference on Platform Technology and Service (PlatCon), Jeju, Korea (South), 2019, pp. 1-5, doi: 10.1109/PlatCon.2019.8668961."
"Online speaking rate estimation using recurrent neural networks,","A reliable online speaking rate estimation tool is useful in many domains, including speech recognition, speech therapy intervention, speaker identification, etc. This paper proposes an online speaking rate estimation model based on recurrent neural networks (RNNs). Speaking rate is a long-term feature of speech, which depends on how many syllables were spoken over an extended time window (seconds). We posit that since RNNs can capture long-term dependencies through the memory of previous hidden states, they are a good match for the speaking rate estimation task. Here we train a long short-term memory (LSTM) RNN on a set of speech features that are known to correlate with speech rhythm. An evaluation on spontaneous speech shows that the method yields a higher correlation between the estimated rate and the ground-truth rate when compared to the state-of-the-art alternatives. The evaluation on longitudinal pathological speech shows that the proposed method can capture long-term and short-term changes in speaking rate.","keywords: {Speech;Estimation;Training;Feature extraction;Recurrent neural networks;Speech recognition;Correlation;recurrent neural networks;speaking rate estimation;clinical tool},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7472678&isnumber=7471614,"Y. Jiao, M. Tu, V. Berisha and J. Liss, ""Online speaking rate estimation using recurrent neural networks,"" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5245-5249, doi: 10.1109/ICASSP.2016.7472678."
"Breath to speech communication with fall detection for elder/patient with take care analytics,","People suffering from Developmental-Disabilities are almost entirely paralyzed disabling them to communicate in any way except using an Augmentative and Alternative Communication device. Survey analysis tells that 1.4% of globe's population suffers from speech disorder which is more than the Karnataka's population. Looking into the elderly group it was analyzed that the fall events cannot be predicted and might be an unsafe event. Estimates tell that 33.33% of 65 and above aged people fall every year. It can be seen that out of these falls 55% occur at home and 23% occur near the home. Hence, a dependable fall detection system has to be developed, and commercially be used all over the globe among the elderly. Depending on fast detection and delivering signals, the cost of the system can be reduced which is interconnected to the reaction and saving time. An enhanced breathe to speech communication and fall detection system for elderly people and also monitoring through a take care analytics is suggested that are based on intelligent sensors that are put by the person using that device.","keywords: {Speech;Senior citizens;Market research;Acceleration;Conferences;Communications technology;breath to speech communication;fail detection;patient monitoring;styling;insert},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7807877&isnumber=7807761,"S. S. Kumar, B. K. Aishwarya, K. N. Bhanutheja and M. Chaitra, ""Breath to speech communication with fall detection for elder/patient with take care analytics,"" 2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT), Bangalore, India, 2016, pp. 527-531, doi: 10.1109/RTEICT.2016.7807877."
"Raw Source and Filter Modelling for Dysarthric Speech Recognition,","Acoustic modelling for automatic dysarthric speech recognition (ADSR) is a challenging task. Data deficiency is a major problem and substantial differences between the typical and dysarthric speech complicates transfer learning. In this paper, we build acoustic models using the raw magnitude spectra of the source and filter components. The proposed multi-stream model consists of convolutional and recurrent layers. It allows for fusing the vocal tract and excitation components at different levels of abstraction and after per-stream pre-processing. We show that such a multi-stream processing leverages these two information streams and helps s model towards normalising the speaker attributes and speaking style. This potentially leads to better handling of the dysarthric speech with a large inter-speaker and intra-speaker variability. We compare the proposed system with various features, study the training dynamics, explore usefulness of the data augmentation and provide interpretation for the learned convolutional filters. On the widely used TORGO dysarthric speech corpus, the proposed approach results in up to 1.7% absolute WER reduction for dysarthric speech compared with the MFCC base-line. Our best model reaches up to 40.6% and 11.8% WER for dysarthric and typical speech, respectively.","keywords: {Training;Representation learning;Convolution;Perturbation methods;Transfer learning;Speech recognition;Information filters;Dysarthric speech recognition;source-filter separation and fusion;multi-stream acoustic modelling},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746553&isnumber=9746004,"Z. Yue, E. Loweimi and Z. Cvetkovic, ""Raw Source and Filter Modelling for Dysarthric Speech Recognition,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 7377-7381, doi: 10.1109/ICASSP43922.2022.9746553."
"Glottal signal parameters as features set for neurological voice disorders diagnosis using K-Nearest Neighbors (KNN),","Disorders affecting nervous system can affect the voice in different ways. Different neurological disorders may lead to speech problems; this may modify the articulatory characteristics related to vocal folds function, which provide important information for detecting certain neurological diseases. In order to improve the diagnosis of Neurological Voice Disorders (NVD) an objective technique based on articulatory evaluations of vocal folds vibration, based on an estimation of a Glottic Signal (GS) extracted from Speech signal. In this work, we propose a method based on parameters extracted from GS obtained by an inverse filtering algorithm for automatic classification and diagnosis of NVD using K-Nearest Neighbors (KNN). Our work is developed around Saarbrucken Voice Database it is an open German database containing deferent samples, words, sentences of normal and pathological voices. We have selected three groups of subjects: persons with normal voices, which considered as reference, persons having suffered Parkinson disease (PD) and persons with spasmodic dysphonia.","keywords: {Pathology;Diseases;Feature extraction;Databases;Filtering;Mel frequency cepstral coefficient;Harmonic analysis;Neurological Voice Disorders;Glottal signal parameters;articultory;KNN},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8374384&isnumber=8374364,"M. Dahmani and M. Guerti, ""Glottal signal parameters as features set for neurological voice disorders diagnosis using K-Nearest Neighbors (KNN),"" 2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP), Algiers, Algeria, 2018, pp. 1-5, doi: 10.1109/ICNLSP.2018.8374384."
"A Kepstrum based approach for enhancement of dysarthric speech,","A novel speech processing algorithm based on Kepstrum analysis procedure is proposed in this paper, which provides very good speech enhancement for Dysarthric speech. Kepstrum approach has so far been used in communication applications like two microphone noise cancellation. The other applications are derivation of Kalman filter and wiener filter equations. So an attempt to use kepstrum approach to enhance the dysarthric speech is made in this paper. The algorithm is tested on various monosyllabic and bisyllabic (Consonant-Vowel pattern and Consonant-Vowel-Consonant-Vowel pattern) dysarthric speech samples of cerebral palsy patients between the age group of 40–60 years and it was found that there was considerable formant shift and modification in the energy of the output signal. Also the results obtained by kepstrum approach is compared with the results obtained by Linear Prediction Coefficients (LPC) method and it is found that kepstrum approach gives better results.","keywords: {Speech;Speech enhancement;Filter bank;Wiener filter;Estimation;Equations;Dysarthric speech;Kepstrum analysis;Formants;Linear Prediction Coefficients},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5646752&isnumber=5646157,"V. Lalitha, P. Prema and L. Mathew, ""A Kepstrum based approach for enhancement of dysarthric speech,"" 2010 3rd International Congress on Image and Signal Processing, Yantai, China, 2010, pp. 3474-3478, doi: 10.1109/CISP.2010.5646752."
"A 40nm CMOS SoC for Real-Time Dysarthric Voice Conversion of Stroke Patients,","This paper presents the first dysarthric voice conversion SoC, which can translate stroke patients' voice into more intelligible and clearer speech in real time. The SoC is composed of a RISC-V MPU and a compact DNN engine with a single 16-bit multiply-accumulator, which improves 12x performance and > 100x energy efficiency, and has been implemented in 40nm CMOS. The silicon area is 0.68×0.79mm2, and the measured power is 18.4mW for converting 3-sec dysarthric voice within 0.5 sec (at 200MHz and 0.8V) and 4.8mW for conversion < 1 sec (at 100MHz and 0.6V).","keywords: {Power measurement;Design automation;Asia;Area measurement;Stroke (medical condition);Real-time systems;Silicon},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9712584&isnumber=9712479,"T. -J. Lin et al., ""A 40nm CMOS SoC for Real-Time Dysarthric Voice Conversion of Stroke Patients,"" 2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC), Taipei, Taiwan, 2022, pp. 7-8, doi: 10.1109/ASP-DAC52403.2022.9712584."
"Speech recognition of deaf and hard of hearing people using hybrid neural network,","This paper describes isolated word recognition of deaf students by unsupervised and supervised neural network. Compared to normal speech, there is high variability in deaf speech and by hearing once we couldn't understand it. By the use of proposed method deaf people can make use of all voice operated devices. In this paper we use combination of SOFM and BPN neural network for recognition. Initially the input is sampled, filtered, windowed and Perceptual Linear Predictive Coefficients are determined for each frame. These coefficients are applied as input to the SOFM neural network. The output of this network is given to BPN neural network comprising of 3 layers for learning. The network has been trained with five words uttered by five different deaf persons in the age group of 5-10 years. Another set of same five words uttered by same five deaf persons were used for test purposes. The recognition results for the word one, three, four is 50 to 60% and for five is 50%...But the recognition results for word two are only10% since the variability is high for two. The results can be improved by varying the parameters of the hybrid neural network.","keywords: {Accuracy;Back propagation neural network (BPN);Self organized feature map neural network (SOFM);Perceptual linear prediction coefficients (PLP)},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5558589&isnumber=5558502,"C. Jeyalakshmi, V. Krishnamurthi and A. Revathi, ""Speech recognition of deaf and hard of hearing people using hybrid neural network,"" 2010 2nd International Conference on Mechanical and Electronics Engineering, Kyoto, Japan, 2010, pp. V1-83-V1-87, doi: 10.1109/ICMEE.2010.5558589."
"ASR for electro-laryngeal speech,","The electro-larynx device (EL) offers the possibility to re-obtain speech when the larynx is removed after a total laryngectomy. Speech produced with an EL suffers from inadequate speech sound quality, therefore there is a strong need to enhance EL speech. When disordered speech is applied to Automatic Speech Recognition (ASR) systems, the performance will significantly decrease. ASR systems are increasingly part of daily life and therefore, the word accuracy rate of disordered speech should be reasonably high in order to be able to make ASR technologies accessible for patients suffering from speech disorders. Moreover, ASR is a method to get an objective rating for the intelligibility of disordered speech. In this paper we apply disordered speech, namely speech produced by an EL, on an ASR system which was designed for normal, healthy speech and evaluate its performance with different types of adaptation. Furthermore, we show that two approaches to reduce the directly radiated EL (DREL) noise from the device itself are able to increase the word accuracy rate compared to the unprocessed EL speech.","keywords: {Speech;Training;Speech enhancement;Speech recognition;Databases;Accuracy;Materials;Automatic Speech Recognition (ASR);electro-larynx (EL);speech enhancement;MLLR adaptation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6707735&isnumber=6707689,"A. K. Fuchs, J. A. Morales-Cordovilla and M. Hagmüller, ""ASR for electro-laryngeal speech,"" 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, Czech Republic, 2013, pp. 234-238, doi: 10.1109/ASRU.2013.6707735."
"Navigo--Accessibility Solutions for Cerebral Palsy Affected,","This paper presents the architecture of an integrated teachable interface, designed and implemented to provide accessibility solutions to the cerebral palsy patient in the virtual and urban space. It enables multimodal interaction between the palsy user and computers, handhelds and remote controlled appliances for enhanced usability. The idea is to redesign the system by changing the way we interpret the input, to suit the user's needs. The USP of the solution is that it is extremely dynamic, flexible and customizable to the entire range and levels of a complex condition as cerebral palsy. It works on the basis of allowing an individual to define simple variable inputs for himself and the system enabling him using the same by various specially designed tools, interfaces and feedback mechanisms.","keywords: {Birth disorders;Home appliances;Keyboards;Computational intelligence;Computer architecture;Handheld computers;Usability;Feedback;Mice;Speech recognition;Human Computer Interaction;Cerebral Palsy;Accessibility Solutions},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4052772&isnumber=4052643,"H. Pokhariya, P. Kulkarni, V. Kantroo and T. Jindal, ""Navigo--Accessibility Solutions for Cerebral Palsy Affected,"" 2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06), Sydney, NSW, Australia, 2006, pp. 143-143, doi: 10.1109/CIMCA.2006.155."
"A study of pronunciation verification in a speech therapy application,",Techniques are presented for detecting phoneme level mispronunciations in utterances obtained from a population of impaired children speakers. The intended application of these approaches is to use the resulting confidence measures to provide feedback to patients concerning the quality of pronunciations in utterances arising within interactive speech therapy sessions. The pronunciation verification scenario involves presenting utterances of known words to a phonetic decoder and generating confusion networks from the resulting phone lattices. Confidence measures are derived from the posterior probabilities obtained from the confusion networks. Phoneme level mispronunciation detection performance was significantly improved with respect to a baseline system by optimizing acoustic models and pronunciation models in the phonetic decoder and applying a nonlinear mapping to the confusion network posteriors.,"keywords: {Speech;Medical treatment;Acoustic measurements;Application software;Decoding;Natural languages;Lattices;Neuromuscular;Loudspeakers;Communications technology;confidence measure;speech therapy},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4960657&isnumber=4959496,"Shou-Chun Yin, R. Rose, O. Saz and E. Lleida, ""A study of pronunciation verification in a speech therapy application,"" 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, Taipei, Taiwan, 2009, pp. 4609-4612, doi: 10.1109/ICASSP.2009.4960657."
"Building a Truly Inclusive Protocol for Students with Disabilities from an Experience in STEM areas,","Three of the most reported strategies for students with disabilities (SWD) along their stay in higher education institutions include adapting changes in management and facilities through protocols that focused on Admission, Retention and Graduation. These strategies are important, although they do not guarantee full inclusivity during the teaching, learning and evaluation process. In this work, through a qualitatively methodology, we describe the perceptions of inclusivity of five lecturers, eight undergraduates and one SWD in science courses for Computer Systems Engineering. Furthermore, we report one successful experience to design an inclusive evaluation in a Mathematics course. The evidence found in this work suggests that the center of every protocol for SWD should be focused on Attention, as a new and longer stage. This stage includes e.g. teacher training, inclusive curriculum (for teaching and evaluation), student service and university extension programs, and inclusivity-focused research. Our purpose is to address this experience to the engineering community and promote the establishment of policies towards a more inclusive higher education system.","keywords: {Training;Protocols;Conferences;Education;Buildings;Documentation;Systems engineering and theory;inclusive education;physically impaired student;inclusive protocols;educational innovation;higher education},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9454024&isnumber=9453843,"M. Peña-Becerril, C. Camacho-Zuñiga, C. Martínez-Peña and J. C. González-Balderas, ""Building a Truly Inclusive Protocol for Students with Disabilities from an Experience in STEM areas,"" 2021 IEEE Global Engineering Education Conference (EDUCON), Vienna, Austria, 2021, pp. 189-193, doi: 10.1109/EDUCON46332.2021.9454024."
"Fuzzy inference system & fuzzy cognitive maps based classification,",Fuzzy classification is very necessary because it has the ability to use interpretable rules. It has got control over the limitations of crisp rule based classifiers. This paper mainly deals with classification on the basis of soft computing techniques fuzzy cognitive maps and fuzzy inference system on the lenses dataset. The results obtained with FIS shows 100% accuracy. Sometimes the data available for classification contain missing or ambiguous data so Neutrosophic logic is used for classification to deal with indeterminacy.,"keywords: {Lenses;Information services;Electronic publishing;Internet;Fuzzy cognitive maps;Computers;Classification;Fuzzy cognitive maps;Fuzzy inference system},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7164720&isnumber=7164643,"K. Bhutani, Gaurav and M. Kumar, ""Fuzzy inference system & fuzzy cognitive maps based classification,"" 2015 International Conference on Advances in Computer Engineering and Applications, Ghaziabad, India, 2015, pp. 305-309, doi: 10.1109/ICACEA.2015.7164720."
"Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion,","Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.","keywords: {Training;Error analysis;Measurement uncertainty;Asia;Information processing},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10317592&isnumber=10317095,"W. -C. Huang and T. Toda, ""Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion,"" 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Taipei, Taiwan, 2023, pp. 1161-1166, doi: 10.1109/APSIPAASC58517.2023.10317592."
"Using Acoustic Deep Neural Network Embeddings to Detect Multiple Sclerosis From Speech,","Multiple sclerosis (MS) is a chronic inflammatory disease of the central nervous system. It affects cognitive and motor functions, and the limitation of executive functions can also manifest itself in speech production. Due to this, automatic speech analysis might serve as an effective technique for assessing MS, or for monitoring the status of the patient. However, choosing the features to be extracted from the recordings is not straightforward. In the past few years, general feature extractors such as i-vectors, d-vectors and x-vectors have found their way into automatic speech analysis. In this study we show that there is no need to employ a special neural network architecture such as x-vectors to calculate effective features, but (even more) indicative features can be derived on the basis of a standard Deep Neural Network acoustic model. From our results, these features could effectively be used to distinguish MS subjects from healthy controls, as we measured AUC scores up to 0.935. We found that classification performance depended only slightly on the choice of the hid-den layer used to extract our features, but the speech task per-formed by the subject turned out to be an important factor.","keywords: {Deep learning;Speech analysis;Multiple sclerosis;Signal processing;Feature extraction;Acoustics;Speech processing;Multiple Sclerosis;medical speech processing;Deep Neural Networks;embeddings;x-vectors},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746856&isnumber=9746004,"G. Gosztolya, L. Tóth, V. Svindt, J. Bóna and I. Hoffmann, ""Using Acoustic Deep Neural Network Embeddings to Detect Multiple Sclerosis From Speech,"" ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, 2022, pp. 6927-6931, doi: 10.1109/ICASSP43922.2022.9746856."
"Speech Therapy Applications based on Speech Technologies,","The proposed paper focuses on the area of ICTbased supportive tools for speech therapy in children with speech and hearing disorders in Slovak language. The main idea was to design a concept of the supportive tool, where speech technologies can be used in a new, modern way. The web application was designed to help a child to train a correct pronunciation of particular sounds. To evaluate the similarity of the spoken word, Dynamic Time Warping algorithm was implemented, which measure distance between spoken word and the pattern in the database. Obtained distance help a caregivers or therapist to evaluate the need of continuation in speech therapy with a particular sound, or it is possible to move to another one. We also proved the concept of using automatic speech recognition in the speech therapy to create modern game-like speech therapy tools. The proposed paper brings the first observations from preliminary tests and discuss advantages and drawbacks of designed tools.","keywords: {Databases;Heuristic algorithms;Medical treatment;Auditory system;Time measurement;Automatic speech recognition;speech therapy;speech technologies;web application;telemedicine},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10109062&isnumber=10109002,"S. Ondáš, A. Čižmár, L. Šimčiková and J. Kováč, ""Speech Therapy Applications based on Speech Technologies,"" 2023 33rd International Conference Radioelektronika (RADIOELEKTRONIKA), Pardubice, Czech Republic, 2023, pp. 1-5, doi: 10.1109/RADIOELEKTRONIKA57919.2023.10109062."
"Automatic Screening Of Children With Speech Sound Disorders Using Paralinguistic Features,","Subjective screening of children with speech disorders is costly, time consuming and infeasible due to the limited availability of Speech and Language Pathologists (SLPs). Therefore, there is an increasing interest in automatic speech analysis of children with speech disorders as it can offer a practical alternative to human assessment. Paralinguistic features are a set of low-level descriptors commonly used in speech emotion recognition. However, they have not yet been examined with childhood speech sound disorders such as, apraxia-of-speech and phonological and articulation disorders. In this paper, we investigated the effectiveness of paralinguistic features in discriminating between typically developing children and those who suffer from different types of speech sound disorders. Two types of standard paralinguistic features were explored, the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) and its extended version, (eGeMAPS) feature sets. We applied feature selection to find the most discriminant set of features and employed binary classification using a support vector machine (SVM) to discriminate between the two groups. The method was tested on a recently-released public speech corpus collected from typically developing children and children with various types of speech sound disorders. The system achieved segment-level and subject-level unweighted average recall (UAR) of around 78% and 87% respectively.","keywords: {speech sound disorders;speech therapy;paralinguistic features},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8918725&isnumber=8918685,"M. Shahin, B. Ahmed, D. V. Smith, A. Duenser and J. Epps, ""Automatic Screening Of Children With Speech Sound Disorders Using Paralinguistic Features,"" 2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP), Pittsburgh, PA, USA, 2019, pp. 1-5, doi: 10.1109/MLSP.2019.8918725."
"Dysarthric vocal interfaces with minimal training data,","Over the past decade, several speech-based electronic assistive technologies (EATs) have been developed that target users with dysarthric speech. These EATs include vocal command & control systems, but also voice-input voice-output communication aids (VIVOCAs). In these systems, the vocal interfaces are based on automatic speech recognition systems (ASR), but this approach requires much training data and detailed annotation. In this work we evaluate an alternative approach, which works by mining utterance-based representations of speech for recurrent acoustic patterns, with the goal of achieving usable recognition accuracies with less speaker-specific training data. Comparisons with a conventional ASR system on dysarthric speech databases show that the proposed approach offers a substantial reduction in the amount of training data needed to achieve the same recognition accuracies.","keywords: {Hidden Markov models;Abstracts;Computers;Filter banks;Films;Accuracy;vocal user interface;dysarthric speech;non-negative matrix factorisation},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7078582&isnumber=7078533,"J. F. Gemmeke, S. Sehgal, S. Cunningham and H. Van hamme, ""Dysarthric vocal interfaces with minimal training data,"" 2014 IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoe, NV, USA, 2014, pp. 248-253, doi: 10.1109/SLT.2014.7078582."
"Interaction of speech disorders with speech coders: effects on speech intelligibility,","Modern speech coding schemes have been developed to address the demand for economical spoken language telecommunication of acceptable quality. A variety of speech coding algorithms have been described, which compress speech to facilitate efficient transmission of spoken language over communication networks ((J.R. Deller Jr., 1993; P.E. Papamichalis, 1987). Most such speech coding algorithms are lossy in the sense that the processed speech is not identical to the original speech. As a result, some distortion is invariably introduced with any lossy speech coding strategy. For this reason, candidate coders undergo detailed evaluation to ensure that the associated speech output is of acceptable quality (S.R. Quackenbush et al., 1988). Three different coding algorithms were investigated relative to unprocessed speech: the Codebook Excited Linear Prediction (CELP), the Global System for Mobile Communications (GSM) algorithm which is a standardized speech coding algorithm in Europe, and the Linear Predictive Coding (LPC) algorithm. The specific coding schemes evaluated were MatLab implementations of NSA FS-1015 LPC-l0e; NSA FS-1016 CELP-v3.2; and ETSI GSM (A. Spanias, 1995). One of the goals of this study was to quantify the coding distortion using objective measures and to correlate these measures with speech intelligibility and subjective quality data, in the hope of identifying one or more measures that can predict the subjective results.","keywords: {Speech coding;GSM;Distortion measurement;Natural languages;Speech processing;Linear predictive coding;Economic forecasting;Communication networks;Speech analysis;Europe},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=607467&isnumber=13324,"D. G. Jamieson, L. Deng, M. Price, V. Parsa and J. Till, ""Interaction of speech disorders with speech coders: effects on speech intelligibility,"" Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96, Philadelphia, PA, USA, 1996, pp. 737-740 vol.2, doi: 10.1109/ICSLP.1996.607467."
"Evaluation of the word fluency in parkinson's disease patients treated with deep brain stimulation-a pilot study-,","Parkinson's disease is a chronic, progressive and common disease of neurological disorders. Motor complication in parkinson's disease (PD) is resting tremor, slow movement, rigidity and postural instability. The motor symptoms of PD respond well to bilateral deep brain stimulation (DBS). Recent study, there are also reports of worsened verbal fluency, executive dysfunction, and processing speed with DBS. Whether subthalamic nuclei (STN) stimulation worsens there are under debate. The aim of this study was to explore the effects of STN stimulation verbal fluency as assessed with clinical neuropsychological tests. Eight patients treated with deep brain stimulation were enrolled, and some of the patients continued anti-PD medications. Assessments were done both with the STN stimulation turned OFF and ON. In both test conditions, the following were assessed: speech , word fluency A, and B. The score of the word fluency test of all patients have undergone DBS surgery significantly worsened as compared with before surgery. Five patients speech ware worsened, but three patients were improved when the STN stimulation was turned OFF. On the other hand, five patients were reduced the word fluency's total score when the STN stimulation was turned OFF. In this sample, STN stimulation significantly worsened the result of the word fluency test. When the STN stimulation was turned OFF, it was reduced. These finding suggests that STN-DBS might be worse speech conditions and verbal fluency.","keywords: {Satellite broadcasting;Speech;Parkinson's disease;Deep brain stimulation;Word fluency;Verbal fluency;Cognitive function},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6275625&isnumber=6275588,"Y. Watanabe, T. Sada, R. Takashima, M. Takano, H. Tateno and K. Hirata, ""Evaluation of the word fluency in parkinson's disease patients treated with deep brain stimulation-a pilot study-,"" 2012 ICME International Conference on Complex Medical Engineering (CME), Kobe, Japan, 2012, pp. 5-8, doi: 10.1109/ICCME.2012.6275625."
"Compression of acoustic inventories using asynchronous interpolation,","A compression method is proposed that takes advantage of a powerful property of acoustic unit inventories: In the appropriate acoustic space, units that share a (context-dependent or -independent) phoneme label must be close to a vector phoneme template associated with the phoneme. The method approximates units by interpolation between templates. The interpolation operation involves two asynchronous weight functions operating on the template. One is associated with spectral peak locations, the second with spectral balance. This enables approximating transitions such as [i:]/spl rarr/[v], in which formant movement precedes frication onset. The algorithm guarantees smooth concatenation points.","keywords: {Interpolation;Speech synthesis;Speech coding;Acoustic devices;Artificial intelligence;Acoustic applications;Loudspeakers;Compression algorithms;Natural languages;Acoustical engineering},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1224378&isnumber=27469,"A. B. Kain and J. P. H. van Santen, ""Compression of acoustic inventories using asynchronous interpolation,"" Proceedings of 2002 IEEE Workshop on Speech Synthesis, 2002., Santa Monica, CA, USA, 2002, pp. 83-86, doi: 10.1109/WSS.2002.1224378."
"Development of Speech Therapy Mobile Application for Speech Disorder Post-Stroke Patients,","In Malaysia, stroke is the third cause of death and disability. Stroke cause significant injury to the brain that may result in long-term problems such as communication, concentration, memory, and executive functions. About one-third of post-stroke patients have speech and communication problems that require to undergo series of one-to-one speech therapy sessions. However, there are only 300 speech therapists in Malaysia which limit the recovery and may not reach to the needed patient. More importantly, frequent therapy conducted could fasten the recovery of the patients' speech. Therefore, this research develops a mobile application to be used as an alternative for speech therapy session. Although there are mobile applications for speech therapy, none of them are in Bahasa Melayu. The mobile application implements an automatic speech recognition technology that accepts the vowel speech sound from a post-stroke patient in Bahasa Melayu. The mobile application will process the sound, evaluate, and provide feedback score for the vowel sound in an accuracy percentage. It is expected that the ASR speech therapy mobile application could help speech disorder post-stroke patients to practice their speech ability at their own time without attending speech therapy sessions.","keywords: {Training;Conferences;Medical treatment;Systems engineering and theory;Mobile applications;Engines;Monitoring;mobile app;interface;automatic speech recognition;stroke},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9612432&isnumber=9612427,"H. Basiron, M. A. Azmi, M. J. Abd Latif, A. I. Kamaruddin, A. I. M. Zaidi and W. M. F. W. Badrulzaman, ""Development of Speech Therapy Mobile Application for Speech Disorder Post-Stroke Patients,"" 2021 IEEE 11th International Conference on System Engineering and Technology (ICSET), Shah Alam, Malaysia, 2021, pp. 130-133, doi: 10.1109/ICSET53708.2021.9612432."
"Exploratory analysis of speech features related to depression in adults with Aphasia,","Aphasia is an acquired communication disorder resulting from brain damage and impairs an individual's ability to use, produce, and comprehend language. Loss of communication skills can be stressful and may result in depression, yet most depression diagnostic tools are designed for adults without aphasia. This paper discusses preliminary results from a research effort to examine acoustic profiles of adults with aphasia who have been assessed as having possible depression versus those who assessment suggests they are not depressed based on tools completed by their caretakers. This study analyzes prosodic and spectral features in 14 participants (7 assessed as having possible depression and 7 whose assessment does not suggest depression). The results showed using Cepstral Peak Prominence provided the best overall performance in separating depressed and non-depressed speech among adults with aphasia.","keywords: {Speech;Cepstral analysis;Feature extraction;Jitter;Stress;Acoustic measurements;aphasia;depression;speech analysis;prosodic features},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7472792&isnumber=7471614,"S. Gillespie, E. Moore, J. Laures-Gore and M. Farina, ""Exploratory analysis of speech features related to depression in adults with Aphasia,"" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5815-5819, doi: 10.1109/ICASSP.2016.7472792."
"Speech enhancement for pathological voice using time-frequency trajectory excitation modeling,","This paper proposes a speech enhancement algorithm for pathological voices using a time-frequency trajectory excitation (TFTE) modeling. The TFTE model has a capability of delicately controlling the periodic and non-periodic excitation components by taking a single pitch based decomposition process. By investigating the difference of frequency characteristics between pathological and normal voices, this paper proposes an enhancement algorithm which can efficiently reduce the breathiness of the pathological voice while maintaining the identity of the speaker. Subjective test results are presented to verify the effectiveness of the proposed algorithm.","keywords: {Pathology;Speech;Indexes;Speech enhancement;Time-frequency analysis;Noise measurement;Speech coding},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6694125&isnumber=6694103,"E. Song, J. Ryu and H. -G. Kang, ""Speech enhancement for pathological voice using time-frequency trajectory excitation modeling,"" 2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, Kaohsiung, Taiwan, 2013, pp. 1-4, doi: 10.1109/APSIPA.2013.6694125."
"Comparative Analysis of Machine Learning and Ensemble Learning Classifiers for Parkinson’s Disease Detection,","A neurological illness called Parkinson's disease (PD) commonly appears between the ages of 55 and 65. Moreover, a patient's entire quality of life is significantly impacted by the progressive development of motor as well as non-motor symptoms due to this disease. There is no known cure for PD, although a number of therapies have been created to assist control its symptoms. Therefore, the management of PD is a field that is expanding, and there is a need to develop a comprehensive framework for the timely detection and classification of PD. In this paper, we developed six machine learning-based and five ensemble learning-based classification models to forecast PD. The six base classifiers which are used in the current study are Support Vector Machine (SVM), Decision Trees (DTs), Random Forest (RF), K-Nearest Neighbor (KNN), Logistic Regression (LR), Naive Bayes (NB); and five ensemble classifiers named XGBoost, Gradient Boost, Bagging, CatBoost and Light Gradient Boosted Machine (LGBM) respectively, are then carefully compared. To improve the performance of the classifiers and to reduce the problem of overfitting, a feature selection method named Principal Component Analysis (PCA) and various preprocessing techniques are applied. Further, this study uses the voice samples dataset from the UCI repository having 188 PD and 64 normal patients. Overall, our findings revealed that, when compared to the other five base classifiers, the RF model offered the best classification performance with an accuracy of 82.37%, and the ensemble classifier named LGBM shows best results when compared with base as well as ensemble classifiers having an accuracy of 85.90%.","keywords: {Support vector machines;Parkinson's disease;Predictive models;Feature extraction;Ensemble learning;Older adults;Random forests;Parkinson’s Disease;Ensemble Classifiers;xgboost;Gradient Boost;Bagging;catboost;LGBM},",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10007376&isnumber=10006842,"P. Goyal, R. Rani and K. Singh, ""Comparative Analysis of Machine Learning and Ensemble Learning Classifiers for Parkinson’s Disease Detection,"" 2022 3rd International Conference on Computing, Analytics and Networks (ICAN), Rajpura, Punjab, India, 2022, pp. 1-6, doi: 10.1109/ICAN56228.2022.10007376."
